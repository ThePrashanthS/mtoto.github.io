<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on Coding with Data</title>
    <link>/tags/docker/index.xml</link>
    <description>Recent content in Docker on Coding with Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Parallelizing R code on kubernetes</title>
      <link>/blog/parallelizing-r-code-on-kubernetes/</link>
      <pubDate>Tue, 07 Aug 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/parallelizing-r-code-on-kubernetes/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;kubernetes-who&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Kubernetes who?&lt;/h1&gt;
&lt;p&gt;The hype around kubernetes is real, but likely also justified. Kubernetes is an open-source tool that facilitates deployment of jobs and services onto computer clusters. It provides different patterns for different type of workloads, be it API servers, databases or running batch jobs. Not only makes kubernetes running workloads and services easy, it also &lt;a href=&#34;https://thenewstack.io/kubernetes-credited-saving-spire-service-s3-outage/&#34;&gt;keeps them running&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At the core of the technology are containers, which kubernetes skillfully manages inside so-called pods. A pod represents a single instance of an application and contains one or sometimes more containers. Pods in turn live on worker nodes - actual servers - and are managed by a controller on the master node. We can interact with pods indirectly via instructions to controller.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/infra2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Mark Edmondson has already written a &lt;a href=&#34;http://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts/&#34;&gt;fantastic blog post&lt;/a&gt; about different use cases for running R application inside kubernetes. I’ll dive into the one topic he didn’t expand upon: the parallel execution of R code on kubernetes.&lt;/p&gt;
&lt;p&gt;I will similarly use GCP’s &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/&#34;&gt;kubernetes engine&lt;/a&gt; to deploy my jobs, however all major cloud providers have similar offerings. It’s worth mentioning that Google provides 300$ worth of credit free to spend on any of their cloud products, so you can freely experiment without burning a hole in your pocket.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;single-job-with-static-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Single job with static parameters&lt;/h1&gt;
&lt;p&gt;The simplest use case of parallelization is running the same script over and over again, but in parallel instead of in a sequential order. A classic example is simulation, i.e. the random generation of numbers given a fixed set of parameters.&lt;/p&gt;
&lt;p&gt;I am taking an example from &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/batch/tutorial-r-doazureparallel#run-a-parallel-simulation&#34;&gt;Azure’s tutorial on running R code in parallel&lt;/a&gt;, simulating stock prices after a a year (365 days) given a fixed value for standard deviation and average stock price movement per day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_change = 1.001 
volatility = 0.01 
opening_price = 100 

getClosingPrice &amp;lt;- function(days) { 
        movement &amp;lt;- rnorm(days, mean=mean_change, sd=volatility) 
        path &amp;lt;- cumprod(c(opening_price, movement)) 
        closingPrice &amp;lt;- path[days] 
        return(closingPrice) 
} 

replicate(1000, getClosingPrice(365)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s save the above script into an executable file, in our case &lt;code&gt;monte-carlo.R&lt;/code&gt;, and write a minimal &lt;code&gt;Dockerfile&lt;/code&gt; encapsulating the script. Remember kubernetes works with containers and can access them directly from &lt;a href=&#34;https://hub.docker.com/&#34;&gt;Dockerhub&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;FROM rocker/r-base
COPY monte-carlo.R ./&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We build the image and upload it to dockerhub using the docker command line tool.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# build image
docker build -t mtoto/mc-demo:latest .
# upload to docker hub
docker push mtoto/mc-demo:latest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now comes the kubernetes bit in the form of a &lt;code&gt;job.yaml&lt;/code&gt; file, that contains the instructions for the controller. Note that under &lt;code&gt;spec:&lt;/code&gt; we specify the number of pods to run our job on in parallel (distribution over pods over nodes is handled by kubernetes), and the number of completions. Each pod picks up a single run and exists after the script has finished. By the end of this workload 100 pods have been created, run and terminated.&lt;/p&gt;
&lt;pre class=&#34;yml&#34;&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: static-demo
spec:
  parallelism: 10
  completions: 100
  template:
    metadata:
      name: static-example
      labels:
        jobgroup: static-example
    spec:
      containers:
      - name: birthday
        image: mtoto/mc-demo
        command: [&amp;quot;Rscript&amp;quot;, &amp;quot;monte-carlo.R&amp;quot;]
      restartPolicy: Never&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With everything in place (&lt;code&gt;R&lt;/code&gt; script, &lt;code&gt;Dockerfile&lt;/code&gt;, &lt;code&gt;.yaml&lt;/code&gt; file), we are ready to deploy our first job to kubernetes. Assuming you have &lt;a href=&#34;https://support.google.com/cloud/answer/6158841?hl=en&#34;&gt;enabled the relevant services&lt;/a&gt; in the google cloud console, downloaded the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;google cloud SDK&lt;/a&gt; and have &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt; installed, we can create our cluster and deploy our first the workload on GCP in the following way:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create 3 node cluster &amp;quot;kubepar&amp;quot; on google kubernetes engine
gcloud container clusters create kubepar --machine-type n1-standard-1 --num-nodes 4
# get credentials to point kubectl to our cluster
gcloud container clusters get-credentials kubepar
# create job
kubectl create -f job.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can monitor the progress of our job using the command &lt;code&gt;kubectl get pods&lt;/code&gt;, to see how many pods have successfully run.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/static-pods-2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Similarly we can look at the state of the nodes with &lt;code&gt;kubectl get nodes&lt;/code&gt; or the overall status of the job with &lt;code&gt;kubectl get jobs static-demo&lt;/code&gt;. For a more detailed output, substitute &lt;code&gt;get&lt;/code&gt; with &lt;code&gt;describe&lt;/code&gt;, such as &lt;code&gt;kubectl describe pods&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once the job has finished, we collect the output of our simulation from the logs of each pod and write it to a &lt;code&gt;.txt&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;for p in $(kubectl get pods -l jobgroup=static-example -o name)
do
  kubectl logs $p &amp;gt;&amp;gt; output.txt
done&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reading the output into &lt;code&gt;R&lt;/code&gt; we can plot the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(stockprices)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/stockprices.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-template-and-multiple-parameters-using-expansion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common template and multiple parameters using expansion&lt;/h1&gt;
&lt;p&gt;Moving on, now we want to parallelize a script with different parameters at each run. Again, I am taking an example from a &lt;a href=&#34;http://blog.revolutionanalytics.com/2018/01/doazureparallel-simulations.html&#34;&gt;doAzureParallel tutorial&lt;/a&gt; where&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… we calculate for a room of N people the probability that someone in the room shares a birthday with someone else in the room.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Below is the simulation script for 100.000 rooms where we supply the number of people in the room as a command line argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#!/usr/bin/env Rscript
args = commandArgs(trailingOnly=TRUE)
n &amp;lt;- as.double(args[1])

pbirthdaysim &amp;lt;- function(n) { 
        ntests &amp;lt;- 100000 
        pop &amp;lt;- 1:365 
        anydup &amp;lt;- function(i) 
                any(duplicated( 
                    sample(pop, n, replace=TRUE)))
        sum(sapply(seq(ntests), anydup)) / ntests 
}

pbirthdaysim(n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike before, we are not creating a single representation of our Job object in a &lt;code&gt;.yaml&lt;/code&gt; file, but a &lt;em&gt;Job template&lt;/em&gt; with placeholders. The &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/expansion/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; is the same as before, except for the script. Don’t forget to build and upload the image before continuing.&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: par-demo-$ITEM
spec:
  template:
    metadata:
      name: par-example
      labels:
        jobgroup: par-example
    spec:
      containers:
      - name: birthday
        image: mtoto/birthday-demo
        command: [&amp;quot;Rscript&amp;quot;, &amp;quot;birthday.R $ITEM&amp;quot;]
      restartPolicy: Never&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we didn’t specify parallelization parameters nor the number of completions. It’s because we are going to &lt;strong&gt;expand&lt;/strong&gt; the above template into 100 different &lt;code&gt;job.yaml&lt;/code&gt; files, one for each run with a different &lt;code&gt;n&lt;/code&gt; parameter for the birthday simulation.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create folder for jobs
mkdir jobs
# create job.yaml files
for i in {1..100}
do
  cat job.yaml | sed &amp;quot;s/\$ITEM/$i/&amp;quot; &amp;gt; ./jobs/job-$i.yaml
done&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the same command as before, we create all the jobs at once: &lt;code&gt;kubectl create -f ./jobs&lt;/code&gt;. Kubernetes will automatically create, distribute and run our jobs in parallel across pods on the nodes of our cluster.&lt;/p&gt;
&lt;p&gt;Using the same &lt;code&gt;bash&lt;/code&gt; script as before, we can retrieve the output from each run and after read it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Plotting the results, the probability that 2 or more people will have the same birthday is 99% after 60 people in the room.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(probabiliy, xlab=&amp;quot;People in room&amp;quot;, 
     ylab=&amp;quot;Probability of shared birthday&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/birthdays.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fine-parallel-processing-using-a-work-queue&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fine parallel processing using a work queue&lt;/h1&gt;
&lt;p&gt;In the previous example, we created all the jobs at once, which can overload the scheduler if the number of jobs is very large. A smarter approach is to create a work queue and let the pods pick them off one by one as they go along. Unlike before, each pod will work on multiple items until the queue is empty instead of creating a pod for each task.&lt;/p&gt;
&lt;p&gt;To illustrate the last approach, we will parallelize different regression models for the Boston housing dataset, a pretty common use case for parallelization in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function below takes the name of an algorithm, loads the dataset, creates a training set, runs a model using the caret package and finally uploads the result to google cloud storage as an &lt;code&gt;.rds&lt;/code&gt; file. This way the work queue only needs to contain the names of the models to run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# modeling function
run_save_model &amp;lt;- function(method) {
        library(mlbench)
        library(caret)
        data(&amp;quot;BostonHousing&amp;quot;)
        # split data
        set.seed(123)
        train_index &amp;lt;- createDataPartition(BostonHousing$medv,1, p = .7)
        train &amp;lt;- BostonHousing[train_index[[1]],]
        # train model
        model &amp;lt;- train(medv ~., 
                       data = train, 
                       method = method)
        
        # upload to storage bucket
        file &amp;lt;- sprintf(&amp;quot;%s_model.rds&amp;quot;, method)
        saveRDS(model, file)
        googleCloudStorageR::gcs_upload(file, 
                   name = file,
                   bucket = &amp;quot;bostonmodels&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;setting-up-redis-on-kubernetes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up Redis on kubernetes&lt;/h2&gt;
&lt;p&gt;We’ll be using Redis for the work queue, so we need an additional pod running Redis and a service so other pods can retrieve the items from the work queue. The recipes for both are within &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/redis-pod.yaml&#34;&gt;redis-master.yaml&lt;/a&gt; and &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/redis-service.yaml&#34;&gt;redis-service.yaml&lt;/a&gt;. Similarly to jobs, we can use &lt;code&gt;kubectl create&lt;/code&gt; command to start the instances and then use the Redis command line tool to add the work items.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create redis pod and redis service
kubectl create -f ./redis-pod.yaml
kubectl create -f ./redis-service.yaml
# create temporary interactive pod
kubectl run temp -i --rm --tty  --image redis --command &amp;quot;/bin/sh&amp;quot;
# initiate redis cli
redis-cli -h redis
# push items into queue named &amp;quot;test&amp;quot;
rpush test &amp;quot;lm&amp;quot; &amp;quot;rf&amp;quot; &amp;quot;gbm&amp;quot; &amp;quot;enet&amp;quot; &amp;quot;brnn&amp;quot; &amp;quot;bridge&amp;quot;
# doublecheck queue
lrange test 0 -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the consumer side, I re-implemented the &lt;a href=&#34;https://kubernetes.io/examples/application/job/redis/rediswq.py&#34;&gt;Redis client from the official docs&lt;/a&gt; in R using the &lt;a href=&#34;https://cran.r-project.org/web/packages/redux/vignettes/redux.html&#34;&gt;redux package&lt;/a&gt;. The file &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/rediswq.R&#34;&gt;rediswq.R&lt;/a&gt; contains all the building blocks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;giving-access-to-google-cloud-storage-from-kubernetes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Giving access to google cloud storage from kubernetes&lt;/h2&gt;
&lt;p&gt;Before we could extract the output from the logs, now we will save the models as &lt;code&gt;.rds&lt;/code&gt; files on cloud storage. For this, the containers running on our cluster need write access to our storage bucket.&lt;/p&gt;
&lt;p&gt;Using Google Cloud, we create a new &lt;a href=&#34;https://cloud.google.com/compute/docs/access/service-accounts&#34;&gt;service account&lt;/a&gt; inside our project and under &lt;strong&gt;Roles&lt;/strong&gt; give it full access to cloud storage by selecting &lt;strong&gt;Storage Object Admin&lt;/strong&gt;. Make sure to check the box for &lt;strong&gt;Furnish a new private key&lt;/strong&gt; and click SAVE.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/service.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Back to the terminal, we can save our credentials as a Secret that will be directly accessible to the kubernetes engine.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create secret named &amp;quot;gcs-key&amp;quot;
kubectl create secret generic gcs-key --from-file=key.json=PATH-TO-KEY-FILE.json&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll see how to use this secret in the &lt;code&gt;job.yaml&lt;/code&gt; file shortly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;worker-program&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Worker program&lt;/h2&gt;
&lt;p&gt;Finally, we write a worker program that takes the work items from the Redis work queue and executes &lt;code&gt;run_save_model()&lt;/code&gt;. While the pods have no knowledge of the number of work items in the queue, they notice when the queue is empty and will automatically terminate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;rediswq.R&amp;quot;)
source(&amp;quot;functions.R&amp;quot;)

# connect to redis host
host &amp;lt;- Sys.getenv(&amp;quot;REDIS_SERVICE_HOST&amp;quot;)
db &amp;lt;- redis_init(host = host)
vars_init(&amp;quot;test&amp;quot;)

# authenticate gcs
library(googleCloudStorageR)

print(paste0(&amp;quot;Worker with sessionID: &amp;quot;, session))
print(paste0(&amp;quot;Initial queue state: empty=&amp;quot;, as.character(empty())))

while (!empty()) {
        item &amp;lt;- lease(lease_secs=10,
                        block = TRUE,
                        timeout = 2)
        if (!is.null(item)) {
                print(paste0(&amp;quot;working on: &amp;quot;, item))
                # actual work
                run_save_model(item)
                complete(item)
        } else {
          print(&amp;quot;waiting for work&amp;quot;)       
        }
}
print(&amp;quot;queue emtpy, finished&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all the scripts in place, let’s not forget to build a Docker image and upload it to Dockerhub. The &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/Dockerfile&#34;&gt;Dockerfie&lt;/a&gt; is going to be a bit longer this time given the numerous dependencies our program needs.&lt;/p&gt;
&lt;p&gt;As for the &lt;code&gt;.yaml&lt;/code&gt; file, it is very similar to what we have written before with the addition of mounting our Secret &lt;code&gt;gcs-key&lt;/code&gt; as a volume so that the containers have access. We name this variable &lt;code&gt;GCS_AUTH_FILE&lt;/code&gt;, which the &lt;a href=&#34;https://github.com/cloudyr/googleCloudStorageR&#34;&gt;googlegoogleCloudStorageR package&lt;/a&gt; looks for when loading the library to authenticate the client.&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: fine-demo
spec:
  parallelism: 4
  template:
    metadata:
      name: fine-example
      labels:
        jobgroup: fine-example
    spec:
      volumes:
      - name: google-cloud-key
        secret:
          secretName: gcs-key
      containers:
      - name: c
        image: mtoto/ml-demo
        volumeMounts:
        - name: google-cloud-key
          mountPath: /var/secrets/google
        env:
        - name: GCS_AUTH_FILE
          value: /var/secrets/google/key.json
        command: [&amp;quot;Rscript&amp;quot;, &amp;quot;worker.R&amp;quot;]
      restartPolicy: OnFailure&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like before, we hit &lt;code&gt;kubectl create -f job.yaml&lt;/code&gt; to start the job and monitor the status of the 4 pods with &lt;code&gt;kubectl get pods&lt;/code&gt;. You will notice that the pods don’t exit until the queue is finished. Once they are done working on one item they pick up the next one, saving additional overhead compared to the previous two approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs-to-keep-in-mind&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Trade-offs to keep in mind&lt;/h1&gt;
&lt;p&gt;Going from static workloads to setting up work queues that feed into the workers, we are introducing additional complexity. It’s not always a good thing, especially not if modifying existing applications is costly. We could’ve done parallel machine learning just as well using parameter expansion (the second approach).&lt;/p&gt;
&lt;p&gt;On the other hand, having one Job object for each work item creates some overhead that a single Job object for all work items does not. Again, the difference will become more apparent the more work we have.&lt;/p&gt;
&lt;p&gt;Lastly, the first two approaches create as many pods as work items, requiring less modification to existing code. With the last approach however each pod can process multiple items, which is a gain in efficiency.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dockerized Shiny App development</title>
      <link>/blog/dockerized-shiny-app-development/</link>
      <pubDate>Tue, 16 Jan 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/dockerized-shiny-app-development/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;getting-on-the-docker-container-ship&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting on the Docker (container) ship&lt;/h2&gt;
&lt;p&gt;Containers are everywhere, including the realms of data science. You can think of them as small self-contained environments, encapsulating an application and its dependencies. If that sounds a lot like a virtual machine, you are not entirely wrong. But unlike VM’s, containers run on the host system’s kernel and the processes inside can only see and access their immediate surroundings.&lt;/p&gt;
&lt;p&gt;Thanks to the good people behind the &lt;a href=&#34;https://hub.docker.com/u/rocker/&#34;&gt;rocker project&lt;/a&gt;, there’s already plenty of R-specific Docker images available for folks looking to containerize their R code. The most often cited benefits are &lt;em&gt;portability&lt;/em&gt; and &lt;em&gt;reproducibility&lt;/em&gt; of your analysis. In the same vein, &lt;a href=&#34;https://maraaverick.rbind.io/2017/11/docker-izing-your-work-in-r/&#34;&gt;lots of great material&lt;/a&gt; is out there with respect to what these bad boys exactly are and how to get them up and running.&lt;/p&gt;
&lt;p&gt;But I haven’t found much on &lt;em&gt;Docker based workflows&lt;/em&gt;, especially how to go about developing dockerized shiny apps. Because what if I want to build a shiny dashboard inside a container, integrate it with &lt;a href=&#34;https://travis-ci.org/&#34;&gt;Travis CI&lt;/a&gt; and run tests on every single commit to GitHub?&lt;/p&gt;
&lt;p&gt;The code in this post is based on a bare bones shiny app (containing USA Trade data) I built for illustration purposes. You can find the app &lt;a href=&#34;http://usatrade.tamaszilagyi.com/&#34;&gt;here&lt;/a&gt;, and all the code on &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testable-shiny-apps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testable shiny apps&lt;/h2&gt;
&lt;p&gt;We all heard of unit testing, but can we test an actual shiny application? As often the case in the R world, &lt;em&gt;there is already a package for that:&lt;/em&gt; &lt;a href=&#34;https://github.com/rstudio/shinytest&#34;&gt;shinytest&lt;/a&gt; - an automated testing agent for, you guessed it…shiny apps. It works as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Shinytest uses snapshot-based testing strategy. The first time it runs a set of tests for an application, it performs some scripted interactions with the app and takes one or more snapshots of the application’s state. These snapshots are saved to disk so that future runs of the tests can compare their results to them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The interface is super easy. You install the package and when the first version of your shiny app is ready to roll, you simply run &lt;code&gt;recordTest()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/shinytest&amp;quot;)
library(shinytest)

recordTest(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This launches an iframe consisting of your dashboard and controls over what to test. Each interaction with the dashboard is recorded, and when you hit &lt;strong&gt;take snapshot&lt;/strong&gt;, the state of your dashboard is saved, along with raw scripts to reproduce the interactions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://i.imgur.com/t3xcuCX.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Upon exiting the test event recorder, a new folder &lt;code&gt;test/&lt;/code&gt; is created inside the app’s directory, containing both the test script - &lt;code&gt;dates.R&lt;/code&gt;, as well as the application’s state as a .json and a .png files in &lt;code&gt;test/dates-expected&lt;/code&gt;. The latter serve as expected output, based on which consequent runs of tests shall be evaluated. Using my example app, &lt;code&gt;dates.R&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(shiny.testmode=TRUE)

app &amp;lt;- ShinyDriver$new(&amp;quot;../&amp;quot;, seed = 123)
app$snapshotInit(&amp;quot;dates&amp;quot;)

app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, running &lt;code&gt;testApp(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt; will look for test scripts inside the &lt;code&gt;test/&lt;/code&gt; folder, and run them to recreate the state of the test recording, comparing the output to what’s expected. It is generally a good idea to only compare the .json files, because the screenshots of the app (the .png file) will likely differ of various systems. We pass the argument &lt;code&gt;compareImages = FALSE&lt;/code&gt; to bypass default behavior. A full fledged test script will then look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(testthat)
test_that(&amp;quot;Application works&amp;quot;, {
        expect_pass(testApp(&amp;quot;/srv/shiny-server/myapp/&amp;quot;,
                            testnames = &amp;quot;dates&amp;quot;,
                            compareImages = FALSE))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I found that having ggplot2 (or plotly) plots as part of your dashboard, there is always a tiny bit of randomness present in the output. And hence the tests fail. It is better to explicitly export parts of the plot objects in my opinion, because they will be a more reliable yardstick to compare against. To do so, we add a few lines of code to &lt;code&gt;server.R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exportTestValues(plot_balance = { ggplot_build(p_b)$data },
                 plot_total   = { ggplot_build(p_t)$data },
                 plot_import  = { ggplot_build(p_i)$data },
                 plot_export  = { ggplot_build(p_e)$data } )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a follow up, we customize which parts of the application’s state should be saved and checked for inside &lt;code&gt;app$snapshot()&lt;/code&gt;, using the &lt;code&gt;items =&lt;/code&gt; argument and update &lt;code&gt;dates.R&lt;/code&gt; so that only the &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;export&lt;/code&gt; (and &lt;em&gt;not the&lt;/em&gt; &lt;code&gt;output&lt;/code&gt;) sections of our .json files are evaluated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;...
app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot(items = list(input = TRUE, export = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is all you really need to get going with shinytest. Keep in mind that the package is still in development, and things might change in the future. For an in-depth walkthrough of shinytest’s capabilities, have a look at the &lt;a href=&#34;https://rstudio.github.io/shinytest/articles/shinytest.html&#34;&gt;official site&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-container-can-we-haz-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A 🐳 container, can we haz it?&lt;/h2&gt;
&lt;p&gt;Now that our shiny app is complete with test scripts, the whole thing can be packaged up and put inside a container. Of course we could deploy the shiny dashboard without a container too, but at the end of the day it makes everybody’s life a lot easier.&lt;/p&gt;
&lt;p&gt;Because if our container runs on our machine, it will also run on &lt;strong&gt;any machine&lt;/strong&gt; that has Docker. Without compatibility issues, independent from host version or platform distribution. In a real life scenario this significantly reduces time between prototypting and deployment, not the least because of the typically lightweight footprint of a Docker image.&lt;/p&gt;
&lt;p&gt;To containerize our shiny app, we first need to create an image that encompasses our:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Shiny application&lt;/li&gt;
&lt;li&gt;R packages our app needs&lt;/li&gt;
&lt;li&gt;System level dependencies these packages need&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We build our image layer by layer, starting with the &lt;a href=&#34;https://hub.docker.com/r/rocker/shiny/&#34;&gt;rocker/shiny image&lt;/a&gt; - which includes the minimal requirements for a Shiny Server. Then, we add everything else our application requires; finishing with copying the contents of our app to &lt;code&gt;/srv/shiny-server/usa-trade/&lt;/code&gt;, where the dashboard will be served from. These instructions are written to the &lt;code&gt;Dockerfile&lt;/code&gt;, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FROM rocker/shiny
MAINTAINER Tamas Szilagyi (tszilagyi@outlook.com)

## install R package dependencies (and clean up)
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2 \
    libssl-dev \
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## install packages from CRAN (and clean up)
RUN Rscript -e &amp;quot;install.packages(c(&amp;#39;devtools&amp;#39;,&amp;#39;dplyr&amp;#39;,&amp;#39;tidyr&amp;#39;,&amp;#39;fuzzyjoin&amp;#39;,&amp;#39;stringr&amp;#39;,&amp;#39;ggthemes&amp;#39;,&amp;#39;quantmod&amp;#39;,&amp;#39;ggplot2&amp;#39;,&amp;#39;shinydashboard&amp;#39;,&amp;#39;shinythemes&amp;#39;), repos=&amp;#39;https://cran.rstudio.com/&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install packages from github (and clean up)
RUN Rscript -e &amp;quot;devtools::install_github(&amp;#39;rstudio/shinytest&amp;#39;,&amp;#39;rstudio/webdriver&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install phantomjs
RUN Rscript -e &amp;quot;webdriver::install_phantomjs()&amp;quot;

## assume shiny app is in build folder /app2
COPY ./app2 /srv/shiny-server/usa-trade/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The smaller your Docker image, the better. Here’s a couple of guidelines to keep in mind when creating one:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Always use &lt;strong&gt;shared base images&lt;/strong&gt; (what comes after the &lt;code&gt;FROM&lt;/code&gt; statement) specific to your application, instead of trying to reinvent the wheel every time you write a Dockerfile.&lt;/li&gt;
&lt;li&gt;Try to &lt;strong&gt;avoid underused dependencies&lt;/strong&gt;. Going back to the my example app, I could’ve installed the package &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyquant/index.html&#34;&gt;tidyquant&lt;/a&gt; to get my trade data in a tidy format out of the box, yet because the package has an insane amount of dependencies (including having Java installed); I wrote three &lt;a href=&#34;https://github.com/mtoto/markets_shiny/blob/master/app2/functions.R#L14&#34;&gt;helper functions&lt;/a&gt; instead.&lt;/li&gt;
&lt;li&gt;Make sure &lt;strong&gt;temporary files are removed&lt;/strong&gt; after the installation of libraries and packages.&lt;/li&gt;
&lt;li&gt;Push down commands that will likely invalidate the &lt;strong&gt;cache&lt;/strong&gt;, so Docker only rebuilds layers that change (more on this in the next section).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the Dockerfile finished, it is time to make ourselves familiar with the essential Docker commands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;docker pull&lt;/strong&gt; pulls an image from the registry (Dockerhub).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker build&lt;/strong&gt; builds a docker image from our Dockerfile.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run&lt;/strong&gt; instantiates the container from our image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker exec&lt;/strong&gt; execute commands from within the container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker rm&lt;/strong&gt; deletes a container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker login&lt;/strong&gt; login to Dockerhub (to upload our image).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker push&lt;/strong&gt; uploads the image back to Dockerhub.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s say we want to run our shiny app on a server that has Docker installed. Assuming we have a &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub repo&lt;/a&gt; containing all relevant files and our Dockerfile is to be found on &lt;a href=&#34;https://hub.docker.com/r/mtoto/shiny/&#34;&gt;Dockerhub&lt;/a&gt;, we can expose our shiny app to the world as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1 clone into repo containing app 
git clone https://github.com/mtoto/markets_shiny.git
# 2 pull Docker file from Dockerhub
docker pull mtoto/shiny:latest
# 3 build Docker image, tag it &amp;#39;mtoto/shiny:latest&amp;#39;
docker build -t mtoto/shiny:latest .
# 4 run container in detached mode, listening on port 80, name it &amp;#39;site&amp;#39;
docker run -d -p 80:3838 --name site mtoto/shiny:latest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And our app should be visible on &lt;em&gt;ht​ps://myserver.com/usa-trade&lt;/em&gt; by default.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integration-with-travis-ci&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integration with Travis CI&lt;/h2&gt;
&lt;p&gt;If you are a seasoned R package developer, you are no stranger to Travis CI. It is a &lt;strong&gt;Continuous Integration&lt;/strong&gt; tool that automatically performs checks and runs tests on your code every time you push a commit to GitHub. The broad idea behind continuous integration is to encourage test-driven development, thereby allowing for frequent commits to the codebase without having to worry about integration problems.&lt;/p&gt;
&lt;p&gt;Travis supports many languages - including R, and can also build from Docker images. After creating an account on the &lt;a href=&#34;https://travis-ci.org&#34;&gt;Travis website&lt;/a&gt;, connect with GitHub and pick the repository for which you’d like to use it.&lt;/p&gt;
&lt;p&gt;The repo needs to contain a &lt;code&gt;.travis.yml&lt;/code&gt; file, encapsulating the instructions for Travis. You’d tempted to write &lt;code&gt;language: R&lt;/code&gt; as the first line, but if we do that Travis will implicitly assume we are developing an R package and will start looking for the &lt;code&gt;DESCRIPTION&lt;/code&gt; file we do not have. Instead, I went with the undocumented option &lt;code&gt;language: generic&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, as we’ll be only running Docker commands anyway.&lt;/p&gt;
&lt;p&gt;The naive approach would be to build our Docker image on every single run, instantiate a test container, run tests inside and upon success get rid of the container. Such a &lt;code&gt;.travis.yml&lt;/code&gt; would look like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker build -t markets-shiny .
- docker run -d -p 3838:3838 markets-shiny:latest --name test

script:
- docker exec test R -f run_tests.R

after_script:
- docker rm -f test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem here is that we are building the Docker image from scratch with every single Travis run, resulting in a build time of over 20 minutes for my simple app. But our image is on Dockerhub, so why not pull it from there and take advantage of caching. Then, we’d only rebuild the changed layers after downloading the latest image from the registry.&lt;/p&gt;
&lt;p&gt;To make sure everything is nice and up to date, we will push the changes back to Dockerhub after every successful run. We need credentials to do so, but Travis conveniently allows for defining environment variables inside the repository settings (or via the CLI):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://i.imgur.com/gdTnLjd.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Now we can go wild and revamp &lt;code&gt;.travis.yml&lt;/code&gt; accordingly:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker pull mtoto/shiny:latest
- docker build --cache-from mtoto/shiny:latest -t mtoto/shiny:latest . 
- docker run --rm -d -p 3838:3838 --name test mtoto/shiny:latest

script:
- docker exec test R -f /srv/shiny-server/usa-trade/run_tests.R

after_success:
- docker rm -f test
- docker login -u mtoto -p $DOCKER_PASSWORD
- docker tag mtoto/shiny:latest mtoto/shiny:$TRAVIS_BUILD_NUMBER
- docker push mtoto/shiny&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the second run (once the latest image is on Dockerhub), the build time is reduced by a factor of 10. Sweet. When we use the flag &lt;code&gt;--cache-from&lt;/code&gt;, Docker only rebuilds changed layers, ie. modifications to our shiny app. We can see this in the Travis log as &lt;code&gt;---&amp;gt; Using cache&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://i.imgur.com/hTNuQhY.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Keep in mind when making significant changes to your dashboard, it is important to update the tests that create fresh expected outputs reflecting these changes. If you don’t trust the outputs will align, remember to use &lt;code&gt;exportTestValues()&lt;/code&gt; and fill it up with the new objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;was-it-all-worth-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Was it all worth it?&lt;/h2&gt;
&lt;p&gt;While this workflow might feel like over-engineering, once all the tools are set up to work in tandem, shiny dashboard development becomes surprisingly efficient. The icing on the cake is that you are creating a dashboard that is pretty much ready for deployment from day one. Devops will love you for it, trust me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In reality this points to &lt;code&gt;language: bash&lt;/code&gt;, &lt;code&gt;language: sh&lt;/code&gt; and &lt;code&gt;language: shell&lt;/code&gt;.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>