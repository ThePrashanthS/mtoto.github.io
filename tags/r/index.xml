<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Coding with Data</title>
    <link>/tags/r/index.xml</link>
    <description>Recent content in R on Coding with Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>An animated neuRal net implementation</title>
      <link>/blog/an-animated-neural-net-implementation/</link>
      <pubDate>Thu, 09 Nov 2017 21:00:30 -0500</pubDate>
      
      <guid>/blog/an-animated-neural-net-implementation/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;yet-another-neural-net-from-scratch-tutorial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Yet another neural net from scratch tutorial?&lt;/h1&gt;
&lt;p&gt;One would be forgiven to think that artificial neural networks are the newest and shiniest of modern data science. On the contrary, the main concepts have been around for decades. But it is recent progress in computational resources and the availability of massive datasets that these learning architectures revealed their true powers. AlphaGo, Siri and Alexa, self-driving cars are all running on some form or other of deep artificial neural networks.&lt;/p&gt;
&lt;p&gt;The hype means the Internet is aflush with tutorials and online resources to get started. Yet, somehow R hasn’t gotten much street cred in the area. Most of the frameworks are implemented in Python, and so are the tutorials. R is supposed to be the de facto lingua franca of statistical computing, so what’s up with that?&lt;/p&gt;
&lt;p&gt;What follows is a custom build of a simple one hidden-layer neural network, where we’ll save just enough parameters at every iteration to be able to &lt;a href=&#34;https://github.com/dgrtwo/gganimate&#34;&gt;gganimate&lt;/a&gt; the training process afterwards.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-main-ingredients&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The main ingredients&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-11-11-animated_net_files/figure-html/fig2-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This post is mostly inspired by &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Andrew Ng’s Deep Learning course&lt;/a&gt; (including the dataset), which I strongly recommend for anyone interested in neural networks. The task is to predict the color of the points in the plot on the right. While it seems like a trivial problem, linear algorithms will inevitably fail at it because the colors are not linearly separable. There’s no single line we can draw that perfectly separates the red dots from the blue dots.&lt;/p&gt;
&lt;div id=&#34;how-does-the-algorithm-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does the algorithm work?&lt;/h2&gt;
&lt;p&gt;The input layer contains the input data. The number of nodes in the input layer is &lt;em&gt;always&lt;/em&gt; equal to the number of features in the data (X, Y coordinates). The input is then &lt;strong&gt;1. propagated forward&lt;/strong&gt; through the hidden layer. Number of hidden nodes and number of hidden layers can be modified at will. The edges between the nodes represent the weights, and the prediction is essentially a function of these weights.&lt;/p&gt;
&lt;p&gt;Once the data has been passed through the entire network, we get the predictions in the output node and &lt;strong&gt;2. compute the cost&lt;/strong&gt; with respect to the actual labels. At each iteration, we adjust the weights to minimize this cost as much possible.&lt;/p&gt;
&lt;p&gt;How do we do that? That’s the job of &lt;strong&gt;3. backward propagation&lt;/strong&gt;. By means of gradient descent, we calculate the partial derivatives of all computations with respect to what came after, alas we go &lt;em&gt;backwards&lt;/em&gt;. First the derivatives of the weights of the hidden layer with respect to the output layer, and secondly those of the input layer with respect to the hidden layer. The gradients we obtain are then used to &lt;strong&gt;update the weights&lt;/strong&gt; and start the process all over again. With each pass - also called &lt;strong&gt;epochs&lt;/strong&gt;, we get closer to the optimal weights.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;down-the-rabbit-hole&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Down the rabbit hole&lt;/h1&gt;
&lt;p&gt;I will now explain in short and code up each of the three computations. The scripts we define will be used inside a single function call that trains the neural network. Significant overhead will be introduced by saving parameters at every iteration, but hopefully the animated plots will be worth it.&lt;/p&gt;
&lt;div id=&#34;forward-propagation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Forward propagation&lt;/h2&gt;
&lt;p&gt;Forward propagation is the first pass through the data, calculating an output based on the weights of each edge. The connections from the input layer to each of the hidden nodes is a linear function, followed by an activation function.&lt;/p&gt;
&lt;p&gt;The computational steps of forward propagations are &lt;span class=&#34;math inline&#34;&gt;\(Input -&amp;gt; Hidden -&amp;gt; Output\)&lt;/span&gt; .&lt;/p&gt;
&lt;p&gt;If we break down each of the two connections into a linear function &lt;span class=&#34;math inline&#34;&gt;\(Z^{[i]}\)&lt;/span&gt; and an activation function &lt;span class=&#34;math inline&#34;&gt;\(A^{[i]}\)&lt;/span&gt;, the architecture becomes &lt;span class=&#34;math inline&#34;&gt;\(X -&amp;gt;Z^{[1]}-&amp;gt;A^{[1]}-&amp;gt;Z^{[2]}-&amp;gt;A^{[2]}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as the input data.&lt;/p&gt;
&lt;p&gt;The activation function is usually a non-linear function that enables the network to cope with non-linear problems. Examples include the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sigmoid_function&#34;&gt;sigmoid function&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_neural_networks&#34;&gt;relu&lt;/a&gt; or &lt;a href=&#34;http://mathworld.wolfram.com/HyperbolicTangent.html&#34;&gt;tanh&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take for example the connections going &lt;strong&gt;from the input layer to one hidden node&lt;/strong&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(X_{m,n}\)&lt;/span&gt; is the vertically stacked dataset where &lt;em&gt;m = number of features (2)&lt;/em&gt; , &lt;em&gt;n = number of observations&lt;/em&gt;, &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is a weight vector of length &lt;em&gt;m&lt;/em&gt;; the linear function in one hidden node can be formally represented as a matrix vector product:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}

w =
 \begin{pmatrix}
  w_{1} \\
  w_{2} \\
 \end{pmatrix};
 X = 
 \begin{pmatrix}
  x_{1,1} &amp;amp; x_{1,2} &amp;amp; \cdots &amp;amp; x_{1,n} \\
  x_{2,1} &amp;amp; x_{2,2} &amp;amp; \cdots &amp;amp; x_{2,n} \\
 \end{pmatrix}
 \end{align*}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
  Z = {w^T}X + b= 
  \begin{pmatrix}
  w_{1} &amp;amp; w_{2} \\
 \end{pmatrix}
  \begin{pmatrix}
  x_{1,1} &amp;amp; x_{1,2} &amp;amp; \cdots &amp;amp; x_{1,n} \\
  x_{2,1} &amp;amp; x_{2,2} &amp;amp; \cdots &amp;amp; x_{2,n} \\
 \end{pmatrix} 
  + b
\end{align*}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
  Z = 
    \begin{pmatrix}
      w_{1}x_{1,1} + 
      w_{2}x_{2,1}+b &amp;amp; 
      w_{1}x_{1,2}+ + 
      w_{2}x_{2,2}+b &amp;amp;
      \cdots &amp;amp;
      w_{1}x_{1,n} + 
      w_{2}x_{2,n}+b
    \end{pmatrix}
\end{align*}\]&lt;/span&gt;
&lt;p&gt;The activation function &lt;span class=&#34;math inline&#34;&gt;\(A^{[1]}\)&lt;/span&gt; is the &lt;em&gt;tanh&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(A^{[1]} = \tanh(Z^{[1]})\)&lt;/span&gt;, for the output layer we’ll use the sigmoid instead &lt;span class=&#34;math inline&#34;&gt;\(A^{[2]} = \sigma(Z^{[2]})\)&lt;/span&gt;. The computation can also be visualised as a subgraph of our neural network:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-11-11-animated_net_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;412.8&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It turns out that this implementation scales to multiple hidden nodes without any formal change to the math. Instead of a &lt;em&gt;weight vector&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, we are computing the same functions using a &lt;em&gt;weight matrix&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. The matrix-vector product now becomes a dot product between the two matrices. With each node in the hidden layer, we add an extra row in the transposed weight matrix. The dimensionality requirements of matrix multiplication are kept intact: &lt;em&gt;The number of columns of first matrix still equal the number of rows of the second&lt;/em&gt;. But the dimensions of the output change accordingly. We go from a transposed vector of length n to an m x n matrix where &lt;em&gt;m = the number of hidden nodes&lt;/em&gt;.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Z = {W^T}X + b= 
  \begin{pmatrix}
  w_{1,1} &amp;amp; w_{1,2} \\
  \vdots  &amp;amp; \vdots  \\
  w_{n,1} &amp;amp; w_{n,2}  
 \end{pmatrix}
  \begin{pmatrix}
  x_{1,1} &amp;amp; \cdots &amp;amp; x_{1,n} \\
  x_{2,1} &amp;amp; \cdots &amp;amp; x_{2,n} \\
 \end{pmatrix} 
  + b
\end{align*}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
  Z = 
    \begin{pmatrix}
      w_{1,1}x_{1,1} + 
       w_{1,2}x_{2,1}+b &amp;amp; 
      \cdots &amp;amp;
      w_{1,1}x_{1,n} + 
      w_{1,2}x_{2,n}+b 
      \\
      \vdots  &amp;amp; \ddots &amp;amp; \vdots\\
      w_{n,1}x_{1,1} + 
      w_{n,2}x_{2,1}+b &amp;amp; 
      \cdots &amp;amp;
      w_{n,1}x_{1,n} + 
       w_{n,2}x_{2,n}+b 
    \end{pmatrix}
  \end{align*}\]&lt;/span&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-11-11-animated_net_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The last step of going from the hidden layer to the output layer follows the same algebra. I’ll spare you the nitty gritty. Before we propagate forward for the first time, it is important to &lt;strong&gt;randomly initialize the weights&lt;/strong&gt;. Otherwise each connection will compute &lt;em&gt;the exact same thing&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initialize_parameters &amp;lt;- function(n_x, n_h, n_y) {

    set.seed(2) 
    # W1 -- weight matrix of shape (n_h, n_x)
    W1 = matrix(rnorm(n_x*n_h), nrow = n_h, ncol = n_x) * 0.01
    # b1 -- bias vector of shape (n_h, 1)
    b1 = rep(0, n_h)
    # W2 -- weight matrix of shape (n_y, n_h)
    W2 = matrix(rnorm(n_h*n_y),  nrow = n_y, ncol = n_h) * 0.01
    # b2 -- bias vector of shape (n_y, 1)
    b2 = rep(0, n_y)

    parameters = list(W1 = W1,b1 = b1,W2 = W2,b2 = b2)
    
    return(parameters)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember the schema is &lt;span class=&#34;math inline&#34;&gt;\(X -&amp;gt;Z^{[1]}-&amp;gt;A^{[1]}-&amp;gt;Z^{[2]}-&amp;gt;A^{[2]}\)&lt;/span&gt;. Both &lt;span class=&#34;math inline&#34;&gt;\(Z^{[1]}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z^{[2]}\)&lt;/span&gt; are the same linear function, while &lt;span class=&#34;math inline&#34;&gt;\(A^{[1]} = \tanh(Z^{[1]})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{[1]} = \sigma(Z^{[2]})\)&lt;/span&gt;. The sigmoid function didn’t make it to &lt;code&gt;base&lt;/code&gt; R, so we define it first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmoid &amp;lt;- function(x) {
   1 / (1 + exp(-x))
}

forward_propagation &amp;lt;- function(X, parameters) {
  
    # Retrieve each parameter from the list &amp;quot;parameters&amp;quot;
    W1 &amp;lt;- parameters$W1; b1 &amp;lt;- parameters$b1
    W2 &amp;lt;- parameters$W2; b2 &amp;lt;- parameters$b2

    # Forward propagation
    Z1 = W1 %*% X + b1
    A1 = tanh(Z1)
    Z2 = W2 %*% A1 + b2
    A2 = sigmoid(Z2)
    
    cache &amp;lt;- list(Z1=Z1,A1=A1,Z2=Z2,A2=A2)
    
    return(cache)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each pass of forward propagation ends with a prediction. Generating a prediction for every pixel of our plot raster, we can simulate decision boundaries. As the algorithm learns, the borders slowly align with the data:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/test_bounds.gif&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-the-cost&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Computing the cost&lt;/h2&gt;
&lt;p&gt;As we have seen above, forward propagation is nothing more than a &lt;em&gt;predict&lt;/em&gt; function. When the dataset has been passed through the network, we get an output that can be compared to the actual label. The purpose of the cost function is to inform the model how far the output is from the target value. One of the most popular cost functions is log loss, formally known as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small Y\log\left(A^{[2]}\right) + (1-Y)\log\left(1- A^{[2]}\right)  \large  \right) \small \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_cost &amp;lt;- function(A2, Y) {
        
        # Number of observations
        m &amp;lt;- dim(Y)[2] 
        
        cost &amp;lt;- -1/m * sum(Y * log(A2) + (1-Y)*log(1-A2))
        
        return(cost)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You saw how the algorithm was getting better at predicting the colors with each iteration. This is the result of reducing the cost as the model learns. &lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/costs.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backward-propagation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Backward propagation&lt;/h2&gt;
&lt;p&gt;Out of all building blocks of neural networks, back propagation is perhaps the most difficult to grasp. In a nutshell, it is calculating the error contribution of each weight to cost. The idea is to backward engineer the derivative or &lt;em&gt;slope&lt;/em&gt; of every computation and update the weights so that the cost will decrease at each iteration.&lt;/p&gt;
&lt;p&gt;We first calculate the gradient of &lt;span class=&#34;math inline&#34;&gt;\(Z^{[2]}\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(A^{[2]}\)&lt;/span&gt;, this is equal to &lt;span class=&#34;math inline&#34;&gt;\(dZ^{[2]} = A^{[2]} - Y\)&lt;/span&gt;. Based on &lt;span class=&#34;math inline&#34;&gt;\(dZ^{[2]}\)&lt;/span&gt; we then calculate the gradients of the weights (and bias terms) going from the hidden layer to the output layer. We continue going backwards until we obtain the gradients for all the weights and bias terms.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A^{[2]} -&amp;gt;dZ^{[2]}-&amp;gt;A^{[1]} -&amp;gt; dZ^{[1]} \\ \quad \quad  \downarrow  \quad \quad \quad \quad \quad  \quad  \downarrow \\ \quad \quad \quad dW^{[2]},db^{[2]} \quad  \quad    dW^{[1]},db^{[1]}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Below is the list of formulae we will need for the computations. Drilling further into the math is beyond the scope of this post, but there are &lt;a href=&#34;http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4&#34;&gt;great blog posts around dedicated to the topic&lt;/a&gt;.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}

 dZ^{[2]} = A^{[2]} - Y &amp;amp;
 \\ dW^{[2]} = \frac{1}mdZ^{[2]}A^{[1]T} &amp;amp;
 \\ db^{[2]} = \frac{1}m\sum_{n=1}^{m} dZ^{[2]} &amp;amp;
 \\ dZ^{[1]} = W^{[2]T}dZ^{[2]} * (1-A^{[1]2}) &amp;amp;
 \\ dW^{[1]} = \frac{1}mdZ^{[1]}X^{T} &amp;amp;
 \\ db^{[1]} = \frac{1}m\sum_{n=1}^{m} dZ^{[1]}
 
\end{align*}\]&lt;/span&gt;
&lt;p&gt;The math certainly looks scarier than &lt;code&gt;R&lt;/code&gt; code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;backward_propagation &amp;lt;-function(parameters, cache, X, Y) {
    
    m = dim(X)[2]
    
    # Retrieve W2 
    W2 &amp;lt;- parameters$W2

    # Retrieve A1 and A2
    A1 &amp;lt;- cache[&amp;quot;A1&amp;quot;][[1]]; A2 &amp;lt;- cache[&amp;quot;A2&amp;quot;][[1]]

    # Backward propagation
    dZ2 &amp;lt;- A2 - Y
    dW2 &amp;lt;- 1/m * dZ2 %*% t(A1)
    db2 &amp;lt;- 1/m * sum(dZ2)
    dZ1 &amp;lt;- t(W2) %*% dZ2 * (1 - A1^2)
    dW1 &amp;lt;- 1/m * dZ1 %*% t(X)
    db1 &amp;lt;- 1/m * sum(dZ1)

    grads &amp;lt;- list(dW1 = dW1,db1 = db1, dW2 = dW2,db2 = db2)
    
    return(grads)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having obtained the gradients, we can choose a &lt;strong&gt;learning rate&lt;/strong&gt; - the size of the step - at which we wish to update the weights at each iteration. This will be the heart of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;gradient descent&lt;/a&gt; optimization we will shorty define.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update_parameters &amp;lt;- function(parameters, grads, learning_rate = 5.2) {

    # Retrieve parameters
    W1 &amp;lt;- parameters$W1; b1 &amp;lt;- parameters$b1
    W2 &amp;lt;- parameters$W2; b2 &amp;lt;- parameters$b2

    # Retrieve gradients
    dW1 &amp;lt;- grads$dW1; db1 &amp;lt;- grads$db1
    dW2 &amp;lt;- grads$dW2; db2 &amp;lt;- grads$db2

    # Update rule for each parameter
    W1 &amp;lt;- W1 - learning_rate * dW1
    b1 &amp;lt;- b1 - learning_rate * db1
    W2 &amp;lt;- W2 - learning_rate * dW2
    b2 &amp;lt;- b2 - learning_rate * db2

    parameters &amp;lt;- list(W1 = W1, b1 = b1, W2 = W2, b2 = b2)

    return(parameters)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weight adjustments are the most dramatic at the start of the training process. As the slope towards the optimum value flattens, the rate at which weights adjust slows down as well.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/test_anim.gif&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bringing-it-all-together&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bringing it all together&lt;/h2&gt;
&lt;p&gt;Now we have all the ingredients of a neural network, it’s only a matter of putting the pieces together in one function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidygraph)

nn_model &amp;lt;- function(X, Y, n_h, num_iterations = 1000) {

    set.seed(3)
    n_x &amp;lt;- 2
    n_y &amp;lt;- 1
    
    # Initialize parameters
    parameters &amp;lt;- initialize_parameters(n_x, n_h, n_y)
    list_of_graphs &amp;lt;- list()
    list_of_params &amp;lt;- list()
    costs &amp;lt;- c()
    # Loop: gradient descent
    for (i in 0:num_iterations){
         
        # Forward propagation
        cache &amp;lt;- forward_propagation(X, parameters)
        A2 &amp;lt;- cache[&amp;quot;A2&amp;quot;][[1]]
        
        # Cost function
        cost &amp;lt;- compute_cost(A2, Y)
 
        # Backpropagation
        grads &amp;lt;- backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update
        parameters &amp;lt;- update_parameters(parameters, grads)
        
        # Save intermediate weights for plotting
        w &amp;lt;- c(as.vector(t(parameters$W1)), as.vector(parameters$W2))
        
        train_df &amp;lt;- dfg %&amp;gt;% activate(edges) %&amp;gt;% 
                mutate(weights = w, iteration = i) %&amp;gt;%
                as_tbl_graph()
        
        list_of_params[[i+1]] &amp;lt;- parameters
        list_of_graphs[[i+1]] &amp;lt;- train_df
        costs[i+1] &amp;lt;- cost 
        
    }

    return(list(list_of_params, list_of_graphs, costs))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under &lt;code&gt;# save intermediate weights for plotting&lt;/code&gt; is the overhead introduced by saving objects for the animations throughout this post. The only thing left is the predict function, and we are good to go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_nn&amp;lt;-function(parameters, X) {

    # Forward propagation 
    cache = forward_propagation(X, parameters)
    # Classify 0/1 with 0.5threshold
    predictions = (cache[&amp;quot;A2&amp;quot;][[1]]&amp;gt; 0.5)

    return(predictions)
}
# Run the model: 
# model &amp;lt;- nn_model(X,Y,n_h=4,100)
# Predict - 100th iteration weights:
# predictions = predict_nn(model[[1]][[100]], X)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/result.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;For the plots, I used the packages &lt;a href=&#34;https://github.com/tidyverse/ggplot2&#34;&gt;ggplot&lt;/a&gt;, &lt;a href=&#34;https://github.com/thomasp85/ggraph&#34;&gt;ggraph&lt;/a&gt; &lt;a href=&#34;https://github.com/thomasp85/ggraph&#34;&gt;gganimate&lt;/a&gt;, &lt;a href=&#34;https://github.com/thomasp85/tweenr&#34;&gt;tweenr&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/animation/index.html&#34;&gt;animation&lt;/a&gt;. Thanks to the creators of these awesome tools, I was able to make all the gifs using only R code. The full script for each of the animations is in the Appendix section at the &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/blog/2017/2017-11-11-animated_net.Rmd&#34;&gt;bottom of this .Rmd file&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A tidy text analysis of Rick and Morty</title>
      <link>/blog/a-tidy-text-analysis-of-rick-and-morty/</link>
      <pubDate>Sat, 07 Oct 2017 23:15:14 -0500</pubDate>
      
      <guid>/blog/a-tidy-text-analysis-of-rick-and-morty/</guid>
      <description>&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://i.imgur.com/a841k9g.gif&#34; /&gt;

&lt;/div&gt;
&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;adventures-in-the-multiverse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adventures in the multiverse&lt;/h2&gt;
&lt;p&gt;For those unfamiliar with the show, Rick and Morty is an animated series about the interuniversal exploits of a half-drunk mad scientist Rick, and his daft grandson Morty. Living under one roof with his daughter, Rick constantly drags his grandson Morty along for adventures into unusual worlds inhabited by surreal creatures. At first hesitant to accompany his eccentric granddad, Morty slowly grows into an indispensable sidekick. Using Rick’s portal gun, they leave the rest of their dysfunctional family at home, and travel through space and time.&lt;/p&gt;
&lt;p&gt;Most episodes draw inspiration from or make fun of cult movies such as Back to the Future, A Nightmare on Elm Street, Inception and many other classics by the likes of John Carpenter or David Cronenberg. Besides the ruthless humor and over-the-top visual scenery, the show brilliantly builds independent sci-fi realms, going about their day-to-day according to their wacky rules.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-mans-weekend-project-another-mans-treasure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One man’s weekend project, another man’s treasure&lt;/h2&gt;
&lt;p&gt;After reading the book &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;Tidy Text Mining&lt;/a&gt; online, I have been wanting to try out some of the concepts outlined in the book, and the functions of the &lt;a href=&#34;https://github.com/juliasilge/tidytext&#34;&gt;accompanying package&lt;/a&gt;, on an interesting dataset. So I was pretty stoked to find &lt;a href=&#34;https://github.com/fkeck/subtools&#34;&gt;Francois Keck’s &lt;strong&gt;subtools package&lt;/strong&gt; on GitHub&lt;/a&gt;, that allows for reading &lt;em&gt;.srt&lt;/em&gt; files (the usual format for subtitles) straight into R. With season 3 of Rick and Morty coming to an end last week, the stars have finally aligned to roll up my sleeves and have some fun with text mining.&lt;/p&gt;
&lt;p&gt;It is very easy to find English subtitles for pretty much anything on the internet. With subtools, an entire series can be read with one command from the containing folder, &lt;code&gt;read.subtitles.serie()&lt;/code&gt;. We convert the resulting MultiSubtitles object to a data.frame with a second command &lt;code&gt;subDataFrame()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(subtools)
a &amp;lt;- read.subtitles.serie(dir = &amp;quot;/series/rick and morty/&amp;quot;)
df &amp;lt;- subDataFrame(a)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Read: 3 seasons, 31 episodes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    16821 obs. of  8 variables:
##  $ ID          : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
##  $ Timecode.in : chr  &amp;quot;00:00:02.445&amp;quot; &amp;quot;00:00:03.950&amp;quot; &amp;quot;00:00:05.890&amp;quot; &amp;quot;00:00:07.420&amp;quot; ...
##  $ Timecode.out: chr  &amp;quot;00:00:03.850&amp;quot; &amp;quot;00:00:05.765&amp;quot; &amp;quot;00:00:07.295&amp;quot; &amp;quot;00:00:08.925&amp;quot; ...
##  $ Text        : chr  &amp;quot;Morty, you got to... come on.&amp;quot; &amp;quot;- You got to come with me. - Rick, what&amp;#39;s going on?&amp;quot; &amp;quot;I got a surprise for you, Morty.&amp;quot; &amp;quot;It&amp;#39;s the middle of the night. What are you talking about?&amp;quot; ...
##  $ season      : chr  &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; ...
##  $ season_num  : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ episode_num : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ serie       : chr  &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;$Text&lt;/code&gt; column contains the subtitle text, surrounded by additional variables for line id, timestamp, season and episode number. This is the structure preferred by the tidytext package, as it is by the rest of tidyverse.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;morty-you-got-tocome-on.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;“Morty, you got to…come on.”&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Let’s start with the bread and butter of text mining, &lt;em&gt;term frequencies&lt;/em&gt;. We split the text by word, exclude stop words,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(stop_words)
tidy_df &amp;lt;- df %&amp;gt;%
  unnest_tokens(word, Text) %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and aggregate and plot the top 10 words per season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)

tidy_df %&amp;gt;% group_by(season) %&amp;gt;%
        count(word, sort = TRUE) %&amp;gt;%
        top_n(10) %&amp;gt;%
        ggplot(aes(reorder(word,n), n, fill = season)) +
        geom_col() +
        coord_flip() +
        facet_wrap(~season, scales = &amp;quot;free_y&amp;quot;) +
        labs(x = NULL) +
        guides(fill = FALSE) +
        scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both seasons are dominated by, well, Rick and Morty. The main characters are tirelessly addressing each other, talking one another either into or out of the mess they find themselves in. What stands out most is the absence of Rick’s daughter, Beth from the top 10 in all seasons. She’s perhaps the only sane person of the family, but then again, sanity doesn’t get too much airtime on this show.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;network-analysis-on-bi-grams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Network analysis on bi-grams&lt;/h2&gt;
&lt;p&gt;We can similarly get the number of times each &lt;em&gt;two words&lt;/em&gt; appear, called &lt;em&gt;bi-grams&lt;/em&gt;. Besides calculating summary statistics on bi-grams, we can now construct a network of words according to co-occurrence using &lt;a href=&#34;https://cran.r-project.org/web/packages/igraph/index.html&#34;&gt;igraph&lt;/a&gt;, the go-to package for network analysis in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(igraph)

bigram_graph &amp;lt;- df %&amp;gt;%
  unnest_tokens(bigram, Text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;%
  filter(!word1 %in% stop_words$word) %&amp;gt;%
  filter(!word2 %in% stop_words$word) %&amp;gt;% 
  group_by(season) %&amp;gt;%
  count(word1, word2, sort = TRUE) %&amp;gt;%
  select(word1, word2, season, n) %&amp;gt;%
  filter(n &amp;gt; 2) %&amp;gt;%
  graph_from_data_frame()

str(bigram_graph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IGRAPH DN-- 310 280 -- 
## + attr: name (v/c), season (e/c), n (e/n)
## + edges (vertex names):
## [1] tiny            -&amp;gt;rick       yeah            -&amp;gt;yeah      
## [3] ice             -&amp;gt;cream      god             -&amp;gt;damn      
## [5] whoa            -&amp;gt;whoa      
##  [ reached getOption(&amp;quot;max.print&amp;quot;) -- omitted 275 entries ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This igraph object contains a directed network, where the vertices are the words and an edge exists between each that appear after one another more than twice. Representing the text as a graph, we can calculate things such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Centrality#Degree_centrality&#34;&gt;degree centrality&lt;/a&gt;, and plot the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the largest connected network, we arrive at the same conclusion as with term frequencies. Rick and Morty are the most important words. They are at the center of the network and so have the highest degree centrality scores.&lt;/p&gt;
&lt;p&gt;Besides visualising the importance of words in our network, we can similarly differentiate between words that precede either Rick or Morty. These are all the 1st degree connections (words) that have an edge pointing towards the main characters, but aren’t shared among the them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the red nodes, we recognize many of the things Rick throws at Morty: &lt;em&gt;“Relax Morty!…It’s science Morty!…Run Morty!”&lt;/em&gt;. There is also a handful of words that precede both characters like &lt;em&gt;“Geez”, “Boy”&lt;/em&gt; or &lt;em&gt;“God”&lt;/em&gt;. All other words that are more than one degree away, are colored blue as out of range.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tf-idf&lt;/h2&gt;
&lt;p&gt;Thus far we have looked at all words across seasons. But where do the seasons differ from each other? And can we summarise each season using a handful of topics? To answer the first question, text mining’s most notorious statistic &lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;&lt;strong&gt;tf-idf&lt;/strong&gt;&lt;/a&gt; comes to the rescue. It stands for &lt;strong&gt;term frequency - inverse document frequency&lt;/strong&gt;. We take the word counts per season and multiply it by the &lt;em&gt;scaled inverse fraction of seasons that contain the word&lt;/em&gt;. Simply put, we penalize words that are common across all seasons, and reward ones that are not. This way, we bring forth the words most typical of each season. Again the tidytext implementation is super easy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tf_idf_df &amp;lt;- tidy_df %&amp;gt;% 
        count(season, word, sort = TRUE) %&amp;gt;%
        bind_tf_idf(word, season, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we get back are the most important elements, characters, motives or places across episodes. I’m somewhat surprised that Mr. Meeseeks didn’t come in first though. I was sure as hell annoyed out of my mind after hearing it uttered for the 100th time during the episode &lt;a href=&#34;https://en.wikipedia.org/wiki/Meeseeks_and_Destroy&#34;&gt;Meeseeks and Destroy&lt;/a&gt;. But then again, Mr Meeseeks does make a cameo in two other seasons, so that kind of torpedoes his chances for the first spot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic models&lt;/h2&gt;
&lt;p&gt;Having seen the most unique words of the script by seasons, we will take our analysis one last step further and try to capture the gist of a the show using topic modeling. Broadly speaking, it’s an unsupervised classification method that tries to represent a document as a collection of topics. Here, I will take the classic &lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34;&gt;Latent Dirichlet Allocation or shortly LDA&lt;/a&gt; algorithm for a spin. The basic idea is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“…a topic is defined as a mixture over words where each word has a probability of belonging to a topic. And a document is a mixture over topics, meaning that a single document can be composed of multiple topics.”&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We could for example take season two, and tell &lt;code&gt;LDA()&lt;/code&gt; that we want to compress 10 episodes into just 6 topics. To compensate for the omnipresence of the top words across episodes, I will exclude them for the purpose of clearer separation of topics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(topicmodels)
popular_words &amp;lt;- c(&amp;quot;rick&amp;quot;,&amp;quot;morty&amp;quot;, &amp;quot;yeah&amp;quot;,&amp;quot;hey&amp;quot;,
                   &amp;quot;summer&amp;quot;, &amp;quot;jerry&amp;quot;, &amp;quot;uh&amp;quot;, &amp;quot;gonna&amp;quot;)

episodes_dtm &amp;lt;- tidy_df %&amp;gt;% filter(season_num == 2 &amp;amp; !word %in% popular_words) %&amp;gt;%
        group_by(episode_num) %&amp;gt;%
        count(word, sort = TRUE) %&amp;gt;%
        cast_dtm(episode_num, word, n) 

episodes_lda &amp;lt;- LDA(episodes_dtm, k = 6, control = list(seed = 1234))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After &lt;code&gt;tidy()&lt;/code&gt;ing the results, we can plot the top 10 words that contribute (&lt;em&gt;beta&lt;/em&gt;) to most to each topic.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt; There’s definitely a few topics that contain multiple elements of a particular episode. Take for example &lt;strong&gt;topic 1&lt;/strong&gt;. It includes “&lt;em&gt;Roy&lt;/em&gt;”, the name of the videogame Morty plays in the same episode “&lt;em&gt;Fart&lt;/em&gt;” appears, a gaseous creature kept under locks by aliens. Or &lt;strong&gt;topic 5&lt;/strong&gt;, which probably relates to the episode where Rick visits his old lover “&lt;em&gt;Unity&lt;/em&gt;”. It further contains words as “&lt;em&gt;remember&lt;/em&gt;” and “&lt;em&gt;memories&lt;/em&gt;”. The episode ends with Unity repeating “I want it &lt;em&gt;real&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;Not only can we examine the &lt;strong&gt;word per topic probabilities&lt;/strong&gt;, we can also plot &lt;strong&gt;the topic per document probabilities&lt;/strong&gt;, or &lt;em&gt;gamma&lt;/em&gt; values. This lets us see what topic belongs to what episode.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(episodes_lda, matrix = &amp;quot;gamma&amp;quot;) %&amp;gt;%
        inner_join(titles) %&amp;gt;%
        ggplot(aes(factor(topic), gamma)) +
        geom_boxplot() +
        facet_wrap(~ title) +
        ggtitle(&amp;quot;Dominant Topics per Episode&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our previous assumptions are confirmed, the first topic does belong to the episode &lt;em&gt;Mortynight Run&lt;/em&gt; as does the fifth topic to &lt;em&gt;Auto-Erotic Assimilation&lt;/em&gt;. It is important to note that the results strongly depend on the number of topics supplied to &lt;code&gt;LDA()&lt;/code&gt;, so inevitably, some experimentation is required to arrive at meaningful results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;I ran through some very interesting concepts fairly quickly in this post. I owe much of it to the tidytext package. With very little coding, we can mine a tremendous amount of insights from textual data. And I have just scrachted the surface of what’s possible. The seamless integration with the tidyverse, as with igraph and topicmodels does make a huge difference.&lt;/p&gt;
&lt;p&gt;Nonetheless, text mining is a complex topic and when arriving at more advanced material, &lt;a href=&#34;https://github.com/trinker/topicmodels_learning&#34;&gt;further reading&lt;/a&gt; on the inner workings of these algorithms might come in handy for effective use. The full &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/tree/master/data/2017-10-07-tidyrick/rick%20and%20morty&#34;&gt;data&lt;/a&gt; and &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/blog/2017/2017-10-07-tidyrick.Rmd&#34;&gt;code&lt;/a&gt; for this post is available as usual on my Github.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Self-learning Hue Lights</title>
      <link>/blog/self-learning-hue-lights/</link>
      <pubDate>Wed, 30 Aug 2017 23:15:14 -0500</pubDate>
      
      <guid>/blog/self-learning-hue-lights/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;the-rise-of-the-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The rise of the API&lt;/h2&gt;
&lt;p&gt;Rich API interfaces are one of the main ingredients of today’s smart devices. They are by definition built for interconnectivity and there is an active community of developers creating apps as microservices on top of them. Philips Hue is no exception with it’s wide variety of &lt;a href=&#34;http://www.developers.meethue.com/otherapps/otherAppsIOS.html#appsList&#34;&gt;apps&lt;/a&gt; available to users.&lt;/p&gt;
&lt;p&gt;But you don’t need to code an entire mobile application to take advantage of the low level access. Using modern tools it only takes a few lines of code to build a self-learning algorithm, running in production in your home. Not only can we access external API’s, we can just as easily expose static files, functions or models as an API of our own.&lt;/p&gt;
&lt;p&gt;My original inspiration for this post was &lt;a href=&#34;https://sc5.io/posts/autonomous-indoor-lighting-using-neural-networks/&#34;&gt;Max Pagel’s article&lt;/a&gt; on training a neural network to automatically control his Philips Hue lights. In fact, I purchased my first set of Hue bulbs because of it. In summary, this post will describe how to build and productionize a classifier in &lt;code&gt;R&lt;/code&gt; that controls the brightness of Philips Hue lights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-dinnertime&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stop, dinnertime!&lt;/h2&gt;
&lt;p&gt;Much like in my &lt;a href=&#34;http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/&#34;&gt;post on Spotify&lt;/a&gt; I have set up a cronjob to execute the Python script that pings the API and saves the lights’ state data locally, to be picked up by Luigi tasks for parsing and copying to S3 further downstream. You can find the relevant code on my &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/tasks.py&#34;&gt;Github&lt;/a&gt;. The &lt;a href=&#34;https://www.developers.meethue.com/philips-hue-api&#34;&gt;Hue API documentation&lt;/a&gt; contains information on authentication and the types of calls available.&lt;/p&gt;
&lt;p&gt;The starting point for this post will be the parsed &lt;code&gt;.json&lt;/code&gt; file containing all of the log data for my “Dinner Lamps”. They are the two main lights in my living and dining area room at the moment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aws.s3)
library(jsonlite)
# read file from amazon
aws.signature::use_credentials()
df &amp;lt;- s3read_using(object = paste0(&amp;quot;hue_full_2017-08-26.json&amp;quot;), fromJSON, bucket = &amp;quot;ams-hue-data&amp;quot;)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    30322 obs. of  15 variables:
##  $ on.1       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ on.2       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ bri.2      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ type.1     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ type.2     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ bri.1      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ modelid.2  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ modelid.1  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name.1     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; ...
##  $ reachable.1: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ reachable.2: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ name.2     : chr  &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ alert.1    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ log_time   : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:10:02&amp;quot; &amp;quot;2017-05-12 17:15:01&amp;quot; ...
##  $ alert.2    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the original &lt;code&gt;.json&lt;/code&gt; file is such that each lamp has a separate (numbered) column for every variable. The dataset is essentially a timeseries where each row represent a snapshot of the lamps’ state at &lt;code&gt;$log_time&lt;/code&gt;, or &lt;strong&gt;every 5 minutes&lt;/strong&gt;. Before moving on, let’s tidy things up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
tidy_df &amp;lt;- df %&amp;gt;% gather(key, value, -log_time) %&amp;gt;%
        separate(key, into = c(&amp;quot;variable&amp;quot;, &amp;quot;lamp&amp;quot;), sep = &amp;quot;\\.&amp;quot;) %&amp;gt;%
        spread(variable, value)
str(tidy_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    60644 obs. of  9 variables:
##  $ log_time : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; ...
##  $ lamp     : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; ...
##  $ alert    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ bri      : chr  &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; ...
##  $ modelid  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ on       : chr  &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; ...
##  $ reachable: chr  &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; ...
##  $ type     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 15 columns are now reduced to 9 because each variable appears only once thanks to adding the key column &lt;code&gt;$lamp&lt;/code&gt; to the dataset. But we are not quite done cleaning yet: I use the two lamps in sync, so we need only data from one of them. When the lamps are not &lt;code&gt;on&lt;/code&gt; nor &lt;code&gt;reachable&lt;/code&gt;, &lt;code&gt;$bri&lt;/code&gt; should be set to &lt;code&gt;0&lt;/code&gt;. Using the now correct brightness values, we create the four categories for the classifier to work with. Lastly, there were days I wasn’t home, so we can rid of of those observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
binned_df &amp;lt;- tidy_df %&amp;gt;% filter(lamp == &amp;quot;1&amp;quot;) %&amp;gt;%
        mutate(bri = as.numeric(replace(bri, on==&amp;quot;FALSE&amp;quot; | reachable==&amp;quot;FALSE&amp;quot;,0)),
               y = as.factor(ifelse(bri == 0, &amp;quot;zero&amp;quot;,
                                    ifelse(between(bri,0,80), &amp;quot;dim&amp;quot;,
                                           ifelse(between(bri,80,160),&amp;quot;mid&amp;quot;,&amp;quot;bright&amp;quot;)))))

off_days &amp;lt;- binned_df %&amp;gt;% group_by(date = as.Date(log_time,tz=&amp;quot;Europe/Amsterdam&amp;quot;)) %&amp;gt;%
                dplyr::summarise(total_bri = sum(bri)) %&amp;gt;%
                filter(total_bri == 0 ) %&amp;gt;%
                select(date)

binned_df &amp;lt;- binned_df %&amp;gt;% filter(!as.Date(log_time) %in% off_days$date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of our target variable look?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(binned_df$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## bright    dim    mid   zero 
##    598   1533   1710  23889&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Roughly 86% of the time the lamps are off, resulting in an unbalanced dataset. What about brightness values lamps were &lt;em&gt;on&lt;/em&gt;, according to the three remaining categories?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems to be close to normal with a positive skew, and a massive outlier all the way at the end of the spectrum. That’s maximum brightness, the default when I switch the lights on/off with a physical switch.&lt;/p&gt;
&lt;p&gt;To get an intuition for my usage patterns, I’ll also plot a histogram of hour of the day for all four categories.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The only times the lamps are &lt;strong&gt;not&lt;/strong&gt; structurally off, is in the evening and the early hours. Dim and mid values are the dominant category after 8PM instead. Another slight dip in zero appears around and shortly after midnight, compensated by the second largest peak in dim, and a few instances of mid and bright. Bright observations in general are sparse and will be tough to predict.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-we-learn-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can we learn this?&lt;/h2&gt;
&lt;p&gt;The only variables I will use for training, are time based: &lt;em&gt;day of the week&lt;/em&gt;; &lt;em&gt;month&lt;/em&gt;; &lt;em&gt;week number&lt;/em&gt;; &lt;em&gt;weekend or not&lt;/em&gt;; &lt;em&gt;time of the day&lt;/em&gt;; and &lt;em&gt;minutes since 12PM, 6AM, 12AM and 6PM&lt;/em&gt;. A datetime string will then suffice to generate a prediction on the fly, a boon for putting things into production later on. I packaged a chain of dplyr commands inside the function &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/functions.R&#34;&gt;add_vars()&lt;/a&gt; to add the above variables to the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_vars &amp;lt;- binned_df %&amp;gt;% add_vars(extra_var = &amp;quot;yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember we are dealing with an unbalanced dataset, that also happens to be a timeseries. To remedy the former, I will use class weights to penalize the misclassification of the minority classes. Secondly, I will look at the &lt;em&gt;Area Under the Curve&lt;/em&gt; to evaluate the model, which will be less affected than &lt;em&gt;Accuracy&lt;/em&gt; by class imbalance if I set dim as the positive class. If not for these measures, the algorithm would gladly classify 100% of instances as &lt;code&gt;&amp;quot;zero&amp;quot;&lt;/code&gt;, achieving stunning accuracy on paper and complete darkness in my living room.&lt;/p&gt;
&lt;p&gt;Now, why does it matter that we have a timeseries? In any dataset with a time component, the split between train and test sets should not be random. Otherwise, the model can and will &lt;strong&gt;learn from the future&lt;/strong&gt;, and severely overfit the data. The correct cross-validation strategy instead is to fold the data according to time. Train should always the before and test the after. For our convenience &lt;code&gt;caret&lt;/code&gt; provides the &lt;code&gt;createTimeSlices()&lt;/code&gt; function to create the indices of the CV-folds. An extra &lt;code&gt;testing&lt;/code&gt; set will be held out to validate our model on unseen data after we are done modeling.&lt;/p&gt;
&lt;p&gt;We’ll now train a &lt;a href=&#34;https://cran.r-project.org/web/packages/gbm/index.html&#34;&gt;gbm&lt;/a&gt; model, using the &lt;a href=&#34;https://topepo.github.io/caret/&#34;&gt;caret&lt;/a&gt; package, which comes with a myriad of convenience tools to make the process easier and the code a lot more concise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
# Split train and test sets
training &amp;lt;- df_vars[df_vars$date &amp;lt; &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date,-log_time)
testing &amp;lt;- df_vars[df_vars$date &amp;gt;= &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date)

# create cross validation folds
idx &amp;lt;- createTimeSlices(1:nrow(training), 
                      initialWindow = 15000, 
                      horizon = 5000, skip = 1000, fixedWindow = F)

# create model weights vector
model_weights &amp;lt;- ifelse(training$y == &amp;quot;zero&amp;quot;,0.2,
                        ifelse(training$y == &amp;quot;mid&amp;quot;,1.2,1))

# define cross validation logic
fitControl &amp;lt;- trainControl(## 10-fold CV
        index = idx[[1]],
        indexOut = idx[[2]],
        summaryFunction = multiClassSummary,
        classProbs = T)

# create tunegrid for hyperparameter search
gbmGrid &amp;lt;-  expand.grid(interaction.depth = c(1,3,5), 
                        n.trees = c(5,10,30), 
                        shrinkage = c(0.1),
                        n.minobsinnode = 5)

# train model
gbmFit &amp;lt;- train(y ~ ., data = training, 
                method = &amp;quot;gbm&amp;quot;, 
                trControl = fitControl,
                metric = &amp;quot;AUC&amp;quot;,
                weights = model_weights,
                verbose = FALSE,
                tuneGrid = gbmGrid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Printing &lt;code&gt;gbmFit&lt;/code&gt; to the console will give us the performance metrics across hyperparameters, and the ultimately selected values maximizing our metric of choice. While this is certainly useful information, I find it more intuitive to immediately look at the confusion matrix and see where our model is going off the rails:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds&amp;lt;-predict(gbmFit, testing)
table(preds, testing$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
## preds     dim  mid bright zero
##   dim     123  213     30  409
##   mid      41   78      6  190
##   bright   20   49      0    6
##   zero     36  103     29 4139&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most mistakes are made trying to classify bright and mid. The model gets 0 bright values right and only manages to do so correctly 14% of the time for mid. But when do errors happen? To dig a little deeper let’s look at the previous histogram of categories by hour again for the test set, but now with the predictions overlaid on top.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bright values were always going be hard to guess, but the model at least comes close in terms of hours, but off on the exact days. The majority of misclassification comes from overzealously predicting dim in the evening and around midnight, when it should really be either mid or zero. That looks like a workable scenario for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-ship-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Let’s ship it!&lt;/h1&gt;
&lt;p&gt;To control the lights, we can make PUT requests to the Hue bridge. To set &lt;em&gt;bri&lt;/em&gt;, we need actual brightness values. An intuitive option is to pick the median values per category per hour:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_values &amp;lt;- binned_df %&amp;gt;% filter(bri &amp;gt; 0) %&amp;gt;% 
                mutate(hour = lubridate::hour(as.POSIXct(log_time, tz = &amp;quot;Europe/Amsterdam&amp;quot;))) %&amp;gt;%
                select(hour,bri, y) %&amp;gt;% 
                group_by(y, hour) %&amp;gt;%
                dplyr::summarise(med = median(bri)) %&amp;gt;%
                ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we only used date and time based features for modeling, all we need for a prediction is a timestamp string. Using &lt;code&gt;for_sample&lt;/code&gt; and &lt;code&gt;def_vars()&lt;/code&gt;, we define a custom function &lt;code&gt;predict_hue()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_hue &amp;lt;- function(timestamp){
        
        df &amp;lt;- data.frame(log_time =as.POSIXct(timestamp)) %&amp;gt;% 
                add_vars(extra_var = &amp;quot;no&amp;quot;)
        
        pred &amp;lt;- predict(gbmFit, newdata = df)
        
        if (pred==&amp;quot;zero&amp;quot;) {
                x &amp;lt;- 0
        } else {
                x &amp;lt;- median_values %&amp;gt;% filter(y == pred &amp;amp; hour == lubridate::hour(timestamp)) %&amp;gt;%
                select(med) %&amp;gt;% unlist()
        }
        
        return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to expose the above function as an API, we literally need three lines of code with &lt;a href=&#34;https://cran.r-project.org/web/packages/jug/vignettes/jug.html&#34;&gt;jug&lt;/a&gt;. Ever since I saw the package &lt;a href=&#34;https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/jug-Building-Web-APIs-for-R&#34;&gt;presented at useR2017&lt;/a&gt;, I have been looking for a use case to play with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jug)
jug() %&amp;gt;% post(&amp;quot;/predict-hue&amp;quot;, decorate(predict_hue)) %&amp;gt;%
        simple_error_handler_json() %&amp;gt;%
        serve_it()
#Serving the jug at http://127.0.0.1:8080&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great. We can now make calls to this API and get a prediction supplying the current time. The nifty thing is that because API’s are language agnostic, we can access it from the programming paradigm of our choosing. I currently have a basic Python function that communicates with both API’s, transferring a prediction to the Hue Bridge every 5 minutes. But we could just as well build a whole interface on top, or create a chatbot for improved user experience. Perhaps I’ll do a follow-up post on this topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;There is something very satisfying about building your own data products and seeing them at work. Even something as trivial as controlling the light switch for you. I only have data since this year May, so there’s a good chance our model will get smarter as days go by. We can easily schedule to retrain the model every week or even day, without having to lift a finger. Most of the code in this post is packaged up as a handful of &lt;code&gt;R&lt;/code&gt; functions deployed on my Raspberry Pi. Now, when I &lt;em&gt;choose&lt;/em&gt; to pass out on my couch next time, at least lights won’t stay on for too long.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>