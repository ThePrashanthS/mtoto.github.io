<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Coding with Data</title>
    <link>/tags/r/index.xml</link>
    <description>Recent content in R on Coding with Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Dockerized Shiny App development</title>
      <link>/blog/dockerized-shiny-app-development/</link>
      <pubDate>Tue, 16 Jan 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/dockerized-shiny-app-development/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;getting-on-the-docker-container-ship&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting on the Docker (container) ship&lt;/h2&gt;
&lt;p&gt;Containers are everywhere, including the realms of data science. You can think of them as small self-contained environments, encapsulating an application and its dependencies. If that sounds a lot like a virtual machine, you are not entirely wrong. But unlike VM‚Äôs, containers run on the host system‚Äôs kernel and the processes inside can only see and access their immediate surroundings.&lt;/p&gt;
&lt;p&gt;Thanks to the good people behind the &lt;a href=&#34;https://hub.docker.com/u/rocker/&#34;&gt;rocker project&lt;/a&gt;, there‚Äôs already plenty of R-specific Docker images available for folks looking to containerize their R code. The most often cited benefits are &lt;em&gt;portability&lt;/em&gt; and &lt;em&gt;reproducibility&lt;/em&gt; of your analysis. In the same vein, &lt;a href=&#34;https://maraaverick.rbind.io/2017/11/docker-izing-your-work-in-r/&#34;&gt;lots of great material&lt;/a&gt; is out there with respect to what these bad boys exactly are and how to get them up and running.&lt;/p&gt;
&lt;p&gt;But I haven‚Äôt found much on &lt;em&gt;Docker based workflows&lt;/em&gt;, especially how to go about developing dockerized shiny apps. Because what if I want to build a shiny dashboard inside a container, integrate it with &lt;a href=&#34;https://travis-ci.org/&#34;&gt;Travis CI&lt;/a&gt; and run tests on every single commit to GitHub? The code in this post is based on a bare bones shiny app (containing USA Trade data) I built for illustration purposes. You can find the app &lt;a href=&#34;http://usatrade.tamaszilagyi.com/&#34;&gt;here&lt;/a&gt;, and all the code on &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testable-shiny-apps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testable shiny apps&lt;/h2&gt;
&lt;p&gt;We all heard of unit testing, but can we test an actual shiny application? As often the case in the R world, &lt;em&gt;there is already a package for that:&lt;/em&gt; &lt;a href=&#34;https://github.com/rstudio/shinytest&#34;&gt;shinytest&lt;/a&gt; - an automated testing agent for, you guessed it‚Ä¶shiny apps. It works as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Shinytest uses snapshot-based testing strategy. The first time it runs a set of tests for an application, it performs some scripted interactions with the app and takes one or more snapshots of the application‚Äôs state. These snapshots are saved to disk so that future runs of the tests can compare their results to them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The interface could not be easier. You install the package and when the first version of your shiny app is ready to roll, you simply run &lt;code&gt;recordTest()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/shinytest&amp;quot;)
library(shinytest)

recordTest(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This launches an iframe consisting of your dashboard and controls over what to test. Each interaction with the dashboard is recorded, and when you hit &lt;strong&gt;take snapshot&lt;/strong&gt;, the state of your dashboard is saved, along with raw scripts to reproduce the interactions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://i.imgur.com/t3xcuCX.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Upon exiting the test event recorder, a new folder &lt;code&gt;test/&lt;/code&gt; is created inside the app‚Äôs directory, containing both the test script - &lt;code&gt;dates.R&lt;/code&gt;, as well as the application‚Äôs state as a .json and a .png file respectively in &lt;code&gt;test/dates-expected&lt;/code&gt;. The latter serve as the expected output, based on which consequent runs of tests will be evaluated. Using my example app, &lt;code&gt;dates.R&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(shiny.testmode=TRUE)

app &amp;lt;- ShinyDriver$new(&amp;quot;../&amp;quot;, seed = 123)
app$snapshotInit(&amp;quot;dates&amp;quot;)

app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, running &lt;code&gt;testApp(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt; will look for test scripts inside the &lt;code&gt;test/&lt;/code&gt; folder, and run them to recreate the state of the test recording, comparing the output to what‚Äôs expected. It is generally a good idea to only compare the .json files, because the screenshots of the app (the .png file) will likely differ of various systems. We pass the argument &lt;code&gt;compareImages = FALSE&lt;/code&gt; to bypass default behavior. A full fledged test script will then look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(testthat)
test_that(&amp;quot;Application works&amp;quot;, {
        expect_pass(testApp(&amp;quot;/srv/shiny-server/myapp/&amp;quot;,
                            testnames = &amp;quot;dates&amp;quot;,
                            compareImages = FALSE))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I found that having ggplot2 (or plotly) plots as part of your dashboard, there is always a tiny bit of randomness present in the output. And so the tests fail. It is better to explicitly export parts of the plot objects in my opinion, because they will be a more reliable yardstick to compare against. To do so, we add a few lines of code to &lt;code&gt;server.R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exportTestValues(plot_balance = { ggplot_build(p_b)$data },
                 plot_total   = { ggplot_build(p_t)$data },
                 plot_import  = { ggplot_build(p_i)$data },
                 plot_export  = { ggplot_build(p_e)$data } )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Secondly, we customize which parts of the application‚Äôs state should be saved and checked for inside &lt;code&gt;app$snapshot()&lt;/code&gt;, using the &lt;code&gt;items =&lt;/code&gt; argument and update &lt;code&gt;dates.R&lt;/code&gt; so that only the &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;export&lt;/code&gt; parts of our .json files are evaluated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;...
app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot(items = list(input = TRUE, export = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is all you really need to get going with shinytest. Keep in mind that the package is still in development, and things might change in the future. For an in-depth walkthrough of shinytest‚Äôs capabilities, have a look at the &lt;a href=&#34;https://rstudio.github.io/shinytest/articles/shinytest.html&#34;&gt;official site&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-container-can-we-has-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A üê≥ container, can we has it?&lt;/h2&gt;
&lt;p&gt;Now that our shiny app is complete with test scripts, the whole thing can be packaged up and put inside a container. Of course we could deploy the shiny dashboard without a container too, but at the end of the day it makes everybody‚Äôs life a lot easier.&lt;/p&gt;
&lt;p&gt;Because if our container runs on our machine, it will also run on &lt;strong&gt;any machine&lt;/strong&gt; that has Docker. Without compatibility issues, independent from host version or platform distribution. In a real life scenario this significantly reduces time between prototypting and deployment, not the least because of the typically lightweight footprint of a Docker image.&lt;/p&gt;
&lt;p&gt;To containerize our shiny app, we first need to create an image that encompasses our:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Shiny application&lt;/li&gt;
&lt;li&gt;R packages our app needs&lt;/li&gt;
&lt;li&gt;System level dependencies these packages need&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We build our image layer by layer, starting with the &lt;a href=&#34;https://hub.docker.com/r/rocker/shiny/&#34;&gt;rocker/shiny image&lt;/a&gt; - which includes the minimal requirements for a Shiny Server. Then, we add everything else our application requires; finishing with copying the contents of our app to &lt;code&gt;/srv/shiny-server/usa-trade/&lt;/code&gt;, where the dashboard will be served from. These instructions are written to the &lt;code&gt;Dockerfile&lt;/code&gt;, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FROM rocker/shiny
MAINTAINER Tamas Szilagyi (tszilagyi@outlook.com)

## install R package dependencies (and clean up)
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2 \
    libssl-dev \
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## install packages from CRAN (and clean up)
RUN Rscript -e &amp;quot;install.packages(c(&amp;#39;devtools&amp;#39;,&amp;#39;dplyr&amp;#39;,&amp;#39;tidyr&amp;#39;,&amp;#39;fuzzyjoin&amp;#39;,&amp;#39;stringr&amp;#39;,&amp;#39;ggthemes&amp;#39;,&amp;#39;quantmod&amp;#39;,&amp;#39;ggplot2&amp;#39;,&amp;#39;shinydashboard&amp;#39;,&amp;#39;shinythemes&amp;#39;), repos=&amp;#39;https://cran.rstudio.com/&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install packages from github (and clean up)
RUN Rscript -e &amp;quot;devtools::install_github(&amp;#39;rstudio/shinytest&amp;#39;,&amp;#39;rstudio/webdriver&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install phantomjs
RUN Rscript -e &amp;quot;webdriver::install_phantomjs()&amp;quot;

## assume shiny app is in build folder /app2
COPY ./app2 /srv/shiny-server/usa-trade/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The smaller your Docker image, the better. Here‚Äôs a couple of guidelines to keep in mind when creating one:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Always use &lt;strong&gt;shared base images&lt;/strong&gt; (what comes after the &lt;code&gt;FROM&lt;/code&gt; statement) specific to your application, instead of trying to reinvent the wheel every time you write a Dockerfile.&lt;/li&gt;
&lt;li&gt;Try to &lt;strong&gt;avoid underused dependencies&lt;/strong&gt;. Going back to the my example app, I could‚Äôve installed the package &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyquant/index.html&#34;&gt;tidyquant&lt;/a&gt; to get my trade data in a tidy format out of the box, yet because the package has an insane amount of dependencies (including having Java installed); I wrote three &lt;a href=&#34;https://github.com/mtoto/markets_shiny/blob/master/app2/functions.R#L14&#34;&gt;helper functions&lt;/a&gt; instead.&lt;/li&gt;
&lt;li&gt;Make sure &lt;strong&gt;temporary files are removed&lt;/strong&gt; after the installation of libraries and packages.&lt;/li&gt;
&lt;li&gt;Push down commands that will likely invalidate the &lt;strong&gt;cache&lt;/strong&gt;, so Docker only rebuilds layers that change (more on this in the next section).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the Dockerfile finished, it is time to make ourselves familiar with the essential Docker commands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;docker pull&lt;/strong&gt; pulls an image from the registry (Dockerhub).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker build&lt;/strong&gt; builds a docker image from our Dockerfile.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run&lt;/strong&gt; instantiates the container from our image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker exec&lt;/strong&gt; execute commands from within the container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker rm&lt;/strong&gt; deletes a container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker login&lt;/strong&gt; login to Dockerhub (to upload our image).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker push&lt;/strong&gt; uploads the image back to Dockerhub.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let‚Äôs say we want to run our shiny app on a server that has Docker installed. Assuming we have a &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub repo&lt;/a&gt; containing all relevant files and our Dockerfile is to be found on &lt;a href=&#34;https://hub.docker.com/r/mtoto/shiny/&#34;&gt;Dockerhub&lt;/a&gt;, we can expose our shiny app to the world as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1 clone into repo containing app 
git clone https://github.com/mtoto/markets_shiny.git
# 2 pull Docker file from Dockerhub
docker pull mtoto/shiny:latest
# 3 build Docker image, tag it &amp;#39;mtoto/shiny:latest&amp;#39;
docker build -t mtoto/shiny:latest .
# 4 run container in detached mode, listening on port 80, name it &amp;#39;site&amp;#39;
docker run -d -p 80:3838 --name site mtoto/shiny:latest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And our app should be visible on &lt;em&gt;ht‚Äãps://myserver.com/usa-trade&lt;/em&gt; by default.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integration-with-travis-ci&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integration with Travis CI&lt;/h2&gt;
&lt;p&gt;If you are a seasoned R package developer, you are no stranger to Travis CI. It is a &lt;strong&gt;Continuous Integration&lt;/strong&gt; tool that automatically performs checks and runs tests on your code every time you push a commit to GitHub. The broad idea behind continuous integration is to encourage test-driven development, thereby allowing for frequent commits to the codebase without having to worry about integration problems.&lt;/p&gt;
&lt;p&gt;Travis supports many languages - including R, and can also build from Docker images. After creating an account on the &lt;a href=&#34;https://travis-ci.org&#34;&gt;Travis website&lt;/a&gt;, connect with GitHub and pick the repository for which you‚Äôd like to use it.&lt;/p&gt;
&lt;p&gt;The repo needs to contain a &lt;code&gt;.travis.yml&lt;/code&gt; file, encapsulating the instructions for Travis. You‚Äôd tempted to write &lt;code&gt;language: R&lt;/code&gt; as the first line, but if we do that Travis will implicitly assume we are developing an R package and will start looking for the &lt;code&gt;DESCRIPTION&lt;/code&gt; file we do not have. Instead, I went with the undocumented option &lt;code&gt;language: generic&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, as we‚Äôll be only running Docker commands anyway.&lt;/p&gt;
&lt;p&gt;The naive approach would be to build our Docker image on every single run, instantiate a test container, run tests inside and upon success get rid of the container. Such a &lt;code&gt;.travis.yml&lt;/code&gt; would look like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker build -t markets-shiny .
- docker run -d -p 3838:3838 markets-shiny:latest --name test

script:
- docker exec test R -f run_tests.R

after_script:
- docker rm -f test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem here is that we are building the Docker image from scratch with every single Travis run, resulting in a build time of over 20 minutes for my simple app. But our image is on Dockerhub, so why not pull it from there and take advantage of caching. Then, we‚Äôd only rebuild the changed layers after downloading the image from Dockerhub.&lt;/p&gt;
&lt;p&gt;To make sure everything is nice and up to date, we will push the changes back to Dockerhub after every successful run. We need credentials to do so, but Travis conveniently allows for defining environment variables inside the repository settings (or via the CLI):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://i.imgur.com/gdTnLjd.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Now we can go wild and revamp &lt;code&gt;.travis.yml&lt;/code&gt; accordingly:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker pull mtoto/shiny:latest
- docker build --cache-from mtoto/shiny:latest -t mtoto/shiny:latest . 
- docker run --rm -d -p 3838:3838 --name test mtoto/shiny:latest

script:
- docker exec test R -f /srv/shiny-server/usa-trade/run_tests.R

after_success:
- docker rm -f test
- docker login -u mtoto -p $DOCKER_PASSWORD
- docker tag mtoto/shiny:latest mtoto/shiny:$TRAVIS_BUILD_NUMBER
- docker push mtoto/shiny&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the second run (once the latest image is on Dockerhub), the build time is reduced by a factor of 10. Sweet. When we use the flag &lt;code&gt;--cache-from&lt;/code&gt;, Docker only rebuilds changed layers, ie. modifications to our shiny app. We can see this in the Travis log as &lt;code&gt;---&amp;gt; Using cache&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://i.imgur.com/hTNuQhY.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Keep in mind when making significant changes to your dashboard, it is important to update the tests that create fresh expected outputs reflecting these changes. If you don‚Äôt trust the outputs will align, remember to use &lt;code&gt;exportTestValues()&lt;/code&gt; and fill it up with the new objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;was-it-all-worth-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Was it all worth it?&lt;/h2&gt;
&lt;p&gt;While this workflow might feel like over-engineering, once all the tools are set up to work in tandem, shiny dashboard development becomes surprisingly efficient. The icing on the cake is that you are creating a dashboard that is pretty much ready for deployment from day one. Devops will love you for it, trust me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In reality this points to &lt;code&gt;language: bash&lt;/code&gt;, &lt;code&gt;language: sh&lt;/code&gt; and &lt;code&gt;language: shell&lt;/code&gt;.&lt;a href=&#34;#fnref1&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A tidy text analysis of Rick and Morty</title>
      <link>/blog/a-tidy-text-analysis-of-rick-and-morty/</link>
      <pubDate>Sat, 07 Oct 2017 23:15:14 -0500</pubDate>
      
      <guid>/blog/a-tidy-text-analysis-of-rick-and-morty/</guid>
      <description>&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://i.imgur.com/a841k9g.gif&#34; /&gt;

&lt;/div&gt;
&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;adventures-in-the-multiverse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adventures in the multiverse&lt;/h2&gt;
&lt;p&gt;For those unfamiliar with the show, Rick and Morty is an animated series about the interuniversal exploits of a half-drunk mad scientist Rick, and his daft grandson Morty. Living under one roof with his daughter, Rick constantly drags his grandson Morty along for adventures into unusual worlds inhabited by surreal creatures. At first hesitant to accompany his eccentric granddad, Morty slowly grows into an indispensable sidekick. Using Rick‚Äôs portal gun, they leave the rest of their dysfunctional family at home, and travel through space and time.&lt;/p&gt;
&lt;p&gt;Most episodes draw inspiration from or make fun of cult movies such as Back to the Future, A Nightmare on Elm Street, Inception and many other classics by the likes of John Carpenter or David Cronenberg. Besides the ruthless humor and over-the-top visual scenery, the show brilliantly builds independent sci-fi realms, going about their day-to-day according to their wacky rules.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-mans-weekend-project-another-mans-treasure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One man‚Äôs weekend project, another man‚Äôs treasure&lt;/h2&gt;
&lt;p&gt;After reading the book &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;Tidy Text Mining&lt;/a&gt; online, I have been wanting to try out some of the concepts outlined in the book, and the functions of the &lt;a href=&#34;https://github.com/juliasilge/tidytext&#34;&gt;accompanying package&lt;/a&gt;, on an interesting dataset. So I was pretty stoked to find &lt;a href=&#34;https://github.com/fkeck/subtools&#34;&gt;Francois Keck‚Äôs &lt;strong&gt;subtools package&lt;/strong&gt; on GitHub&lt;/a&gt;, that allows for reading &lt;em&gt;.srt&lt;/em&gt; files (the usual format for subtitles) straight into R. With season 3 of Rick and Morty coming to an end last week, the stars have finally aligned to roll up my sleeves and have some fun with text mining.&lt;/p&gt;
&lt;p&gt;It is very easy to find English subtitles for pretty much anything on the Internet. With subtools, an entire series can be read with one command from the containing folder, &lt;code&gt;read.subtitles.serie()&lt;/code&gt;. We convert the resulting MultiSubtitles object to a data.frame with a second command &lt;code&gt;subDataFrame()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(subtools)
a &amp;lt;- read.subtitles.serie(dir = &amp;quot;/series/rick and morty/&amp;quot;)
df &amp;lt;- subDataFrame(a)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Read: 3 seasons, 31 episodes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    16821 obs. of  8 variables:
##  $ ID          : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
##  $ Timecode.in : chr  &amp;quot;00:00:02.445&amp;quot; &amp;quot;00:00:03.950&amp;quot; &amp;quot;00:00:05.890&amp;quot; &amp;quot;00:00:07.420&amp;quot; ...
##  $ Timecode.out: chr  &amp;quot;00:00:03.850&amp;quot; &amp;quot;00:00:05.765&amp;quot; &amp;quot;00:00:07.295&amp;quot; &amp;quot;00:00:08.925&amp;quot; ...
##  $ Text        : chr  &amp;quot;Morty, you got to... come on.&amp;quot; &amp;quot;- You got to come with me. - Rick, what&amp;#39;s going on?&amp;quot; &amp;quot;I got a surprise for you, Morty.&amp;quot; &amp;quot;It&amp;#39;s the middle of the night. What are you talking about?&amp;quot; ...
##  $ season      : chr  &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; ...
##  $ season_num  : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ episode_num : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ serie       : chr  &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;$Text&lt;/code&gt; column contains the subtitle text, surrounded by additional variables for line id, timestamp, season and episode number. This is the structure preferred by the tidytext package, as it is by the rest of tidyverse.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;morty-you-got-tocome-on.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;‚ÄúMorty, you got to‚Ä¶come on.‚Äù&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Let‚Äôs start with the bread and butter of text mining, &lt;em&gt;term frequencies&lt;/em&gt;. We split the text by word, exclude stop words,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(stop_words)
tidy_df &amp;lt;- df %&amp;gt;%
  unnest_tokens(word, Text) %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and aggregate and plot the top 10 words per season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)

tidy_df %&amp;gt;% group_by(season) %&amp;gt;%
        count(word, sort = TRUE) %&amp;gt;%
        top_n(10) %&amp;gt;%
        ggplot(aes(reorder(word,n), n, fill = season)) +
        geom_col() +
        coord_flip() +
        facet_wrap(~season, scales = &amp;quot;free_y&amp;quot;) +
        labs(x = NULL) +
        guides(fill = FALSE) +
        scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-6-1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Both seasons are dominated by, well, Rick and Morty. The main characters are tirelessly addressing each other, talking one another either into or out of the mess they find themselves in. What stands out most is the absence of Rick‚Äôs daughter, Beth from the top 10 in all seasons. She‚Äôs perhaps the only sane person of the family, but then again, sanity doesn‚Äôt get too much airtime on this show.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;network-analysis-on-bi-grams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Network analysis on bi-grams&lt;/h2&gt;
&lt;p&gt;We can similarly get the number of times each &lt;em&gt;two words&lt;/em&gt; appear, called &lt;em&gt;bi-grams&lt;/em&gt;. Besides calculating summary statistics on bi-grams, we can now construct a network of words according to co-occurrence using &lt;a href=&#34;https://cran.r-project.org/web/packages/igraph/index.html&#34;&gt;igraph&lt;/a&gt;, the go-to package for network analysis in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(igraph)

bigram_graph &amp;lt;- df %&amp;gt;%
  unnest_tokens(bigram, Text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;%
  filter(!word1 %in% stop_words$word) %&amp;gt;%
  filter(!word2 %in% stop_words$word) %&amp;gt;% 
  group_by(season) %&amp;gt;%
  count(word1, word2, sort = TRUE) %&amp;gt;%
  select(word1, word2, season, n) %&amp;gt;%
  filter(n &amp;gt; 2) %&amp;gt;%
  graph_from_data_frame()

print(bigram_graph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IGRAPH 7d93551 DN-- 310 280 -- 
## + attr: name (v/c), season (e/c), n (e/n)
## + edges from 7d93551 (vertex names):
## [1] tiny        -&amp;gt;rick    yeah        -&amp;gt;yeah    ice         -&amp;gt;cream  
## [4] god         -&amp;gt;damn    whoa        -&amp;gt;whoa   
##  [ reached getOption(&amp;quot;max.print&amp;quot;) -- omitted 20 entries ]
## + ... omitted several edges&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This igraph object contains a directed network, where the vertices are the words and an edge exists between each that appear after one another more than twice. Representing the text as a graph, we can calculate things such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Centrality#Degree_centrality&#34;&gt;degree centrality&lt;/a&gt;, and plot the results.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-9-1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Looking at the largest connected network, we arrive at the same conclusion as with term frequencies. Rick and Morty are the most important words. They are at the center of the network and so have the highest degree centrality scores.&lt;/p&gt;
&lt;p&gt;Besides visualising the importance of words in our network, we can similarly differentiate between words that precede either Rick or Morty. These are all the 1st degree connections (words) that have an edge pointing towards the main characters, but aren‚Äôt shared among the them.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-10-1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Looking at the red nodes, we recognize many of the things Rick throws at Morty: &lt;em&gt;‚ÄúRelax Morty!‚Ä¶It‚Äôs science Morty!‚Ä¶Run Morty!‚Äù&lt;/em&gt;. There is also a handful of words that precede both characters like &lt;em&gt;‚ÄúGeez‚Äù, ‚ÄúBoy‚Äù&lt;/em&gt; or &lt;em&gt;‚ÄúGod‚Äù&lt;/em&gt;. All other words that are more than one degree away, are colored blue as out of range.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tf-idf&lt;/h2&gt;
&lt;p&gt;Thus far we have looked at all words across seasons. But where do the seasons differ from each other? And can we summarise each season using a handful of topics? To answer the first question, text mining‚Äôs most notorious statistic &lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;&lt;strong&gt;tf-idf&lt;/strong&gt;&lt;/a&gt; comes to the rescue. It stands for &lt;strong&gt;term frequency - inverse document frequency&lt;/strong&gt;. We take the word counts per season and multiply it by the &lt;em&gt;scaled inverse fraction of seasons that contain the word&lt;/em&gt;. Simply put, we penalize words that are common across all seasons, and reward ones that are not. This way, we bring forth the words most typical of each season. Again the tidytext implementation is super easy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tf_idf_df &amp;lt;- tidy_df %&amp;gt;% 
        count(season, word, sort = TRUE) %&amp;gt;%
        bind_tf_idf(word, season, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-12-1.png&#34; /&gt; What we get back are the most important elements, characters, motives or places across episodes. I‚Äôm somewhat surprised that Mr.¬†Meeseeks didn‚Äôt come in first though. I was sure as hell annoyed out of my mind after hearing it uttered for the 100th time during the episode &lt;a href=&#34;https://en.wikipedia.org/wiki/Meeseeks_and_Destroy&#34;&gt;Meeseeks and Destroy&lt;/a&gt;. But then again, Mr Meeseeks does make a cameo in two other seasons, so that kind of torpedoes his chances for the first spot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic models&lt;/h2&gt;
&lt;p&gt;Having seen the most unique words of the script by seasons, we will take our analysis one last step further and try to capture the gist of a the show using topic modeling. Broadly speaking, it‚Äôs an unsupervised classification method that tries to represent a document as a collection of topics. Here, I will take the classic &lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34;&gt;Latent Dirichlet Allocation or shortly LDA&lt;/a&gt; algorithm for a spin. The basic idea is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚Äú‚Ä¶a topic is defined as a mixture over words where each word has a probability of belonging to a topic. And a document is a mixture over topics, meaning that a single document can be composed of multiple topics.‚Äù&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We could for example take season two, and tell &lt;code&gt;LDA()&lt;/code&gt; that we want to compress 10 episodes into just 6 topics. To compensate for the omnipresence of the top words across episodes, I will exclude them for the purpose of clearer separation of topics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(topicmodels)
popular_words &amp;lt;- c(&amp;quot;rick&amp;quot;,&amp;quot;morty&amp;quot;, &amp;quot;yeah&amp;quot;,&amp;quot;hey&amp;quot;,
                   &amp;quot;summer&amp;quot;, &amp;quot;jerry&amp;quot;, &amp;quot;uh&amp;quot;, &amp;quot;gonna&amp;quot;)

episodes_dtm &amp;lt;- tidy_df %&amp;gt;% filter(season_num == 2 &amp;amp; !word %in% popular_words) %&amp;gt;%
        group_by(episode_num) %&amp;gt;%
        count(word, sort = TRUE) %&amp;gt;%
        cast_dtm(episode_num, word, n) 

episodes_lda &amp;lt;- LDA(episodes_dtm, k = 6, control = list(seed = 1234))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After &lt;code&gt;tidy()&lt;/code&gt;ing the results, we can plot the top 10 words that contribute (&lt;em&gt;beta&lt;/em&gt;) to most to each topic.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-14-1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;There‚Äôs definitely a few topics that contain multiple elements of a particular episode. Take for example &lt;strong&gt;topic 1&lt;/strong&gt;. It includes ‚Äú&lt;em&gt;Roy&lt;/em&gt;‚Äù, the name of the videogame Morty plays in the same episode ‚Äú&lt;em&gt;Fart&lt;/em&gt;‚Äù appears, a gaseous creature kept under locks by aliens. Or &lt;strong&gt;topic 5&lt;/strong&gt;, which probably relates to the episode where Rick visits his old lover ‚Äú&lt;em&gt;Unity&lt;/em&gt;‚Äù. It further contains words as ‚Äú&lt;em&gt;remember&lt;/em&gt;‚Äù and ‚Äú&lt;em&gt;memories&lt;/em&gt;‚Äù. The episode ends with Unity repeating ‚ÄúI want it &lt;em&gt;real&lt;/em&gt;‚Äù.&lt;/p&gt;
&lt;p&gt;Not only can we examine the &lt;strong&gt;word per topic probabilities&lt;/strong&gt;, we can also plot &lt;strong&gt;the topic per document probabilities&lt;/strong&gt;, or &lt;em&gt;gamma&lt;/em&gt; values. This lets us see what topic belongs to what episode.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(episodes_lda, matrix = &amp;quot;gamma&amp;quot;) %&amp;gt;%
        inner_join(titles) %&amp;gt;%
        ggplot(aes(factor(topic), gamma)) +
        geom_boxplot() +
        facet_wrap(~ title) +
        ggtitle(&amp;quot;Dominant Topics per Episode&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-16-1.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Our previous assumptions are confirmed, the first topic does belong to the episode &lt;em&gt;Mortynight Run&lt;/em&gt; as does the fifth topic to &lt;em&gt;Auto-Erotic Assimilation&lt;/em&gt;. It is important to note that the results strongly depend on the number of topics supplied to &lt;code&gt;LDA()&lt;/code&gt;, so inevitably, some experimentation is required to arrive at meaningful results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;I ran through some very interesting concepts fairly quickly in this post. I owe much of it to the tidytext package. With very little coding, we can mine a tremendous amount of insights from textual data. And I have just scrachted the surface of what‚Äôs possible. The seamless integration with the tidyverse, as with igraph and topicmodels does make a huge difference.&lt;/p&gt;
&lt;p&gt;Nonetheless, text mining is a complex topic and when arriving at more advanced material, &lt;a href=&#34;https://github.com/trinker/topicmodels_learning&#34;&gt;further reading&lt;/a&gt; on the inner workings of these algorithms might come in handy for effective use. The full &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/tree/master/data/2017-10-07-tidyrick/rick%20and%20morty&#34;&gt;data&lt;/a&gt; and &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/blog/2017/2017-10-07-tidyrick.Rmd&#34;&gt;code&lt;/a&gt; for this post is available as usual on my Github.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Self-learning Hue Lights</title>
      <link>/blog/self-learning-hue-lights/</link>
      <pubDate>Wed, 30 Aug 2017 23:15:14 -0500</pubDate>
      
      <guid>/blog/self-learning-hue-lights/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;the-rise-of-the-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The rise of the API&lt;/h2&gt;
&lt;p&gt;Rich API interfaces are one of the main ingredients of today‚Äôs smart devices. They are by definition built for interconnectivity and there is an active community of developers creating apps as microservices on top of them. Philips Hue is no exception with it‚Äôs wide variety of &lt;a href=&#34;http://www.developers.meethue.com/otherapps/otherAppsIOS.html#appsList&#34;&gt;apps&lt;/a&gt; available to users.&lt;/p&gt;
&lt;p&gt;But you don‚Äôt need to code an entire mobile application to take advantage of the low level access. Using modern tools it only takes a few lines of code to build a self-learning algorithm, running in production in your home. Not only can we access external API‚Äôs, we can just as easily expose static files, functions or models as an API of our own.&lt;/p&gt;
&lt;p&gt;My original inspiration for this post was &lt;a href=&#34;https://sc5.io/posts/autonomous-indoor-lighting-using-neural-networks/&#34;&gt;Max Pagel‚Äôs article&lt;/a&gt; on training a neural network to automatically control his Philips Hue lights. In fact, I purchased my first set of Hue bulbs because of it. In summary, this post will describe how to build and productionize a classifier in &lt;code&gt;R&lt;/code&gt; that controls the brightness of Philips Hue lights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-dinnertime&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stop, dinnertime!&lt;/h2&gt;
&lt;p&gt;Much like in my &lt;a href=&#34;http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/&#34;&gt;post on Spotify&lt;/a&gt; I have set up a cronjob to execute the Python script that pings the API and saves the lights‚Äô state data locally, to be picked up by Luigi tasks for parsing and copying to S3 further downstream. You can find the relevant code on my &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/tasks.py&#34;&gt;Github&lt;/a&gt;. The &lt;a href=&#34;https://www.developers.meethue.com/philips-hue-api&#34;&gt;Hue API documentation&lt;/a&gt; contains information on authentication and the types of calls available.&lt;/p&gt;
&lt;p&gt;The starting point for this post will be the parsed &lt;code&gt;.json&lt;/code&gt; file containing all of the log data for my ‚ÄúDinner Lamps‚Äù. They are the two main lights in my living and dining area room at the moment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aws.s3)
library(jsonlite)
# read file from amazon
aws.signature::use_credentials()
df &amp;lt;- s3read_using(object = paste0(&amp;quot;hue_full_2017-08-26.json&amp;quot;), fromJSON, bucket = &amp;quot;ams-hue-data&amp;quot;)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    30322 obs. of  15 variables:
##  $ on.1       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ on.2       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ bri.2      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ type.1     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ type.2     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ bri.1      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ modelid.2  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ modelid.1  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name.1     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; ...
##  $ reachable.1: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ reachable.2: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ name.2     : chr  &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ alert.1    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ log_time   : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:10:02&amp;quot; &amp;quot;2017-05-12 17:15:01&amp;quot; ...
##  $ alert.2    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the original &lt;code&gt;.json&lt;/code&gt; file is such that each lamp has a separate (numbered) column for every variable. The dataset is essentially a timeseries where each row represent a snapshot of the lamps‚Äô state at &lt;code&gt;$log_time&lt;/code&gt;, or &lt;strong&gt;every 5 minutes&lt;/strong&gt;. Before moving on, let‚Äôs tidy things up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
tidy_df &amp;lt;- df %&amp;gt;% gather(key, value, -log_time) %&amp;gt;%
        separate(key, into = c(&amp;quot;variable&amp;quot;, &amp;quot;lamp&amp;quot;), sep = &amp;quot;\\.&amp;quot;) %&amp;gt;%
        spread(variable, value)
str(tidy_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    60644 obs. of  9 variables:
##  $ log_time : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; ...
##  $ lamp     : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; ...
##  $ alert    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ bri      : chr  &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; ...
##  $ modelid  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ on       : chr  &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; ...
##  $ reachable: chr  &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; ...
##  $ type     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 15 columns are now reduced to 9 because each variable appears only once thanks to adding the key column &lt;code&gt;$lamp&lt;/code&gt; to the dataset. But we are not quite done cleaning yet: I use the two lamps in sync, so we need only data from one of them. When the lamps are not &lt;code&gt;on&lt;/code&gt; nor &lt;code&gt;reachable&lt;/code&gt;, &lt;code&gt;$bri&lt;/code&gt; should be set to &lt;code&gt;0&lt;/code&gt;. Using the now correct brightness values, we create the four categories for the classifier to work with. Lastly, there were days I wasn‚Äôt home, so we can rid of of those observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
binned_df &amp;lt;- tidy_df %&amp;gt;% filter(lamp == &amp;quot;1&amp;quot;) %&amp;gt;%
        mutate(bri = as.numeric(replace(bri, on==&amp;quot;FALSE&amp;quot; | reachable==&amp;quot;FALSE&amp;quot;,0)),
               y = as.factor(ifelse(bri == 0, &amp;quot;zero&amp;quot;,
                                    ifelse(between(bri,0,80), &amp;quot;dim&amp;quot;,
                                           ifelse(between(bri,80,160),&amp;quot;mid&amp;quot;,&amp;quot;bright&amp;quot;)))))

off_days &amp;lt;- binned_df %&amp;gt;% group_by(date = as.Date(log_time,tz=&amp;quot;Europe/Amsterdam&amp;quot;)) %&amp;gt;%
                dplyr::summarise(total_bri = sum(bri)) %&amp;gt;%
                filter(total_bri == 0 ) %&amp;gt;%
                select(date)

binned_df &amp;lt;- binned_df %&amp;gt;% filter(!as.Date(log_time) %in% off_days$date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of our target variable look?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(binned_df$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## bright    dim    mid   zero 
##    598   1533   1710  23889&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Roughly 86% of the time the lamps are off, resulting in an unbalanced dataset. What about brightness values lamps were &lt;em&gt;on&lt;/em&gt;, according to the three remaining categories?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems to be close to normal with a positive skew, and a massive outlier all the way at the end of the spectrum. That‚Äôs maximum brightness, the default when I switch the lights on/off with a physical switch.&lt;/p&gt;
&lt;p&gt;To get an intuition for my usage patterns, I‚Äôll also plot a histogram of hour of the day for all four categories.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The only times the lamps are &lt;strong&gt;not&lt;/strong&gt; structurally off, is in the evening and the early hours. Dim and mid values are the dominant category after 8PM instead. Another slight dip in zero appears around and shortly after midnight, compensated by the second largest peak in dim, and a few instances of mid and bright. Bright observations in general are sparse and will be tough to predict.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-we-learn-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can we learn this?&lt;/h2&gt;
&lt;p&gt;The only variables I will use for training, are time based: &lt;em&gt;day of the week&lt;/em&gt;; &lt;em&gt;month&lt;/em&gt;; &lt;em&gt;week number&lt;/em&gt;; &lt;em&gt;weekend or not&lt;/em&gt;; &lt;em&gt;time of the day&lt;/em&gt;; and &lt;em&gt;minutes since 12PM, 6AM, 12AM and 6PM&lt;/em&gt;. A datetime string will then suffice to generate a prediction on the fly, a boon for putting things into production later on. I packaged a chain of dplyr commands inside the function &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/functions.R&#34;&gt;add_vars()&lt;/a&gt; to add the above variables to the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_vars &amp;lt;- binned_df %&amp;gt;% add_vars(extra_var = &amp;quot;yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember we are dealing with an unbalanced dataset, that also happens to be a timeseries. To remedy the former, I will use class weights to penalize the misclassification of the minority classes. Secondly, I will look at the &lt;em&gt;Area Under the Curve&lt;/em&gt; to evaluate the model, which will be less affected than &lt;em&gt;Accuracy&lt;/em&gt; by class imbalance if I set dim as the positive class. If not for these measures, the algorithm would gladly classify 100% of instances as &lt;code&gt;&amp;quot;zero&amp;quot;&lt;/code&gt;, achieving stunning accuracy on paper and complete darkness in my living room.&lt;/p&gt;
&lt;p&gt;Now, why does it matter that we have a timeseries? In any dataset with a time component, the split between train and test sets should not be random. Otherwise, the model can and will &lt;strong&gt;learn from the future&lt;/strong&gt;, and severely overfit the data. The correct cross-validation strategy instead is to fold the data according to time. Train should always the before and test the after. For our convenience &lt;code&gt;caret&lt;/code&gt; provides the &lt;code&gt;createTimeSlices()&lt;/code&gt; function to create the indices of the CV-folds. An extra &lt;code&gt;testing&lt;/code&gt; set will be held out to validate our model on unseen data after we are done modeling.&lt;/p&gt;
&lt;p&gt;We‚Äôll now train a &lt;a href=&#34;https://cran.r-project.org/web/packages/gbm/index.html&#34;&gt;gbm&lt;/a&gt; model, using the &lt;a href=&#34;https://topepo.github.io/caret/&#34;&gt;caret&lt;/a&gt; package, which comes with a myriad of convenience tools to make the process easier and the code a lot more concise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
# Split train and test sets
training &amp;lt;- df_vars[df_vars$date &amp;lt; &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date,-log_time)
testing &amp;lt;- df_vars[df_vars$date &amp;gt;= &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date)

# create cross validation folds
idx &amp;lt;- createTimeSlices(1:nrow(training), 
                      initialWindow = 15000, 
                      horizon = 5000, skip = 1000, fixedWindow = F)

# create model weights vector
model_weights &amp;lt;- ifelse(training$y == &amp;quot;zero&amp;quot;,0.2,
                        ifelse(training$y == &amp;quot;mid&amp;quot;,1.2,1))

# define cross validation logic
fitControl &amp;lt;- trainControl(## 10-fold CV
        index = idx[[1]],
        indexOut = idx[[2]],
        summaryFunction = multiClassSummary,
        classProbs = T)

# create tunegrid for hyperparameter search
gbmGrid &amp;lt;-  expand.grid(interaction.depth = c(1,3,5), 
                        n.trees = c(5,10,30), 
                        shrinkage = c(0.1),
                        n.minobsinnode = 5)

# train model
gbmFit &amp;lt;- train(y ~ ., data = training, 
                method = &amp;quot;gbm&amp;quot;, 
                trControl = fitControl,
                metric = &amp;quot;AUC&amp;quot;,
                weights = model_weights,
                verbose = FALSE,
                tuneGrid = gbmGrid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Printing &lt;code&gt;gbmFit&lt;/code&gt; to the console will give us the performance metrics across hyperparameters, and the ultimately selected values maximizing our metric of choice. While this is certainly useful information, I find it more intuitive to immediately look at the confusion matrix and see where our model is going off the rails:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds&amp;lt;-predict(gbmFit, testing)
table(preds, testing$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
## preds     dim  mid bright zero
##   dim      94  216     34  192
##   mid      72  135      6   86
##   bright    0    0      0    0
##   zero     54   92     25 4466&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most mistakes are made trying to classify bright and mid. The model gets 0 bright values right and only manages to do so correctly 14% of the time for mid. But when do errors happen? To dig a little deeper let‚Äôs look at the previous histogram of categories by hour again for the test set, but now with the predictions overlaid on top.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bright values were always going be hard to guess, but the model at least comes close in terms of hours, but off on the exact days. The majority of misclassification comes from overzealously predicting dim in the evening and around midnight, when it should really be either mid or zero. That looks like a workable scenario for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-ship-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Let‚Äôs ship it!&lt;/h1&gt;
&lt;p&gt;To control the lights, we can make PUT requests to the Hue bridge. To set &lt;em&gt;bri&lt;/em&gt;, we need actual brightness values. An intuitive option is to pick the median values per category per hour:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_values &amp;lt;- binned_df %&amp;gt;% filter(bri &amp;gt; 0) %&amp;gt;% 
                mutate(hour = lubridate::hour(as.POSIXct(log_time, tz = &amp;quot;Europe/Amsterdam&amp;quot;))) %&amp;gt;%
                select(hour,bri, y) %&amp;gt;% 
                group_by(y, hour) %&amp;gt;%
                dplyr::summarise(med = median(bri)) %&amp;gt;%
                ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we only used date and time based features for modeling, all we need for a prediction is a timestamp string. Using &lt;code&gt;for_sample&lt;/code&gt; and &lt;code&gt;def_vars()&lt;/code&gt;, we define a custom function &lt;code&gt;predict_hue()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_hue &amp;lt;- function(timestamp){
        
        df &amp;lt;- data.frame(log_time =as.POSIXct(timestamp)) %&amp;gt;% 
                add_vars(extra_var = &amp;quot;no&amp;quot;)
        
        pred &amp;lt;- predict(gbmFit, newdata = df)
        
        if (pred==&amp;quot;zero&amp;quot;) {
                x &amp;lt;- 0
        } else {
                x &amp;lt;- median_values %&amp;gt;% filter(y == pred &amp;amp; hour == lubridate::hour(timestamp)) %&amp;gt;%
                select(med) %&amp;gt;% unlist()
        }
        
        return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to expose the above function as an API, we literally need three lines of code with &lt;a href=&#34;https://cran.r-project.org/web/packages/jug/vignettes/jug.html&#34;&gt;jug&lt;/a&gt;. Ever since I saw the package &lt;a href=&#34;https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/jug-Building-Web-APIs-for-R&#34;&gt;presented at useR2017&lt;/a&gt;, I have been looking for a use case to play with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jug)
jug() %&amp;gt;% post(&amp;quot;/predict-hue&amp;quot;, decorate(predict_hue)) %&amp;gt;%
        simple_error_handler_json() %&amp;gt;%
        serve_it()
#Serving the jug at http://127.0.0.1:8080&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great. We can now make calls to this API and get a prediction supplying the current time. The nifty thing is that because API‚Äôs are language agnostic, we can access it from the programming paradigm of our choosing. I currently have a basic Python function that communicates with both API‚Äôs, transferring a prediction to the Hue Bridge every 5 minutes. But we could just as well build a whole interface on top, or create a chatbot for improved user experience. Perhaps I‚Äôll do a follow-up post on this topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;There is something very satisfying about building your own data products and seeing them at work. Even something as trivial as controlling the light switch for you. I only have data since this year May, so there‚Äôs a good chance our model will get smarter as days go by. We can easily schedule to retrain the model every week or even day, without having to lift a finger. Most of the code in this post is packaged up as a handful of &lt;code&gt;R&lt;/code&gt; functions deployed on my Raspberry Pi. Now, when I &lt;em&gt;choose&lt;/em&gt; to pass out on my couch next time, at least lights won‚Äôt stay on for too long.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Starting a blog(down)</title>
      <link>/blog/starting-a-blogdown/</link>
      <pubDate>Sun, 14 May 2017 21:13:14 -0500</pubDate>
      
      <guid>/blog/starting-a-blogdown/</guid>
      <description>&lt;div id=&#34;starting-an-analytics-blog&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Starting an analytics blog&lt;/h1&gt;
&lt;p&gt;Having learned lots from the open source community over the past years - from blogs and videos to attending meetups and awesome conferences - I have decided to start a blog myself, and share some of the things I find interesting. I expect most of the posts to be &lt;code&gt;R&lt;/code&gt; specific, because that‚Äôs what I am most comfortable with. However I do enjoy fiddling with other technologies such as &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;Spark&lt;/code&gt;, so watch out! In a nuthsell though, this blog will be about using open source tools to build all sorts of cool things with &lt;strong&gt;data&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;its-easy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;It‚Äôs easy&lt;/h1&gt;
&lt;p&gt;You can get your blog up and running with literally three lines of &lt;code&gt;R&lt;/code&gt; code. After hearing about the &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;&lt;strong&gt;blogdown&lt;/strong&gt;&lt;/a&gt; package on Twitter, I went ahead and downloaded the current build from Github, running &lt;code&gt;install_github(&#39;rstudio/blogdown&#39;)&lt;/code&gt; inside &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Under the hood, blogdown uses &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; to generate the website, but wraps most functionality nicely, so there‚Äôs no need for much manual configuration during the process, if at all.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;We first create a folder for the blog on our computer, and set it as our home directory using &lt;code&gt;setwd(&amp;quot;path-to-blog&amp;quot;)&lt;/code&gt;. Then we simply run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1 if you haven&amp;#39;t already, install blogdown
devtools::install_github(&amp;#39;rstudio/blogdown&amp;#39;)
# 2 install hugo
blogdown::install_hugo()
# 3 create new site
blogdown::new_site()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That‚Äôs it. You now have a complete folder structure initialized in your working directory:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/img/folderstruct.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The local build of your new site is now running on &lt;code&gt;localhost&lt;/code&gt;. You can see it in RStudio‚Äôs Viewer, or inside a browser by clicking &lt;em&gt;Show in new window&lt;/em&gt; in the top left corner of the Viewer.&lt;/p&gt;
&lt;p&gt;You future blog posts will reside in the &lt;code&gt;content/post&lt;/code&gt; folder. Here we find two pre-existing posts as &lt;code&gt;.Rmd&lt;/code&gt; files. We can start editing these straight away and see the results immediately after saving. Because everytime you save changes, your site is instantly rebuilt. If you come back to work on your existing site, you can simply run the function &lt;code&gt;serve_site()&lt;/code&gt; after you are done editing, and see the site regenerated accordingly in the Viewer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;customization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Customization&lt;/h2&gt;
&lt;p&gt;Now we can begin to customize the look of our blog by installing a theme using the function &lt;code&gt;install_theme(&#39;username/theme&#39;)&lt;/code&gt;. For my site, I picked &lt;a href=&#34;https://github.com/nishanths/cocoa-hugo-theme&#34;&gt;nishanths/cocoa-hugo-theme&lt;/a&gt; which I like very much for its minimalistic design. You can browse other themes on &lt;a href=&#34;https://themes.gohugo.io/&#34;&gt;themes.gohugo.io/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configuration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;p&gt;The only thing left to do, is to edit the &lt;code&gt;config.toml&lt;/code&gt; file and set the name of your blog, avatars, or even link a &lt;code&gt;Google Analytics&lt;/code&gt; account - if the theme allows for. The file contains parameters such as:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;title        = &amp;quot;Tamas Szilagyi&amp;quot;
baseurl      = &amp;quot;http://tamaszilagyi.com/&amp;quot;
relativeurls = true
languageCode = &amp;quot;en-us&amp;quot;
theme        = &amp;quot;cocoa-hugo-theme&amp;quot;
faviconfile  = &amp;quot;img/leaf.ico&amp;quot;
github       = &amp;quot;//github.com/mtoto&amp;quot;
highlightjs  = true
avatar       = &amp;quot;img/profile_pic.png&amp;quot; 
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are going to include &lt;code&gt;R&lt;/code&gt; codechunks in your posts, also don‚Äôt forget to set &lt;code&gt;highlightjslanguages = [&amp;quot;r&amp;quot;]&lt;/code&gt;. When the blog is ready, we run &lt;code&gt;build_site()&lt;/code&gt; to compile the files to &lt;code&gt;html&lt;/code&gt; and build the website. What we need for deployment will reside under the &lt;code&gt;/public&lt;/code&gt; folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deployment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Again, publishing is a piece of cake. There are &lt;a href=&#34;https://bookdown.org/yihui/blogdown/deployment.html&#34;&gt;multiple ways&lt;/a&gt; for conveniently deploying a blogdown site, and being somewhat familiar with &lt;a href=&#34;https://pages.github.com/&#34;&gt;Github Pages&lt;/a&gt;, that‚Äôs what I went for. I created a new repository named &lt;code&gt;mtoto.github.io&lt;/code&gt; and simply pushed the contents of &lt;code&gt;/public&lt;/code&gt; to the master branch.&lt;/p&gt;
&lt;p&gt;The website should be almost immediately available at the same address as the repo name. If you want an url other than &lt;code&gt;username.github.io&lt;/code&gt; however, you will need to sign up with a hosting provider. Then put a file in the &lt;code&gt;/public&lt;/code&gt; folder called &lt;code&gt;CNAME&lt;/code&gt;, with a one liner containing your blog url such as &lt;code&gt;tamaszilagyi.com&lt;/code&gt;. After, you push this file to Github and ask your provider to point your domain to the github pages url.&lt;/p&gt;
&lt;p&gt;And voil√†, we have ourselves a full functioning static website that looks great, is easy to manage and as portable as it gets may you decide to switch for different hosting solutions.&lt;/p&gt;
&lt;p&gt;For a more in-depth overview of what &lt;code&gt;blogdown&lt;/code&gt; is capable of, keep an eye on its &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;bookdown site&lt;/a&gt; which is currently under development.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>