<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Coding with Data</title>
    <link>/categories/r/index.xml</link>
    <description>Recent content in R on Coding with Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Lightweight streaming analytics with NATS</title>
      <link>/blog/lightweight-streaming-analytics-with-nats/</link>
      <pubDate>Tue, 02 Oct 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/lightweight-streaming-analytics-with-nats/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

library(knitr)

eng_go &lt;- function(options) {

  # create a temporary file

  f &lt;- basename(tempfile(&#34;go&#34;, &#39;.&#39;, paste(&#39;.&#39;, &#34;go&#34;, sep = &#39;&#39;)))
  on.exit(unlink(f)) # cleanup temp file on function exit
  writeLines(options$code, f)

  out &lt;- &#39;&#39;

  # if eval != FALSE compile/run the code, preserving output

  if (options$eval) {
    out &lt;- system(sprintf(&#39;go run %s&#39;, paste(f, options$engine.opts)), intern=TRUE)
  }

  # spit back stuff to the user

  engine_output(options, options$code, out)
}

knitr::knit_engines$set(go=eng_go)
&lt;/style&gt;
&lt;div id=&#34;go-in-the-fast-lane&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Go in the fast lane&lt;/h2&gt;
&lt;p&gt;Fast data is the new big data. But how difficult is it really to set up a complete streaming analytics solution from the ground up? It turns out not that hard, not if you are using &lt;a href=&#34;https://github.com/nats-io/go-nats-streaming&#34;&gt;NATS Streaming&lt;/a&gt;. Developed in Go&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“…NATS Streaming is an extremely performant, lightweight reliable streaming platform built on NATS.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have been wanting to experiment with Go for some time, so building a toy service on top NATS Streaming seemed like a interesting project to start with. To get familiar with the basics, I worked through about two thirds of the &lt;a href=&#34;https://github.com/nats-io/go-nats-streaming&#34;&gt;Tour of Go&lt;/a&gt; - a dope interactive introduction to the fundamentals of the language. At this point, I figured I know enough for the fifty or so lines of Go code I was about to write and headed for the IDE. What I had in mind was the classic streaming analytics demo: The real-time Twitter dashboard. After some initial research I was able to break down the task at hand into 4 subtasks:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Communicate with the &lt;a href=&#34;https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data.html&#34;&gt;Twitter Streaming API&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Ingest tweets into &lt;a href=&#34;https://github.com/nats-io/go-nats-streaming&#34;&gt;NATS Streaming&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Provision a MySQL database where the tweets will be written to.&lt;/li&gt;
&lt;li&gt;Create a Shiny App as a (near) real-time NLP dashboard.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To ensure everything is working as expected I’ll use Docker containers in conjunction with &lt;a href=&#34;https://docs.docker.com/compose/overview/&#34;&gt;Docker Compose&lt;/a&gt; as the orchestration tool.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-nats-streaming&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About NATS Streaming&lt;/h2&gt;
&lt;p&gt;Undeniably, Kafka is the most widely used streaming solution right now. But is it the only option out there? Is it even the best option? It depends, of course. But if you prefer a lightweight footprint and simplicity without sacrificing performance, NATS is very, very hard to beat. NATS Streaming is a service layer on top the original NATS framework. The latter was originally conceived as a distributed messaging system with few guarantees, but blazing fast performance. NATS Streaming extends the original framework through the introduction of at-least-once delivery, durable storage, message replay and a couple other enhanced quality of service features.&lt;/p&gt;
&lt;p&gt;The central piece is the NATS (Streaming) Server. It manages subscriptions on specific subjects and handles communications between clients. Once the server is up and running, we can create and publish messages unto subjects, and on the receiving end subscribe to them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/nats.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nats-io/nats-streaming-server#getting-started&#34;&gt;Installing the package&lt;/a&gt; creates an executable &lt;code&gt;nats-streaming-server.go&lt;/code&gt; that we can run to start the server.&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;[12505] 2018/10/01 11:53:13.037897 [INF] STREAM: Starting nats-streaming-server[test-cluster] version 0.11.0
[12505] 2018/10/01 11:53:13.038015 [INF] STREAM: ServerID: iDV733mTgjWieVayqCLiG2
[12505] 2018/10/01 11:53:13.038022 [INF] STREAM: Go version: go1.11
[12505] 2018/10/01 11:53:13.038880 [INF] Starting nats-server version 1.3.0
[12505] 2018/10/01 11:53:13.038894 [INF] Git commit [not set]
[12505] 2018/10/01 11:53:13.039199 [INF] Listening for client connections on 0.0.0.0:4222
[12505] 2018/10/01 11:53:13.039208 [INF] Server is ready
[12505] 2018/10/01 11:53:13.068118 [INF] STREAM: Recovering the state...
[12505] 2018/10/01 11:53:13.068165 [INF] STREAM: No recovered state
[12505] 2018/10/01 11:53:13.320178 [INF] STREAM: Message store is MEMORY
[12505] 2018/10/01 11:53:13.320295 [INF] STREAM: ---------- Store Limits ----------
[12505] 2018/10/01 11:53:13.320305 [INF] STREAM: Channels:                  100 *
[12505] 2018/10/01 11:53:13.320312 [INF] STREAM: --------- Channels Limits --------
[12505] 2018/10/01 11:53:13.320320 [INF] STREAM:   Subscriptions:          1000 *
[12505] 2018/10/01 11:53:13.320329 [INF] STREAM:   Messages     :       1000000 *
[12505] 2018/10/01 11:53:13.320337 [INF] STREAM:   Bytes        :     976.56 MB *
[12505] 2018/10/01 11:53:13.320343 [INF] STREAM:   Age          :     unlimited *
[12505] 2018/10/01 11:53:13.320349 [INF] STREAM:   Inactivity   :     unlimited *
[12505] 2018/10/01 11:53:13.320356 [INF] STREAM: ----------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, NATS Streaming persists the messages as files. It suffices to start publishing messages to a subject, and they will be saved to memory. The other option is a MySQL database. Using the flags &lt;code&gt;--store&lt;/code&gt;, &lt;code&gt;--sql_driver&lt;/code&gt; and &lt;code&gt;--sql_source&lt;/code&gt; when starting the &lt;code&gt;nats-streaming-server&lt;/code&gt;, we can configure access to the database, or alternatively supply a &lt;code&gt;.conf&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Once the server is up and running, we can create subjects and publish messages. A simple worker program written in Go that ingests data from the Twitter Streaming API, and funnels it into the database using NATS Streaming, is barely ~50 lines of code.&lt;/p&gt;
&lt;pre class=&#34;go&#34;&gt;&lt;code&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;os&amp;quot;
    &amp;quot;stream/keys&amp;quot;

    &amp;quot;github.com/dghubble/go-twitter/twitter&amp;quot;
    &amp;quot;github.com/dghubble/oauth1&amp;quot;
    stan &amp;quot;github.com/nats-io/go-nats-streaming&amp;quot;
)

func main() {
    var err error
    word := os.Getenv(&amp;quot;TWITTER&amp;quot;)  // Get word to filter Twitter stream on 

    config := oauth1.NewConfig(keys.Key, keys.Secret)
    token := oauth1.NewToken(keys.Token, keys.TokenSecret)
    httpClient := config.Client(oauth1.NoContext, token)

    // Twitter client
    twitterClient := twitter.NewClient(httpClient)
    // Nats client
    natsClient, err := stan.Connect(&amp;quot;test-cluster&amp;quot;, &amp;quot;test&amp;quot;,
        stan.NatsURL(&amp;quot;nats://nats:4222&amp;quot;))
    if err != nil {
        log.Fatal(err)
    }
    
    // Convenience Demux demultiplexed stream messages
    demux := twitter.NewSwitchDemux()
    demux.Tweet = func(tweet *twitter.Tweet) {
        natsClient.Publish(word, []byte(tweet.Text))
    }

    // Filter parameters for Twitter stream
    filterParams := &amp;amp;twitter.StreamFilterParams{
        Track:         []string{word},
        StallWarnings: twitter.Bool(true),
        Language:      []string{&amp;quot;en&amp;quot;},
    }
    
    stream, err := twitterClient.Streams.Filter(filterParams)
    if err != nil {
        log.Fatal(err)
    }
    for message := range stream.Messages {
        demux.Handle(message)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it. Of course this is only two pieces of the puzzle. We still need to provision a SQL database for the message store and build a Shiny App to munge and visualize the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;meet-the-architect-docker-compose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Meet the architect: Docker Compose&lt;/h2&gt;
&lt;p&gt;Instead of creating the remaining services one by one and linking them up, it is better to deploy this mini infrastructure in its entirety. With Docker containers, we can package each piece with all its dependencies. Using Docker Compose, we can configure how the containers should work in tandem and communicate with each other if needed.&lt;/p&gt;
&lt;p&gt;With a Docker image for everything nowadays, our Dockerfiles won’t be long. Adding a couple of &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/db/Dockerfile&#34;&gt;environment variables here&lt;/a&gt; or &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/shiny/Dockerfile&#34;&gt;installing additional package there&lt;/a&gt;, most configurations will already be taken care of by the read-only layers of the base images.&lt;/p&gt;
&lt;p&gt;Our infra consists of 4 containers: One for the MySQL database, the NATS streaming server, the NATS worker that will publish the messages and finally the Shiny app. A couple of pointers with regards to the &lt;code&gt;docker-compose.yml&lt;/code&gt; file below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;build: context:&lt;/code&gt; parameter is the location of the Dockerfile.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;restart: always&lt;/code&gt; is set because services aren’t booted in sequence despite dependencies.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ports:&lt;/code&gt; will be shared among services, and also exposed to the outside world.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;environment: - TWITTER=${TWITTER}&lt;/code&gt; ensures that the &lt;code&gt;$TWITTER&lt;/code&gt; environment variable (as defined in the &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/.env&#34;&gt;.env file&lt;/a&gt;) is available for all.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without further ado, this is what the final &lt;code&gt;docker-compose.yml&lt;/code&gt; file looks like:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;version: &amp;quot;3&amp;quot;

services:
  db:
    build:
      context: &amp;quot;./db&amp;quot;
    restart: always
    command: --default-authentication-plugin=mysql_native_password
    ports:
      - &amp;quot;3306&amp;quot;
  nats:
    image: nats-streaming:latest
    restart: always
    depends_on:
      - db
    command: -m 8222 --store SQL --sql_driver mysql --sql_source &amp;quot;root:pwd@tcp(db:3306)/nss_db&amp;quot;
    ports:
      - &amp;quot;4222&amp;quot;
      - &amp;quot;8222:8222&amp;quot;
  nats-worker:
    build:
      context: &amp;quot;./nats&amp;quot;
    environment:
    - TWITTER=${TWITTER}
    restart: always
    entrypoint: /go/main
    depends_on:
      - nats
  shiny:
    build:
      context: &amp;quot;./shiny&amp;quot;
    environment:
    - TWITTER=${TWITTER}
    ports:
      - &amp;quot;80:3838&amp;quot;
    depends_on:
      - db&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To build all the images we can use &lt;code&gt;docker-compose build&lt;/code&gt;; the command to spin up the services is &lt;code&gt;docker-compose -f docker-compose.yml up&lt;/code&gt;. Similarly to stop the containers we have &lt;code&gt;docker-compose stop&lt;/code&gt; and &lt;code&gt;docker-compose rm -fv&lt;/code&gt; to get rid of the stopped containers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-apps-and-streaming-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny Apps and streaming data&lt;/h2&gt;
&lt;p&gt;I have written about &lt;a href=&#34;http://tamaszilagyi.com/blog/dockerized-shiny-app-development/&#34;&gt;Shiny Apps and how to containerize them before&lt;/a&gt;, so I will only briefly touch upon dealing with real-time data here. As we have seen before, NATS Streaming is continuously dumping new data into our MySQL database according to a &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny/blob/master/db/dump/schema.sql&#34;&gt;predefined schema&lt;/a&gt;. On the R side it turns out we have pretty sweet tools for dealing with databases, such as &lt;a href=&#34;https://shiny.rstudio.com/articles/pool-basics.html&#34;&gt;pool and DBI&lt;/a&gt;. Specifically for shiny apps, there is also a function called &lt;code&gt;shiny::reactivePoll()&lt;/code&gt; that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“..create a reactive data source, which works by periodically polling a non-reactive data source.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Two of the required arguments are functions. One to check whether some value in our database has been updated, and if so, one to pull the updated data from the database. The other two required arguments are the number of milliseconds to wait between checks, and the user session.&lt;/p&gt;
&lt;p&gt;This is the relevant bit from the shiny app:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pool)
library(DBI)
library(shiny)
library(anytime)
library(tidytext)

pool &amp;lt;- dbPool(
        drv = RMySQL::MySQL(),
        dbname = &amp;quot;nss_db&amp;quot;,
        host = &amp;quot;db&amp;quot;, 
        port = 3306,
        username = &amp;quot;nss&amp;quot;,
        password = &amp;quot;password&amp;quot; 
)

data &amp;lt;- reactivePoll(1000, session,
             # This function returns the latest timestamp from the DB
             checkFunc = function() {
                     pool %&amp;gt;% tbl(&amp;quot;Messages&amp;quot;) %&amp;gt;%
                             summarise(max_time = max(timestamp, na.rm = TRUE)) %&amp;gt;%
                             collect() %&amp;gt;%
                             unlist()
                     
             },
             # This function returns a data.frame ready for text mining
             valueFunc = function() {
                     pool %&amp;gt;% tbl(&amp;quot;Messages&amp;quot;) %&amp;gt;%
                             filter(!data %like% &amp;quot;%http%&amp;quot;) %&amp;gt;% 
                             arrange(-timestamp) %&amp;gt;%
                             head(20000) %&amp;gt;%
                             collect() %&amp;gt;%
                             mutate(data = gsub(&amp;quot;[^[:alnum:][:space:]]&amp;quot;,&amp;quot;&amp;quot;,data)) %&amp;gt;%
                             unnest_tokens(word, data) %&amp;gt;%
                             anti_join(stop_words) %&amp;gt;% 
                             mutate(timestamp = anytime(timestamp/1e+9)) %&amp;gt;%
                             inner_join(get_sentiments(&amp;quot;bing&amp;quot;)) 
                   
             }
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After establishing the pool connection, it is used to check whether the latest timestamp is different from the previous one. If that’s the case, we pull the last 20.000 tweets from the database, collect it as an R &lt;code&gt;data.frame&lt;/code&gt; and transform it using the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html&#34;&gt;tidytext&lt;/a&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postscript&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Postscript&lt;/h2&gt;
&lt;p&gt;The app I have outlined in this post is currently live on &lt;a href=&#34;http://stream.tamaszilagyi.com/&#34;&gt;stream.tamaszilagyi.com&lt;/a&gt; plotting a few metrics for tweets containing the word “trump”, for demonstration purposes. It is running on a small Linux VM on Azure so don’t be intimidated by slow load times. I only have so much free Azure credit.&lt;/p&gt;
&lt;p&gt;With minor modifications though, we could deploy our containers onto a cluster of computers and scale the crap out of this little streaming service. Such is the beauty of cloud resources and using cloud-native technologies like Docker and NATS.&lt;/p&gt;
&lt;p&gt;As always, all the code is on my &lt;a href=&#34;https://github.com/mtoto/stream-go-shiny&#34;&gt;GitHub&lt;/a&gt;, including instructions on how to try it for yourself.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Parallelizing R code on Kubernetes</title>
      <link>/blog/parallelizing-r-code-on-kubernetes/</link>
      <pubDate>Tue, 07 Aug 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/parallelizing-r-code-on-kubernetes/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;kubernetes-who&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Kubernetes who?&lt;/h1&gt;
&lt;p&gt;The hype around kubernetes is real, but likely also justified. Kubernetes is an open-source tool that facilitates deployment of jobs and services onto computer clusters. It provides different patterns for different type of workloads, be it API servers, databases or running batch jobs. Not only makes kubernetes running workloads and services easy, it also &lt;a href=&#34;https://thenewstack.io/kubernetes-credited-saving-spire-service-s3-outage/&#34;&gt;keeps them running&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At the core of the technology are containers, which kubernetes skillfully manages inside so-called pods. A pod represents a single instance of an application and contains one or sometimes more containers. Pods in turn live on worker nodes - actual servers - and are managed by a controller on the master node. We can interact with pods indirectly via instructions to controller.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/infra2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Mark Edmondson has already written a &lt;a href=&#34;http://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts/&#34;&gt;fantastic blog post&lt;/a&gt; about different use cases for running R application inside kubernetes. I’ll dive into the one topic he didn’t expand upon: the parallel execution of R code on kubernetes.&lt;/p&gt;
&lt;p&gt;I will similarly use GCP’s &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/&#34;&gt;kubernetes engine&lt;/a&gt; to deploy my jobs, however all major cloud providers have similar offerings. It’s worth mentioning that Google provides 300$ worth of credit free to spend on any of their cloud products, so you can freely experiment without burning a hole in your pocket.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;single-job-with-static-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Single job with static parameters&lt;/h1&gt;
&lt;p&gt;The simplest use case of parallelization is running the same script over and over again, but in parallel instead of in a sequential order. A classic example is simulation, i.e. the random generation of numbers given a fixed set of parameters.&lt;/p&gt;
&lt;p&gt;I am taking an example from &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/batch/tutorial-r-doazureparallel#run-a-parallel-simulation&#34;&gt;Azure’s tutorial on running R code in parallel&lt;/a&gt;, simulating stock prices after a year (365 days) given a fixed value for standard deviation and average stock price movement per day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_change = 1.001 
volatility = 0.01 
opening_price = 100 

getClosingPrice &amp;lt;- function(days) { 
        movement &amp;lt;- rnorm(days, mean=mean_change, sd=volatility) 
        path &amp;lt;- cumprod(c(opening_price, movement)) 
        closingPrice &amp;lt;- path[days] 
        return(closingPrice) 
} 

replicate(1000, getClosingPrice(365)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s save the above script into an executable file, in our case &lt;code&gt;monte-carlo.R&lt;/code&gt;, and write a minimal &lt;code&gt;Dockerfile&lt;/code&gt; encapsulating the script. Remember kubernetes works with containers and can access them directly from &lt;a href=&#34;https://hub.docker.com/&#34;&gt;Dockerhub&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;FROM rocker/r-base
COPY monte-carlo.R ./&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We build the image and upload it to Dockerhub using the docker command line tool.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# build image
docker build -t mtoto/mc-demo:latest .
# upload to docker hub
docker push mtoto/mc-demo:latest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now comes the kubernetes bit in the form of a &lt;code&gt;job.yaml&lt;/code&gt; file, that contains the instructions for the controller. Note that under &lt;code&gt;spec:&lt;/code&gt; we specify the number of pods to run our job on in parallel (distribution of pods over nodes is handled by kubernetes), and the number of completions. Each pod picks up a single run and exists after the script has finished. By the end of this workload 100 pods have been created, run and terminated.&lt;/p&gt;
&lt;pre class=&#34;yml&#34;&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: static-demo
spec:
  parallelism: 10
  completions: 100
  template:
    metadata:
      name: static-example
      labels:
        jobgroup: static-example
    spec:
      containers:
      - name: birthday
        image: mtoto/mc-demo
        command: [&amp;quot;Rscript&amp;quot;, &amp;quot;monte-carlo.R&amp;quot;]
      restartPolicy: Never&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With everything in place (&lt;code&gt;R&lt;/code&gt; script, &lt;code&gt;Dockerfile&lt;/code&gt;, &lt;code&gt;.yaml&lt;/code&gt; file), we are ready to deploy our first job to kubernetes. Assuming you have &lt;a href=&#34;https://support.google.com/cloud/answer/6158841?hl=en&#34;&gt;enabled the relevant services&lt;/a&gt; in the google cloud console, downloaded the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;google cloud SDK&lt;/a&gt; and have &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt; installed, we can create our cluster and deploy our first the workload on GCP in the following way:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create 4 node cluster &amp;quot;kubepar&amp;quot; on google kubernetes engine
gcloud container clusters create kubepar --machine-type n1-standard-1 --num-nodes 4
# get credentials to point kubectl to our cluster
gcloud container clusters get-credentials kubepar
# create job
kubectl create -f job.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can monitor the progress of our job using the command &lt;code&gt;kubectl get pods&lt;/code&gt;, to see how many pods have successfully run.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/static-pods-2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly we can look at the state of the nodes with &lt;code&gt;kubectl get nodes&lt;/code&gt; or the overall status of the job with &lt;code&gt;kubectl get jobs static-demo&lt;/code&gt;. For a more detailed output, substitute &lt;code&gt;get&lt;/code&gt; with &lt;code&gt;describe&lt;/code&gt;, such as &lt;code&gt;kubectl describe pods&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once the job has finished, we collect the output of our simulation from the logs of each pod and write it to a &lt;code&gt;.txt&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;for p in $(kubectl get pods -l jobgroup=static-example -o name)
do
  kubectl logs $p &amp;gt;&amp;gt; output.txt
done&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reading the output into &lt;code&gt;R&lt;/code&gt; we can plot the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(stockprices)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/stockprices.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-template-and-multiple-parameters-using-expansion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Common template and multiple parameters using expansion&lt;/h1&gt;
&lt;p&gt;Moving on, we want to parallelize a script with different parameters at each run. Again, I am taking an example from a &lt;a href=&#34;http://blog.revolutionanalytics.com/2018/01/doazureparallel-simulations.html&#34;&gt;doAzureParallel tutorial&lt;/a&gt; where&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… we calculate for a room of N people the probability that someone in the room shares a birthday with someone else in the room.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Below is the simulation script for 100.000 rooms where we supply the number of people in the room as a command line argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#!/usr/bin/env Rscript
args = commandArgs(trailingOnly=TRUE)
n &amp;lt;- as.double(args[1])

pbirthdaysim &amp;lt;- function(n) { 
        ntests &amp;lt;- 100000 
        pop &amp;lt;- 1:365 
        anydup &amp;lt;- function(i) 
                any(duplicated( 
                    sample(pop, n, replace=TRUE)))
        sum(sapply(seq(ntests), anydup)) / ntests 
}

pbirthdaysim(n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike before, we are not creating a single representation of our Job object in a &lt;code&gt;.yaml&lt;/code&gt; file, but a &lt;em&gt;Job template&lt;/em&gt; with placeholders. The &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/expansion/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; is the same as before, except for the script. Don’t forget to build and upload the image before continuing.&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: par-demo-$ITEM
spec:
  template:
    metadata:
      name: par-example
      labels:
        jobgroup: par-example
    spec:
      containers:
      - name: birthday
        image: mtoto/birthday-demo
        command: [&amp;quot;Rscript&amp;quot;, &amp;quot;birthday.R $ITEM&amp;quot;]
      restartPolicy: Never&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we didn’t specify parallelization parameters nor the number of completions. It’s because we are going to &lt;strong&gt;expand&lt;/strong&gt; the above template into 100 different &lt;code&gt;job.yaml&lt;/code&gt; files, one for each run with a different &lt;code&gt;n&lt;/code&gt; parameter (&lt;code&gt;$ITEM&lt;/code&gt; in the &lt;code&gt;.yaml&lt;/code&gt; fie) for the birthday simulation.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create folder for jobs
mkdir jobs
# create job.yaml files
for i in {1..100}
do
  cat job.yaml | sed &amp;quot;s/\$ITEM/$i/&amp;quot; &amp;gt; ./jobs/job-$i.yaml
done&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the same command as before, we create all the jobs at once: &lt;code&gt;kubectl create -f ./jobs&lt;/code&gt;. Kubernetes will automatically create, distribute and run our jobs in parallel across pods on the nodes of our cluster.&lt;/p&gt;
&lt;p&gt;Using the same &lt;code&gt;bash&lt;/code&gt; script as before, we can retrieve the output from each run and after read it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Plotting the results, the probability that 2 or more people will have the same birthday is 99% after 60 people are in the room.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(probabiliy, xlab=&amp;quot;People in room&amp;quot;, 
     ylab=&amp;quot;Probability of shared birthday&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/birthdays.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fine-parallel-processing-using-a-work-queue&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fine parallel processing using a work queue&lt;/h1&gt;
&lt;p&gt;In the previous example, we created all the jobs at once, which can overload the scheduler if the number of jobs is very large. A smarter approach is to create a work queue and let the pods pick them off one by one as they go along. Unlike before, each pod will work on multiple items until the queue is empty instead of creating a pod for each task.&lt;/p&gt;
&lt;p&gt;To illustrate the last approach, we will parallelize the training of different regression models, a common use case for parallelization in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function below takes the name of an algorithm, loads the dataset, creates a training set, runs a model using the caret package and finally uploads the result to google cloud storage as an &lt;code&gt;.rds&lt;/code&gt; file. This way the work queue only needs to contain the names of the models to run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# modeling function
run_save_model &amp;lt;- function(method) {
        # load pkgs and data
        library(mlbench)
        library(caret)
        data(&amp;quot;BostonHousing&amp;quot;)
        # split data
        set.seed(123)
        train_index &amp;lt;- createDataPartition(BostonHousing$medv,1, p = .7)
        train &amp;lt;- BostonHousing[train_index[[1]],]
        # train model
        model &amp;lt;- train(medv ~., 
                       data = train, 
                       method = method)
        
        # upload to storage bucket
        file &amp;lt;- sprintf(&amp;quot;%s_model.rds&amp;quot;, method)
        saveRDS(model, file)
        googleCloudStorageR::gcs_upload(file, 
                   name = file,
                   bucket = &amp;quot;bostonmodels&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;setting-up-redis-on-kubernetes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up Redis on kubernetes&lt;/h2&gt;
&lt;p&gt;We’ll be using Redis for the work queue, so we need an additional pod running Redis and a service so other pods can find it. The recipes for both are within &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/redis-pod.yaml&#34;&gt;redis-master.yaml&lt;/a&gt; and &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/redis-service.yaml&#34;&gt;redis-service.yaml&lt;/a&gt;. Similarly to jobs, we can use &lt;code&gt;kubectl create&lt;/code&gt; command to start the instances and then use the Redis command line tool to add the work items to the queue.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create redis pod and redis service
kubectl create -f ./redis-pod.yaml
kubectl create -f ./redis-service.yaml
# create temporary interactive pod
kubectl run temp -i --rm --tty  --image redis --command &amp;quot;/bin/sh&amp;quot;
# initiate redis cli
redis-cli -h redis
# push items into queue named &amp;quot;test&amp;quot;
rpush test &amp;quot;lm&amp;quot; &amp;quot;rf&amp;quot; &amp;quot;gbm&amp;quot; &amp;quot;enet&amp;quot; &amp;quot;brnn&amp;quot; &amp;quot;bridge&amp;quot;
# doublecheck queue
lrange test 0 -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the consumer side, I re-implemented the &lt;a href=&#34;https://kubernetes.io/examples/application/job/redis/rediswq.py&#34;&gt;Redis client from the official docs&lt;/a&gt; in R using the &lt;a href=&#34;https://cran.r-project.org/web/packages/redux/vignettes/redux.html&#34;&gt;redux package&lt;/a&gt;. The file &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/rediswq.R&#34;&gt;rediswq.R&lt;/a&gt; contains all the building blocks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;giving-access-to-google-cloud-storage-from-kubernetes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Giving access to google cloud storage from kubernetes&lt;/h2&gt;
&lt;p&gt;Before we could extract the output from the logs, now we will save the models as &lt;code&gt;.rds&lt;/code&gt; files on cloud storage. For this, the containers running on our cluster need write access to our storage bucket.&lt;/p&gt;
&lt;p&gt;Using GCP, we need to create a new &lt;a href=&#34;https://cloud.google.com/compute/docs/access/service-accounts&#34;&gt;service account&lt;/a&gt; inside our project and under &lt;strong&gt;Roles&lt;/strong&gt; give it full access to cloud storage by selecting &lt;strong&gt;Storage Object Admin&lt;/strong&gt;. Make sure to check the box for &lt;strong&gt;Furnish a new private key&lt;/strong&gt; and click SAVE.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/mtoto/mtoto.github.io/raw/master/blog/2018/service.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Back to the terminal, we can save our credentials as a Secret that will be directly accessible to the kubernetes engine.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# create secret named &amp;quot;gcs-key&amp;quot;
kubectl create secret generic gcs-key --from-file=key.json=PATH-TO-KEY-FILE.json&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll see how to use this secret in the &lt;code&gt;job.yaml&lt;/code&gt; file shortly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;worker-program&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Worker program&lt;/h2&gt;
&lt;p&gt;Finally, we write a worker program that takes the work items from the Redis work queue and executes &lt;code&gt;run_save_model()&lt;/code&gt;. While the pods have no knowledge of the number of work items in the queue, they notice when the queue is empty and will automatically terminate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;rediswq.R&amp;quot;)
source(&amp;quot;functions.R&amp;quot;)

# connect to redis host
host &amp;lt;- Sys.getenv(&amp;quot;REDIS_SERVICE_HOST&amp;quot;)
db &amp;lt;- redis_init(host = host)
vars_init(&amp;quot;test&amp;quot;)

# authenticate gcs
library(googleCloudStorageR)

print(paste0(&amp;quot;Worker with sessionID: &amp;quot;, session))
print(paste0(&amp;quot;Initial queue state: empty=&amp;quot;, as.character(empty())))

while (!empty()) {
        item &amp;lt;- lease(lease_secs=10,
                        block = TRUE,
                        timeout = 2)
        if (!is.null(item)) {
                print(paste0(&amp;quot;working on: &amp;quot;, item))
                # actual work
                run_save_model(item)
                complete(item)
        } else {
          print(&amp;quot;waiting for work&amp;quot;)       
        }
}
print(&amp;quot;queue emtpy, finished&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all the scripts in place, let’s not forget to build a Docker image and upload it to Dockerhub. The &lt;a href=&#34;https://github.com/mtoto/kubernetes-r-playground/blob/master/fine/Dockerfile&#34;&gt;Dockerfie&lt;/a&gt; is going to be a bit longer this time given the numerous dependencies our program needs.&lt;/p&gt;
&lt;p&gt;As for the &lt;code&gt;.yaml&lt;/code&gt; file, it is very similar to what we have written before with the addition of mounting our Secret &lt;code&gt;gcs-key&lt;/code&gt; as a volume so that the containers have access. We name this variable &lt;code&gt;GCS_AUTH_FILE&lt;/code&gt;, which the &lt;a href=&#34;https://github.com/cloudyr/googleCloudStorageR&#34;&gt;googlegoogleCloudStorageR package&lt;/a&gt; looks for when loading the library to authenticate the client.&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;apiVersion: batch/v1
kind: Job
metadata:
  name: fine-demo
spec:
  parallelism: 4
  template:
    metadata:
      name: fine-example
      labels:
        jobgroup: fine-example
    spec:
      volumes:
      - name: google-cloud-key
        secret:
          secretName: gcs-key
      containers:
      - name: c
        image: mtoto/ml-demo
        volumeMounts:
        - name: google-cloud-key
          mountPath: /var/secrets/google
        env:
        - name: GCS_AUTH_FILE
          value: /var/secrets/google/key.json
        command: [&amp;quot;Rscript&amp;quot;, &amp;quot;worker.R&amp;quot;]
      restartPolicy: OnFailure&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like before, we hit &lt;code&gt;kubectl create -f job.yaml&lt;/code&gt; to start the job and monitor the status of the 4 pods with &lt;code&gt;kubectl get pods&lt;/code&gt;. You will notice that the pods don’t exit until the queue is finished. Once they are done working on one item they pick up the next one, saving additional overhead compared to the previous two approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs-to-keep-in-mind&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Trade-offs to keep in mind&lt;/h1&gt;
&lt;p&gt;Going from static workloads to setting up work queues that feed into the workers, we are introducing additional complexity. It’s not always a good thing, especially not if modifying existing applications is costly. We could’ve done parallel machine learning just as well using parameter expansion (the second approach).&lt;/p&gt;
&lt;p&gt;On the other hand, having one Job object for each work item creates some overhead that a single Job object for all work items does not. Again, the difference will become more apparent the more work we have.&lt;/p&gt;
&lt;p&gt;Lastly, the first two approaches create as many pods as work items, requiring less modification to existing code. With the last approach however each pod can process multiple items, which is a gain in efficiency.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Interpretable GDPR Classifiers</title>
      <link>/blog/interpretable-gdpr-classifiers/</link>
      <pubDate>Tue, 19 Jun 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/interpretable-gdpr-classifiers/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;we-have-updated-our-privacy-policies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;em&gt;We have updated our privacy policies…&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Whether internet companies are now compliant with GDPR is hard to say, but they sure left updating their privacy policies to the last minute. What happened in the last days of May was the greatest corporate email tsunami since Y2K. I hardly read the updated policies, or remember what the old ones looked like. Wouldn’t it be great if we could model where GDPR inspired policies are different from the old ones instead of reading them?&lt;/p&gt;
&lt;p&gt;Looking for data, I came upon the &lt;a href=&#34;https://usableprivacy.org/data&#34;&gt;Usable Privacy Project&lt;/a&gt;, that has a few datasets available consisting of privacy policies. I downloaded the ACL/COLING 2014 Dataset &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, which contains roughly 1000 documents in .xml files. Collected pre GDPR, this is a great starting point. But I still need a respectable amount of policies inspired by recent European regulation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;async-web-scraping-ftw&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Async web scraping FTW&lt;/h1&gt;
&lt;p&gt;The researchers behind the Usable Privacy Project paid mechanical turks to collect policies; I opted for the second best thing: asynchronous web scraping. There is a very promising package on GitHub called &lt;a href=&#34;https://github.com/r-lib/async&#34;&gt;&lt;strong&gt;async&lt;/strong&gt;&lt;/a&gt; for asynchronous HTTP requests in R. The advantage of asynchronous over synchronous code for network operations is that in case of a response delay, an asynchronous client will issue another request instead of staying idle. Because time is not being wasted on waiting for responses, asynchronous HTTP requests are orders of magnitude faster than their sequential cousins.&lt;/p&gt;
&lt;p&gt;Extending the example from the package’s &lt;a href=&#34;https://github.com/r-lib/async/blob/master/README.md#deferred-chains&#34;&gt;README.md&lt;/a&gt; with some help from &lt;a href=&#34;https://github.com/hadley/rvest&#34;&gt;rvest&lt;/a&gt;, we can easily create an asynchronous scraping function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(async)
library(rvest)

http_content &amp;lt;- function(url) {
        def &amp;lt;- http_get(url)$
                then(function(response) {
                        if(response$status_code == 200) {
                                rawToChar(response$content) %&amp;gt;%
                                        read_xml(as_html = TRUE)  %&amp;gt;%
                                        html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
                                        html_text()
                        } 
                }
                )$
                catch(error = function(...) setNames(&amp;quot;error&amp;quot;, url))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are essentially creating a deferred value &lt;code&gt;def&lt;/code&gt; which is only operated on - what follows after &lt;code&gt;then(function(response)&lt;/code&gt; - when this value becomes known.&lt;/p&gt;
&lt;p&gt;Now, I need &lt;em&gt;possible&lt;/em&gt; url’s of online privacy policies to plug in, and I need a lot of them. I went ahead and retrieved the most popular sites from the US, Canada, UK, South Africa and Australia using the &lt;a href=&#34;https://aws.amazon.com/alexa-top-sites/getting-started/&#34;&gt;Alexa Topsites API&lt;/a&gt;. All you need is an AWS account and the service is free up to top 500 sites per country. To figure out what suffix I need for the url’s, I looked at a few online privacy policy online and concluded that most of them would have the same pattern. I generated all combinations of the url’s and suffixes and plugged the result into async’s native asynchronous iterator &lt;code&gt;async_map()&lt;/code&gt; along with the async scraping function. Finally wrapping the script with &lt;code&gt;synchronise()&lt;/code&gt; that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… creates an event loop, which manages the computation of the deferred values in this particular async phase.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create all possible combinations of urls &amp;amp; suffixes
suffix &amp;lt;- c(&amp;quot;/privacy_policy&amp;quot;,&amp;quot;/privacy-policy&amp;quot;,&amp;quot;/privacy&amp;quot;)
all_urls &amp;lt;- as.vector(outer(urls, suffix, paste, sep=&amp;quot;&amp;quot;))

# call async iterator and initate event loop
usable_policies_list &amp;lt;- synchronise(
        async_map(
                all_urls, 
                http_content)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On about 4.500 url’s this completes in less than 10 minutes. I wanted to benchmark the performance on the same data with &lt;code&gt;lapply()&lt;/code&gt; and regular GET requests, but I ran out of patience after running it overnight to no end.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/data/2018-06-19-gdpr/data_prep.R&#34;&gt;After all the scraping, cleaning and parsing&lt;/a&gt;, I ended up with a final dataset containing 1196 policies and an indicator whether it mentions “&lt;em&gt;GDPR&lt;/em&gt;” or not. I took the conservative approach of classifying the scraped policies as GDPR compliant only if the document contains the term, giving me 188 policies. It’s not a whole lot - because most url’s were either invalid or guarded against scraping; but hopefully just enough to get us going.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-first-answer-questions-last&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model first, answer questions last&lt;/h1&gt;
&lt;p&gt;I am going to skip exploratory data analysis altogether and jump straight into modeling the difference between policies. If you want to see an example of the former, I encourage you to check out my previous post &lt;a href=&#34;http://tamaszilagyi.com/blog/a-tidy-text-analysis-of-rick-and-morty/&#34;&gt;on Rick and Morty&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With an imbalanced class distribution,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(data$is_gdpr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   no  yes 
## 1008  188&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the average policy about 15.000 characters long,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(nchar(data$policy))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       0    7936   14114   15944   20194  141715&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this is going to be an interesting task.&lt;/p&gt;
&lt;div id=&#34;data-prep&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data prep&lt;/h2&gt;
&lt;p&gt;First we have to convert policy texts to numbers. A common approach is constructing a document-term matrix where each row is a document and each word is a column. The cells in the matrix contain a statistic about each word in their respective document, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;tf-idf score&lt;/a&gt;. It is a simple and powerful way to identify important words in each document. We simply count the number of words per policy and use the number of times it appears in other policies as a weighing factor.&lt;/p&gt;
&lt;p&gt;The dimensions of the resulting document-term matrix depend on how many words we include in our vocabulary. Either way, the number of variables will quickly run up to thousands and because most words only appear in a subset of documents, the result will be a sparse matrix. Before anything, let’s split the dataset for modeling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(321)
# sample row indices for split
smp_size &amp;lt;- floor(0.75 * nrow(data))
train_ind &amp;lt;- sample(seq_len(nrow(data)), size = smp_size)
# create train and test sets
train &amp;lt;- data[train_ind,]
test &amp;lt;- data[-train_ind,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To tokenize our text and apply the tf-idf transformations I’ll use the &lt;code&gt;text2vec&lt;/code&gt; package. We calculate the tf-idf scores based on the train set, and only map the same values onto the validation set to prevent leakage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(text2vec)
library(magrittr)
# exclude stopwords &amp;amp; anything referring to EU
stopwords_gdpr = c(stopwords::stopwords(&amp;quot;en&amp;quot;),
                   &amp;quot;gdpr&amp;quot;,&amp;quot;eu&amp;quot;, &amp;quot;eea&amp;quot;, 
                   &amp;quot;european&amp;quot;, &amp;quot;europe&amp;quot;)

# iterators for word level tokenization
it_train &amp;lt;- itoken(train$policy, preprocessor = tolower,
                   tokenizer = word_tokenizer, progressbar = FALSE)
it_test &amp;lt;- itoken(test$policy, preprocessor = tolower,
                  tokenizer = word_tokenizer, progressbar = FALSE)

# create vectorizer function based on iterator and vocab size
vectorizer &amp;lt;- create_vocabulary(it_train, stopwords = stopwords_gdpr) %&amp;gt;%
        prune_vocabulary(doc_count_min = 10, vocab_term_max = 1000) %&amp;gt;%
        vocab_vectorizer()

train_dtm &amp;lt;- create_dtm(it_train, vectorizer)

# tf-idf transformation
tfidf = TfIdf$new()
dtm_train_tfidf = fit_transform(train_dtm, tfidf) # tfidf modified in place!

test_dtm &amp;lt;- create_dtm(it_test, vectorizer)
dtm_test_tfidf = transform(test_dtm, tfidf)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling&lt;/h2&gt;
&lt;p&gt;Now I’ll train three different models: Regularized logistic regression (glm), support vector machines (svm) and a gradient boosted machines (gbm). The first two are simple linear models remarkably apt at dealing with wide data - remember we have a 1,000 variables; while gbm is a tree-based method considered to be the top of the hill for classifiers. I won’t discuss the internals of these algorithms, and I will only explicitly tune the L1 regularization parameter for the glm (it performs both variable selection as well as coefficient shrinkage, thereby reducing model variance). Hopefully we get respectable performance at first attempt and can jump straight into some explainable machine learning stuffs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
y_train &amp;lt;- factor(train$is_gdpr, levels = c(&amp;quot;yes&amp;quot;,&amp;quot;no&amp;quot;))
y_test &amp;lt;- factor(test$is_gdpr, levels = c(&amp;quot;yes&amp;quot;,&amp;quot;no&amp;quot;))
x_train &amp;lt;- as.data.frame(as.matrix(dtm_train_tfidf))
x_test &amp;lt;- as.data.frame(as.matrix(dtm_test_tfidf))

# boosted trees, glm, svm
# Using caret to perform CV
set.seed(123)
ctrl &amp;lt;- trainControl(method=&amp;quot;cv&amp;quot;, number=3,classProbs=TRUE)

glm_fit &amp;lt;- train(x = x_train, y = y_train,
                 method=&amp;quot;glmnet&amp;quot;, trControl = ctrl,
                 tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
svm_fit &amp;lt;- train(x = x_train, y = y_train, 
                 method=&amp;quot;svmLinear&amp;quot;, trControl = ctrl)
gbm_fit &amp;lt;- train(x = x_train, y = y_train, 
                 method=&amp;quot;gbm&amp;quot;, trControl = ctrl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generating predictions for the validation set, let’s plot the performance for the three models:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2018/2018-07-19_gdpr_files/figure-html/accuracy%20n%20sensitity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Gbm has the highest accuracy by a inch with around 96%, while svm slightly outperforms the rest with 82% for sensitivity, meaning that 82% of GDPR policies are classified correctly. We are of course most interested what words are differentiating the two classes of policies. We can retrieve measures for variable importance for both the glm as well the gbm models out of the box. For glm we can use &lt;code&gt;varImp(glm_fit)&lt;/code&gt; that returns an &lt;strong&gt;importance percentage&lt;/strong&gt; for each variable that’s based on the size of the models &lt;strong&gt;coefficients&lt;/strong&gt;. For gbm we can access the &lt;strong&gt;relative influence&lt;/strong&gt; of each variable using &lt;code&gt;summary(gbm_fit)&lt;/code&gt;. Relative influence depicts the average empirical improvement of splitting by a variable across all trees generated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2018/2018-07-19_gdpr_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In both cases the word &lt;strong&gt;regulation&lt;/strong&gt; comes out as most important word (I wonder if it has anything to do with GDP&lt;strong&gt;Regulation&lt;/strong&gt;). Other than that, the words &lt;strong&gt;data&lt;/strong&gt;, &lt;strong&gt;shield&lt;/strong&gt; and &lt;strong&gt;legitimate&lt;/strong&gt; appear in both plots for the two models. The problem is of course that we are looking at two different measures for two different models. It would be better to compare the models using the same tools on the same scale using the same metrics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;descriptive-machine-learning-explanations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Descriptive mAchine Learning EXplanations&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://ico.org.uk/for-organisations/guide-to-the-general-data-protection-regulation-gdpr/individual-rights/rights-related-to-automated-decision-making-including-profiling/&#34;&gt;Article 22 of GDPR&lt;/a&gt; states user’s rights to receive information why automated decisions were made and the underlying data processing. Thanks to GDPR, there is a growing interest in developing methods and tools to investigate how predictions are generated by black box models. Enter the world of explainable machine learning frameworks.&lt;/p&gt;
&lt;p&gt;One of the newer packages in the field is &lt;a href=&#34;https://github.com/pbiecek/DALEX&#34;&gt;DALEX&lt;/a&gt;, which I learned about at &lt;a href=&#34;http://2018.erum.io/&#34;&gt;eRum 2018&lt;/a&gt;. It has a set of model agnostic tools to investigate what our model is doing. For calculating variable importance:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“… we simply subtract the loss function calculated for validation dataset with permuted values for a single variable from the loss function calculated for validation dataset. This concept and some extensions are described in (Fisher, Rudin, and Dominici 2018).”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Basically we look at how our loss function changes with the permutation of the values in each column. For this we define a prediction and loss function, create an explainer object using &lt;code&gt;explain()&lt;/code&gt;, plug into &lt;code&gt;variable_importance()&lt;/code&gt; and plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DALEX)
# predict classes
p_fun &amp;lt;- function(object, newdata){
        predict(object, newdata=newdata, type=&amp;quot;raw&amp;quot;)
}
# accuracy loss, aka error-rate
accuracy_loss &amp;lt;- function(observed, predicted) {
        Metrics::ce(observed, predicted)
}
# explainer object 1
explainer_glm &amp;lt;- explain(glm_fit, label = &amp;quot;glm&amp;quot;, data = x_test, 
                         y = y_test, predict_function = p_fun)
# explainer object 2
explainer_svm &amp;lt;- explain(svm_fit, label = &amp;quot;svm&amp;quot;, data = x_test, 
                         y = y_test, predict_function = p_fun)
# explainer object 3
explainer_gbm &amp;lt;- explain(gbm_fit, label = &amp;quot;gbm&amp;quot;, data = x_test, 
                         y = y_test, predict_function = p_fun)
set.seed(321)
vi_glm &amp;lt;- variable_importance(explainer_glm, loss_function = accuracy_loss, n_sample = -1)
vi_gbm &amp;lt;- variable_importance(explainer_gbm, loss_function = accuracy_loss, n_sample = -1)
vi_svm &amp;lt;- variable_importance(explainer_svm, loss_function = accuracy_loss, n_sample = -1)

plot(vi_glm, vi_svm, vi_gbm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2018/2018-07-19_gdpr_files/figure-html/imp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I used error rate (1-accuracy) as the loss function, hence the interpretation of the model is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The left edges on the plot represent the error rate of the full model.&lt;/li&gt;
&lt;li&gt;Length of the interval means larger loss -&amp;gt; more important variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Looking at the above plot, it confirms that gbm has the lowest overall error rate and there are words shared among the three plots such as &lt;strong&gt;shield&lt;/strong&gt;, &lt;strong&gt;controller&lt;/strong&gt; or &lt;strong&gt;regulation&lt;/strong&gt; that appear in all three plots.&lt;/p&gt;
&lt;p&gt;Knowing which variables are important is only the half the story; we would also like to investigate the nature of relationship between the predictor and the prediction. DALEX implements &lt;a href=&#34;https://journal.r-project.org/archive/2017/RJ-2017-016/RJ-2017-016.pdf&#34;&gt;Partial Dependence Plots&lt;/a&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; that show how different values of a numeric variable affect the prediction in isolation. We only need to change our prediction function to return probabilities,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_fun &amp;lt;- function(object, newdata){
        predict(object, newdata=newdata, type=&amp;quot;prob&amp;quot;)[,1]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then rerun our &lt;code&gt;explain()&lt;/code&gt; function calls from above, and plug the result into &lt;code&gt;variable_response()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdp_glm  &amp;lt;- variable_response(explainer_glm, variable = &amp;quot;shield&amp;quot;, type = &amp;quot;pdp&amp;quot;)
pdp_svm  &amp;lt;- variable_response(explainer_svm, variable = &amp;quot;shield&amp;quot;, type = &amp;quot;pdp&amp;quot;)
pdp_gbm  &amp;lt;- variable_response(explainer_gbm, variable = &amp;quot;shield&amp;quot;, type = &amp;quot;pdp&amp;quot;)

plot(pdp_glm, pdp_svm, pdp_gbm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2018/2018-07-19_gdpr_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For example, at around a tf-idf score of 0.04 for the word &lt;strong&gt;shield&lt;/strong&gt;, the glm will always predict a GDPR policy. For SVM the cure is lower and the gbm flatlines at a tf-idf score 0.005. One explanation is that our glm model is a lot smaller (thanks to L1 regularization) and so assigns greater weight to important variables. Looking at a couple more words, a similar pattern emerges:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2018/2018-07-19_gdpr_files/figure-html/pdp%202-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that for the words &lt;strong&gt;regulation&lt;/strong&gt; and &lt;strong&gt;legitimate&lt;/strong&gt;, lower tf-idf scores are sufficient for higher probability of being classified as a GDPR policy.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gdpr-is-good-for-you&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;GDPR is good for you&lt;/h1&gt;
&lt;p&gt;It only took fitting some models and creating a few plots and I already feel a lot safer under GDPR. After all, this policy is aimed at protecting your data against the sort of abuse we have been hearing about in the news of late. Not only that, but the coming of GDPR and the need for explainable machine learning has also been a boon for a fast growing collection of model interpretability tools such as &lt;a href=&#34;https://github.com/thomasp85/lime&#34;&gt;lime&lt;/a&gt;, &lt;a href=&#34;https://github.com/redichh/ShapleyR&#34;&gt;ShapleyR&lt;/a&gt;, &lt;a href=&#34;https://mi2datalab.github.io/live/&#34;&gt;live&lt;/a&gt;, &lt;a href=&#34;https://github.com/AppliedDataSciencePartners/xgboostExplainer&#34;&gt;xgboostExplainer&lt;/a&gt;, &lt;a href=&#34;https://pbiecek.github.io/breakDown/&#34;&gt;breakDown&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While text classification is perhaps an atypical usecase for DALEX, it works just as well and is suprisingly fast even with our large document-term matrix. However, I am curious to see how an actual case of someone asking for model clarification would look like under GDPR and what explanations would suffice to satisfy. Either way, the R community seems to be prepared for now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Corpus of 1,010 privacy policies from the top websites ranked on Alexa.com, created as part of: F. Liu, R. Ramanath, N. Sadeh, N.A. Smith. A Step Towards Usable Privacy Policy: Automatic Alignment of Privacy Statements. Proceedings of the 25th International Conference on Computational Linguistics (COLING). 2014.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Greenwell, Brandon M. 2017. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1):421–36. &lt;a href=&#34;https://journal.r-project.org/archive/2017/RJ-2017-016/index.html&#34; class=&#34;uri&#34;&gt;https://journal.r-project.org/archive/2017/RJ-2017-016/index.html&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dockerized Shiny App development</title>
      <link>/blog/dockerized-shiny-app-development/</link>
      <pubDate>Tue, 16 Jan 2018 22:13:14 -0500</pubDate>
      
      <guid>/blog/dockerized-shiny-app-development/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;getting-on-the-docker-container-ship&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting on the Docker (container) ship&lt;/h2&gt;
&lt;p&gt;Containers are everywhere, including the realms of data science. You can think of them as small self-contained environments, encapsulating an application and its dependencies. If that sounds a lot like a virtual machine, you are not entirely wrong. But unlike VM’s, containers run on the host system’s kernel and the processes inside can only see and access their immediate surroundings.&lt;/p&gt;
&lt;p&gt;Thanks to the good people behind the &lt;a href=&#34;https://hub.docker.com/u/rocker/&#34;&gt;rocker project&lt;/a&gt;, there’s already plenty of R-specific Docker images available for folks looking to containerize their R code. The most often cited benefits are &lt;em&gt;portability&lt;/em&gt; and &lt;em&gt;reproducibility&lt;/em&gt; of your analysis. In the same vein, &lt;a href=&#34;https://maraaverick.rbind.io/2017/11/docker-izing-your-work-in-r/&#34;&gt;lots of great material&lt;/a&gt; is out there with respect to what these bad boys exactly are and how to get them up and running.&lt;/p&gt;
&lt;p&gt;But I haven’t found much on &lt;em&gt;Docker based workflows&lt;/em&gt;, especially how to go about developing dockerized shiny apps. Because what if I want to build a shiny dashboard inside a container, integrate it with &lt;a href=&#34;https://travis-ci.org/&#34;&gt;Travis CI&lt;/a&gt; and run tests on every single commit to GitHub?&lt;/p&gt;
&lt;p&gt;The code in this post is based on a bare bones shiny app (containing USA Trade data) I built for illustration purposes. You can find the app &lt;a href=&#34;http://usatrade.tamaszilagyi.com/&#34;&gt;here&lt;/a&gt;, and all the code on &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testable-shiny-apps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testable shiny apps&lt;/h2&gt;
&lt;p&gt;We all heard of unit testing, but can we test an actual shiny application? As often the case in the R world, &lt;em&gt;there is already a package for that:&lt;/em&gt; &lt;a href=&#34;https://github.com/rstudio/shinytest&#34;&gt;shinytest&lt;/a&gt; - an automated testing agent for, you guessed it…shiny apps. It works as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Shinytest uses snapshot-based testing strategy. The first time it runs a set of tests for an application, it performs some scripted interactions with the app and takes one or more snapshots of the application’s state. These snapshots are saved to disk so that future runs of the tests can compare their results to them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The interface is super easy. You install the package and when the first version of your shiny app is ready to roll, you simply run &lt;code&gt;recordTest()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/shinytest&amp;quot;)
library(shinytest)

recordTest(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This launches an iframe consisting of your dashboard and controls over what to test. Each interaction with the dashboard is recorded, and when you hit &lt;strong&gt;take snapshot&lt;/strong&gt;, the state of your dashboard is saved, along with raw scripts to reproduce the interactions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/t3xcuCX.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Upon exiting the test event recorder, a new folder &lt;code&gt;test/&lt;/code&gt; is created inside the app’s directory, containing both the test script - &lt;code&gt;dates.R&lt;/code&gt;, as well as the application’s state as a .json and a .png files in &lt;code&gt;test/dates-expected&lt;/code&gt;. The latter serve as expected output, based on which consequent runs of tests shall be evaluated. Using my example app, &lt;code&gt;dates.R&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(shiny.testmode=TRUE)

app &amp;lt;- ShinyDriver$new(&amp;quot;../&amp;quot;, seed = 123)
app$snapshotInit(&amp;quot;dates&amp;quot;)

app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, running &lt;code&gt;testApp(&amp;quot;path/to/app&amp;quot;)&lt;/code&gt; will look for test scripts inside the &lt;code&gt;test/&lt;/code&gt; folder, and run them to recreate the state of the test recording, comparing the output to what’s expected. It is generally a good idea to only compare the .json files, because the screenshots of the app (the .png file) will likely differ of various systems. We pass the argument &lt;code&gt;compareImages = FALSE&lt;/code&gt; to bypass default behavior. A full fledged test script will then look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(testthat)
test_that(&amp;quot;Application works&amp;quot;, {
        expect_pass(testApp(&amp;quot;/srv/shiny-server/myapp/&amp;quot;,
                            testnames = &amp;quot;dates&amp;quot;,
                            compareImages = FALSE))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I found that having ggplot2 (or plotly) plots as part of your dashboard, there is always a tiny bit of randomness present in the output. And hence the tests fail. It is better to explicitly export parts of the plot objects in my opinion, because they will be a more reliable yardstick to compare against. To do so, we add a few lines of code to &lt;code&gt;server.R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exportTestValues(plot_balance = { ggplot_build(p_b)$data },
                 plot_total   = { ggplot_build(p_t)$data },
                 plot_import  = { ggplot_build(p_i)$data },
                 plot_export  = { ggplot_build(p_e)$data } )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a follow up, we customize which parts of the application’s state should be saved and checked for inside &lt;code&gt;app$snapshot()&lt;/code&gt;, using the &lt;code&gt;items =&lt;/code&gt; argument and update &lt;code&gt;dates.R&lt;/code&gt; so that only the &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;export&lt;/code&gt; (and &lt;em&gt;not the&lt;/em&gt; &lt;code&gt;output&lt;/code&gt;) sections of our .json files are evaluated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;...
app$setInputs(date1 = &amp;quot;2000-10-02&amp;quot;)
app$setInputs(date2 = &amp;quot;2013-11-01&amp;quot;)
app$snapshot(items = list(input = TRUE, export = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is all you really need to get going with shinytest. Keep in mind that the package is still in development, and things might change in the future. For an in-depth walkthrough of shinytest’s capabilities, have a look at the &lt;a href=&#34;https://rstudio.github.io/shinytest/articles/shinytest.html&#34;&gt;official site&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-container-can-we-haz-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A 🐳 container, can we haz it?&lt;/h2&gt;
&lt;p&gt;Now that our shiny app is complete with test scripts, the whole thing can be packaged up and put inside a container. Of course we could deploy the shiny dashboard without a container too, but at the end of the day it makes everybody’s life a lot easier.&lt;/p&gt;
&lt;p&gt;Because if our container runs on our machine, it will also run on &lt;strong&gt;any machine&lt;/strong&gt; that has Docker. Without compatibility issues, independent from host version or platform distribution. In a real life scenario this significantly reduces time between prototypting and deployment, not the least because of the typically lightweight footprint of a Docker image.&lt;/p&gt;
&lt;p&gt;To containerize our shiny app, we first need to create an image that encompasses our:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Shiny application&lt;/li&gt;
&lt;li&gt;R packages our app needs&lt;/li&gt;
&lt;li&gt;System level dependencies these packages need&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We build our image layer by layer, starting with the &lt;a href=&#34;https://hub.docker.com/r/rocker/shiny/&#34;&gt;rocker/shiny image&lt;/a&gt; - which includes the minimal requirements for a Shiny Server. Then, we add everything else our application requires; finishing with copying the contents of our app to &lt;code&gt;/srv/shiny-server/usa-trade/&lt;/code&gt;, where the dashboard will be served from. These instructions are written to the &lt;code&gt;Dockerfile&lt;/code&gt;, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FROM rocker/shiny
MAINTAINER Tamas Szilagyi (tszilagyi@outlook.com)

## install R package dependencies (and clean up)
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2 \
    libssl-dev \
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## install packages from CRAN (and clean up)
RUN Rscript -e &amp;quot;install.packages(c(&amp;#39;devtools&amp;#39;,&amp;#39;dplyr&amp;#39;,&amp;#39;tidyr&amp;#39;,&amp;#39;fuzzyjoin&amp;#39;,&amp;#39;stringr&amp;#39;,&amp;#39;ggthemes&amp;#39;,&amp;#39;quantmod&amp;#39;,&amp;#39;ggplot2&amp;#39;,&amp;#39;shinydashboard&amp;#39;,&amp;#39;shinythemes&amp;#39;), repos=&amp;#39;https://cran.rstudio.com/&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install packages from github (and clean up)
RUN Rscript -e &amp;quot;devtools::install_github(&amp;#39;rstudio/shinytest&amp;#39;,&amp;#39;rstudio/webdriver&amp;#39;)&amp;quot; \
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

## install phantomjs
RUN Rscript -e &amp;quot;webdriver::install_phantomjs()&amp;quot;

## assume shiny app is in build folder /app2
COPY ./app2 /srv/shiny-server/usa-trade/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The smaller your Docker image, the better. Here’s a couple of guidelines to keep in mind when creating one:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Always use &lt;strong&gt;shared base images&lt;/strong&gt; (what comes after the &lt;code&gt;FROM&lt;/code&gt; statement) specific to your application, instead of trying to reinvent the wheel every time you write a Dockerfile.&lt;/li&gt;
&lt;li&gt;Try to &lt;strong&gt;avoid underused dependencies&lt;/strong&gt;. Going back to the my example app, I could’ve installed the package &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyquant/index.html&#34;&gt;tidyquant&lt;/a&gt; to get my trade data in a tidy format out of the box, yet because the package has an insane amount of dependencies (including having Java installed); I wrote three &lt;a href=&#34;https://github.com/mtoto/markets_shiny/blob/master/app2/functions.R#L14&#34;&gt;helper functions&lt;/a&gt; instead.&lt;/li&gt;
&lt;li&gt;Make sure &lt;strong&gt;temporary files are removed&lt;/strong&gt; after the installation of libraries and packages.&lt;/li&gt;
&lt;li&gt;Push down commands that will likely invalidate the &lt;strong&gt;cache&lt;/strong&gt;, so Docker only rebuilds layers that change (more on this in the next section).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the Dockerfile finished, it is time to make ourselves familiar with the essential Docker commands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;docker pull&lt;/strong&gt; pulls an image from the registry (Dockerhub).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker build&lt;/strong&gt; builds a docker image from our Dockerfile.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker run&lt;/strong&gt; instantiates the container from our image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker exec&lt;/strong&gt; execute commands from within the container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker rm&lt;/strong&gt; deletes a container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker login&lt;/strong&gt; login to Dockerhub (to upload our image).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docker push&lt;/strong&gt; uploads the image back to Dockerhub.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s say we want to run our shiny app on a server that has Docker installed. Assuming we have a &lt;a href=&#34;https://github.com/mtoto/markets_shiny&#34;&gt;GitHub repo&lt;/a&gt; containing all relevant files and our Dockerfile is to be found on &lt;a href=&#34;https://hub.docker.com/r/mtoto/shiny/&#34;&gt;Dockerhub&lt;/a&gt;, we can expose our shiny app to the world as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1 clone into repo containing app 
git clone https://github.com/mtoto/markets_shiny.git
# 2 pull Docker file from Dockerhub
docker pull mtoto/shiny:latest
# 3 build Docker image, tag it &amp;#39;mtoto/shiny:latest&amp;#39;
docker build -t mtoto/shiny:latest .
# 4 run container in detached mode, listening on port 80, name it &amp;#39;site&amp;#39;
docker run -d -p 80:3838 --name site mtoto/shiny:latest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And our app should be visible on &lt;em&gt;ht​ps://myserver.com/usa-trade&lt;/em&gt; by default.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integration-with-travis-ci&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integration with Travis CI&lt;/h2&gt;
&lt;p&gt;If you are a seasoned R package developer, you are no stranger to Travis CI. It is a &lt;strong&gt;Continuous Integration&lt;/strong&gt; tool that automatically performs checks and runs tests on your code every time you push a commit to GitHub. The broad idea behind continuous integration is to encourage test-driven development, thereby allowing for frequent commits to the codebase without having to worry about integration problems.&lt;/p&gt;
&lt;p&gt;Travis supports many languages - including R, and can also build from Docker images. After creating an account on the &lt;a href=&#34;https://travis-ci.org&#34;&gt;Travis website&lt;/a&gt;, connect with GitHub and pick the repository for which you’d like to use it.&lt;/p&gt;
&lt;p&gt;The repo needs to contain a &lt;code&gt;.travis.yml&lt;/code&gt; file, encapsulating the instructions for Travis.
You’d tempted to write &lt;code&gt;language: R&lt;/code&gt; as the first line, but if we do that Travis will implicitly assume we are developing an R package and will start looking for the &lt;code&gt;DESCRIPTION&lt;/code&gt; file we do not have. Instead, I went with the undocumented option &lt;code&gt;language: generic&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, as we’ll be only running Docker commands anyway.&lt;/p&gt;
&lt;p&gt;The naive approach would be to build our Docker image on every single run, instantiate a test container, run tests inside and upon success get rid of the container. Such a &lt;code&gt;.travis.yml&lt;/code&gt; would look like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker build -t markets-shiny .
- docker run -d -p 3838:3838 markets-shiny:latest --name test

script:
- docker exec test R -f run_tests.R

after_script:
- docker rm -f test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem here is that we are building the Docker image from scratch with every single Travis run, resulting in a build time of over 20 minutes for my simple app. But our image is on Dockerhub, so why not pull it from there and take advantage of caching. Then, we’d only rebuild the changed layers after downloading the latest image from the registry.&lt;/p&gt;
&lt;p&gt;To make sure everything is nice and up to date, we will push the changes back to Dockerhub after every successful run. We need credentials to do so, but Travis conveniently allows for defining environment variables inside the repository settings (or via the CLI):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/gdTnLjd.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can go wild and revamp &lt;code&gt;.travis.yml&lt;/code&gt; accordingly:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;language: generic
sudo: required

services:
- docker

before_install:
- docker pull mtoto/shiny:latest
- docker build --cache-from mtoto/shiny:latest -t mtoto/shiny:latest . 
- docker run --rm -d -p 3838:3838 --name test mtoto/shiny:latest

script:
- docker exec test R -f /srv/shiny-server/usa-trade/run_tests.R

after_success:
- docker rm -f test
- docker login -u mtoto -p $DOCKER_PASSWORD
- docker tag mtoto/shiny:latest mtoto/shiny:$TRAVIS_BUILD_NUMBER
- docker push mtoto/shiny&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the second run (once the latest image is on Dockerhub), the build time is reduced by a factor of 10. Sweet. When we use the flag &lt;code&gt;--cache-from&lt;/code&gt;, Docker only rebuilds changed layers, ie. modifications to our shiny app. We can see this in the Travis log as &lt;code&gt;---&amp;gt; Using cache&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/hTNuQhY.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Keep in mind when making significant changes to your dashboard, it is important to update the tests that create fresh expected outputs reflecting these changes. If you don’t trust the outputs will align, remember to use &lt;code&gt;exportTestValues()&lt;/code&gt; and fill it up with the new objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;was-it-all-worth-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Was it all worth it?&lt;/h2&gt;
&lt;p&gt;While this workflow might feel like over-engineering, once all the tools are set up to work in tandem, shiny dashboard development becomes surprisingly efficient. The icing on the cake is that you are creating a dashboard that is pretty much ready for deployment from day one. Devops will love you for it, trust me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In reality this points to &lt;code&gt;language: bash&lt;/code&gt;, &lt;code&gt;language: sh&lt;/code&gt; and &lt;code&gt;language: shell&lt;/code&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An animated neuRal net implementation</title>
      <link>/blog/an-animated-neural-net-implementation/</link>
      <pubDate>Thu, 09 Nov 2017 21:00:30 -0500</pubDate>
      
      <guid>/blog/an-animated-neural-net-implementation/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;yet-another-neural-net-from-scratch-tutorial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Yet another neural net from scratch tutorial?&lt;/h1&gt;
&lt;p&gt;One would be forgiven to think that artificial neural networks are the newest and shiniest of modern data science. On the contrary, the main concepts have been around for decades. But it is recent progress in computational resources and the availability of massive datasets that these learning architectures revealed their true powers. AlphaGo, Siri and Alexa, self-driving cars are all running on some form or other of deep artificial neural networks.&lt;/p&gt;
&lt;p&gt;The hype means the Internet is aflush with tutorials and online resources to get started. Yet, somehow R hasn’t gotten much street cred in the area. Most of the frameworks are implemented in Python, and so are the tutorials. R is supposed to be the de facto lingua franca of statistical computing, so what’s up with that?&lt;/p&gt;
&lt;p&gt;What follows is a custom build of a simple one hidden-layer neural network, where we’ll save just enough parameters at every iteration to be able to &lt;a href=&#34;https://github.com/dgrtwo/gganimate&#34;&gt;gganimate&lt;/a&gt; the training process afterwards.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-main-ingredients&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The main ingredients&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-11-11-animated_net_files/figure-html/fig2-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This post is mostly inspired by &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Andrew Ng’s Deep Learning course&lt;/a&gt; (including the dataset), which I strongly recommend for anyone interested in neural networks. The task is to predict the color of the points in the plot on the right. While it seems like a trivial problem, linear algorithms will inevitably fail at it because the colors are not linearly separable. There’s no single line we can draw that perfectly separates the red dots from the blue dots.&lt;/p&gt;
&lt;div id=&#34;how-does-the-algorithm-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does the algorithm work?&lt;/h2&gt;
&lt;p&gt;The input layer contains the input data. The number of nodes in the input layer is &lt;em&gt;always&lt;/em&gt; equal to the number of features in the data (X, Y coordinates). The input is then &lt;strong&gt;1. propagated forward&lt;/strong&gt; through the hidden layer. Number of hidden nodes and number of hidden layers can be modified at will. The edges between the nodes represent the weights, and the prediction is essentially a function of these weights.&lt;/p&gt;
&lt;p&gt;Once the data has been passed through the entire network, we get the predictions in the output node and &lt;strong&gt;2. compute the cost&lt;/strong&gt; with respect to the actual labels. At each iteration, we adjust the weights to minimize this cost as much possible.&lt;/p&gt;
&lt;p&gt;How do we do that? That’s the job of &lt;strong&gt;3. backward propagation&lt;/strong&gt;. By means of gradient descent, we calculate the partial derivatives of all computations with respect to what came after, alas we go &lt;em&gt;backwards&lt;/em&gt;. First the derivatives of the weights of the hidden layer with respect to the output layer, and secondly those of the input layer with respect to the hidden layer. The gradients we obtain are then used to &lt;strong&gt;update the weights&lt;/strong&gt; and start the process all over again. With each pass - also called &lt;strong&gt;epochs&lt;/strong&gt;, we get closer to the optimal weights.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;down-the-rabbit-hole&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Down the rabbit hole&lt;/h1&gt;
&lt;p&gt;I will now explain in short and code up each of the three computations. The scripts we define will be used inside a single function call that trains the neural network. Significant overhead will be introduced by saving parameters at every iteration, but hopefully the animated plots will be worth it.&lt;/p&gt;
&lt;div id=&#34;forward-propagation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Forward propagation&lt;/h2&gt;
&lt;p&gt;Forward propagation is the first pass through the data, calculating an output based on the weights of each edge. The connections from the input layer to each of the hidden nodes is a linear function, followed by an activation function.&lt;/p&gt;
&lt;p&gt;The computational steps of forward propagations are &lt;span class=&#34;math inline&#34;&gt;\(Input -&amp;gt; Hidden -&amp;gt; Output\)&lt;/span&gt; .&lt;/p&gt;
&lt;p&gt;If we break down each of the two connections into a linear function &lt;span class=&#34;math inline&#34;&gt;\(Z^{[i]}\)&lt;/span&gt; and an activation function &lt;span class=&#34;math inline&#34;&gt;\(A^{[i]}\)&lt;/span&gt;, the architecture becomes &lt;span class=&#34;math inline&#34;&gt;\(X -&amp;gt;Z^{[1]}-&amp;gt;A^{[1]}-&amp;gt;Z^{[2]}-&amp;gt;A^{[2]}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as the input data.&lt;/p&gt;
&lt;p&gt;The activation function is usually a non-linear function that enables the network to cope with non-linear problems. Examples include the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sigmoid_function&#34;&gt;sigmoid function&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_neural_networks&#34;&gt;relu&lt;/a&gt; or &lt;a href=&#34;http://mathworld.wolfram.com/HyperbolicTangent.html&#34;&gt;tanh&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take for example the connections going &lt;strong&gt;from the input layer to one hidden node&lt;/strong&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(X_{m,n}\)&lt;/span&gt; is the vertically stacked dataset where &lt;em&gt;m = number of features (2)&lt;/em&gt; , &lt;em&gt;n = number of observations&lt;/em&gt;, &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; is a weight vector of length &lt;em&gt;m&lt;/em&gt;; the linear function in one hidden node can be formally represented as a matrix vector product:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}

w =
 \begin{pmatrix}
  w_{1} \\
  w_{2} \\
 \end{pmatrix};
 X = 
 \begin{pmatrix}
  x_{1,1} &amp;amp; x_{1,2} &amp;amp; \cdots &amp;amp; x_{1,n} \\
  x_{2,1} &amp;amp; x_{2,2} &amp;amp; \cdots &amp;amp; x_{2,n} \\
 \end{pmatrix}
 \end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
  Z = {w^T}X + b= 
  \begin{pmatrix}
  w_{1} &amp;amp; w_{2} \\
 \end{pmatrix}
  \begin{pmatrix}
  x_{1,1} &amp;amp; x_{1,2} &amp;amp; \cdots &amp;amp; x_{1,n} \\
  x_{2,1} &amp;amp; x_{2,2} &amp;amp; \cdots &amp;amp; x_{2,n} \\
 \end{pmatrix} 
  + b
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
  Z = 
    \begin{pmatrix}
      w_{1}x_{1,1} + 
      w_{2}x_{2,1}+b &amp;amp; 
      w_{1}x_{1,2}+ + 
      w_{2}x_{2,2}+b &amp;amp;
      \cdots &amp;amp;
      w_{1}x_{1,n} + 
      w_{2}x_{2,n}+b
    \end{pmatrix}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The activation function &lt;span class=&#34;math inline&#34;&gt;\(A^{[1]}\)&lt;/span&gt; is the &lt;em&gt;tanh&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(A^{[1]} = \tanh(Z^{[1]})\)&lt;/span&gt;, for the output layer we’ll use the sigmoid instead &lt;span class=&#34;math inline&#34;&gt;\(A^{[2]} = \sigma(Z^{[2]})\)&lt;/span&gt;. The computation can also be visualised as a subgraph of our neural network:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-11-11-animated_net_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;412.8&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It turns out that this implementation scales to multiple hidden nodes without any formal change to the math. Instead of a &lt;em&gt;weight vector&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;, we are computing the same functions using a &lt;em&gt;weight matrix&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. The matrix-vector product now becomes a dot product between the two matrices. With each node in the hidden layer, we add an extra row in the transposed weight matrix. The dimensionality requirements of matrix multiplication are kept intact: &lt;em&gt;The number of columns of first matrix still equal the number of rows of the second&lt;/em&gt;. But the dimensions of the output change accordingly. We go from a transposed vector of length n to an m x n matrix where &lt;em&gt;m = the number of hidden nodes&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
Z = {W^T}X + b= 
  \begin{pmatrix}
  w_{1,1} &amp;amp; w_{1,2} \\
  \vdots  &amp;amp; \vdots  \\
  w_{n,1} &amp;amp; w_{n,2}  
 \end{pmatrix}
  \begin{pmatrix}
  x_{1,1} &amp;amp; \cdots &amp;amp; x_{1,n} \\
  x_{2,1} &amp;amp; \cdots &amp;amp; x_{2,n} \\
 \end{pmatrix} 
  + b
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
  Z = 
    \begin{pmatrix}
      w_{1,1}x_{1,1} + 
       w_{1,2}x_{2,1}+b &amp;amp; 
      \cdots &amp;amp;
      w_{1,1}x_{1,n} + 
      w_{1,2}x_{2,n}+b 
      \\
      \vdots  &amp;amp; \ddots &amp;amp; \vdots\\
      w_{n,1}x_{1,1} + 
      w_{n,2}x_{2,1}+b &amp;amp; 
      \cdots &amp;amp;
      w_{n,1}x_{1,n} + 
       w_{n,2}x_{2,n}+b 
    \end{pmatrix}
  \end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-11-11-animated_net_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The last step of going from the hidden layer to the output layer follows the same algebra. I’ll spare you the nitty gritty. Before we propagate forward for the first time, it is important to &lt;strong&gt;randomly initialize the weights&lt;/strong&gt;. Otherwise each connection will compute &lt;em&gt;the exact same thing&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initialize_parameters &amp;lt;- function(n_x, n_h, n_y) {

    set.seed(2) 
    # W1 -- weight matrix of shape (n_h, n_x)
    W1 = matrix(rnorm(n_x*n_h), nrow = n_h, ncol = n_x) * 0.01
    # b1 -- bias vector of shape (n_h, 1)
    b1 = rep(0, n_h)
    # W2 -- weight matrix of shape (n_y, n_h)
    W2 = matrix(rnorm(n_h*n_y),  nrow = n_y, ncol = n_h) * 0.01
    # b2 -- bias vector of shape (n_y, 1)
    b2 = rep(0, n_y)

    parameters = list(W1 = W1,b1 = b1,W2 = W2,b2 = b2)
    
    return(parameters)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember the schema is &lt;span class=&#34;math inline&#34;&gt;\(X -&amp;gt;Z^{[1]}-&amp;gt;A^{[1]}-&amp;gt;Z^{[2]}-&amp;gt;A^{[2]}\)&lt;/span&gt;. Both &lt;span class=&#34;math inline&#34;&gt;\(Z^{[1]}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z^{[2]}\)&lt;/span&gt; are the same linear function, while &lt;span class=&#34;math inline&#34;&gt;\(A^{[1]} = \tanh(Z^{[1]})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{[2]} = \sigma(Z^{[2]})\)&lt;/span&gt;. The sigmoid function didn’t make it to &lt;code&gt;base&lt;/code&gt; R, so we define it first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmoid &amp;lt;- function(x) {
   1 / (1 + exp(-x))
}

forward_propagation &amp;lt;- function(X, parameters) {
  
    # Retrieve each parameter from the list &amp;quot;parameters&amp;quot;
    W1 &amp;lt;- parameters$W1; b1 &amp;lt;- parameters$b1
    W2 &amp;lt;- parameters$W2; b2 &amp;lt;- parameters$b2

    # Forward propagation
    Z1 = W1 %*% X + b1
    A1 = tanh(Z1)
    Z2 = W2 %*% A1 + b2
    A2 = sigmoid(Z2)
    
    cache &amp;lt;- list(Z1=Z1,A1=A1,Z2=Z2,A2=A2)
    
    return(cache)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each pass of forward propagation ends with a prediction. Generating a prediction for every pixel of our plot raster, we can simulate decision boundaries. As the algorithm learns, the borders slowly align with the data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/test_bounds.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computing-the-cost&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Computing the cost&lt;/h2&gt;
&lt;p&gt;As we have seen above, forward propagation is nothing more than a &lt;em&gt;predict&lt;/em&gt; function. When the dataset has been passed through the network, we get an output that can be compared to the actual label. The purpose of the cost function is to inform the model how far the output is from the target value. One of the most popular cost functions is log loss, formally known as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small Y\log\left(A^{[2]}\right) + (1-Y)\log\left(1- A^{[2]}\right)  \large  \right) \small \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_cost &amp;lt;- function(A2, Y) {
        
        # Number of observations
        m &amp;lt;- dim(Y)[2] 
        
        cost &amp;lt;- -1/m * sum(Y * log(A2) + (1-Y)*log(1-A2))
        
        return(cost)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You saw how the algorithm was getting better at predicting the colors with each iteration. This is the result of reducing the cost as the model learns.
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/costs.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backward-propagation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Backward propagation&lt;/h2&gt;
&lt;p&gt;Out of all building blocks of neural networks, back propagation is perhaps the most difficult to grasp. In a nutshell, it is calculating the error contribution of each weight to cost. The idea is to backward engineer the derivative or &lt;em&gt;slope&lt;/em&gt; of every computation and update the weights so that the cost will decrease at each iteration.&lt;/p&gt;
&lt;p&gt;We first calculate the gradient of &lt;span class=&#34;math inline&#34;&gt;\(Z^{[2]}\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(A^{[2]}\)&lt;/span&gt;, this is equal to &lt;span class=&#34;math inline&#34;&gt;\(dZ^{[2]} = A^{[2]} - Y\)&lt;/span&gt;. Based on &lt;span class=&#34;math inline&#34;&gt;\(dZ^{[2]}\)&lt;/span&gt; we then calculate the gradients of the weights (and bias terms) going from the hidden layer to the output layer. We continue going backwards until we obtain the gradients for all the weights and bias terms.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A^{[2]} -&amp;gt;dZ^{[2]}-&amp;gt;A^{[1]} -&amp;gt; dZ^{[1]} \\ \quad \quad  \downarrow  \quad \quad \quad \quad \quad  \quad  \downarrow \\ \quad \quad \quad dW^{[2]},db^{[2]} \quad  \quad    dW^{[1]},db^{[1]}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Below is the list of formulae we will need for the computations. Drilling further into the math is beyond the scope of this post, but there are &lt;a href=&#34;http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4&#34;&gt;great blog posts around dedicated to the topic&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}

 dZ^{[2]} = A^{[2]} - Y &amp;amp;
 \\ dW^{[2]} = \frac{1}mdZ^{[2]}A^{[1]T} &amp;amp;
 \\ db^{[2]} = \frac{1}m\sum_{n=1}^{m} dZ^{[2]} &amp;amp;
 \\ dZ^{[1]} = W^{[2]T}dZ^{[2]} * (1-A^{[1]2}) &amp;amp;
 \\ dW^{[1]} = \frac{1}mdZ^{[1]}X^{T} &amp;amp;
 \\ db^{[1]} = \frac{1}m\sum_{n=1}^{m} dZ^{[1]}
 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The math certainly looks scarier than &lt;code&gt;R&lt;/code&gt; code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;backward_propagation &amp;lt;-function(parameters, cache, X, Y) {
    
    m = dim(X)[2]
    
    # Retrieve W2 
    W2 &amp;lt;- parameters$W2

    # Retrieve A1 and A2
    A1 &amp;lt;- cache[&amp;quot;A1&amp;quot;][[1]]; A2 &amp;lt;- cache[&amp;quot;A2&amp;quot;][[1]]

    # Backward propagation
    dZ2 &amp;lt;- A2 - Y
    dW2 &amp;lt;- 1/m * dZ2 %*% t(A1)
    db2 &amp;lt;- 1/m * sum(dZ2)
    dZ1 &amp;lt;- t(W2) %*% dZ2 * (1 - A1^2)
    dW1 &amp;lt;- 1/m * dZ1 %*% t(X)
    db1 &amp;lt;- 1/m * sum(dZ1)

    grads &amp;lt;- list(dW1 = dW1,db1 = db1, dW2 = dW2,db2 = db2)
    
    return(grads)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having obtained the gradients, we can choose a &lt;strong&gt;learning rate&lt;/strong&gt; - the size of the step - at which we wish to update the weights at each iteration. This will be the heart of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;gradient descent&lt;/a&gt; optimization we will shorty define.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update_parameters &amp;lt;- function(parameters, grads, learning_rate = 5.2) {

    # Retrieve parameters
    W1 &amp;lt;- parameters$W1; b1 &amp;lt;- parameters$b1
    W2 &amp;lt;- parameters$W2; b2 &amp;lt;- parameters$b2

    # Retrieve gradients
    dW1 &amp;lt;- grads$dW1; db1 &amp;lt;- grads$db1
    dW2 &amp;lt;- grads$dW2; db2 &amp;lt;- grads$db2

    # Update rule for each parameter
    W1 &amp;lt;- W1 - learning_rate * dW1
    b1 &amp;lt;- b1 - learning_rate * db1
    W2 &amp;lt;- W2 - learning_rate * dW2
    b2 &amp;lt;- b2 - learning_rate * db2

    parameters &amp;lt;- list(W1 = W1, b1 = b1, W2 = W2, b2 = b2)

    return(parameters)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weight adjustments are the most dramatic at the start of the training process. As the slope towards the optimum value flattens, the rate at which weights adjust slows down as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/test_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bringing-it-all-together&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bringing it all together&lt;/h2&gt;
&lt;p&gt;Now we have all the ingredients of a neural network, it’s only a matter of putting the pieces together in one function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidygraph)

nn_model &amp;lt;- function(X, Y, n_h, num_iterations = 1000) {

    set.seed(3)
    n_x &amp;lt;- 2
    n_y &amp;lt;- 1
    
    # Initialize parameters
    parameters &amp;lt;- initialize_parameters(n_x, n_h, n_y)
    list_of_graphs &amp;lt;- list()
    list_of_params &amp;lt;- list()
    costs &amp;lt;- c()
    # Loop: gradient descent
    for (i in 0:num_iterations){
         
        # Forward propagation
        cache &amp;lt;- forward_propagation(X, parameters)
        A2 &amp;lt;- cache[&amp;quot;A2&amp;quot;][[1]]
        
        # Cost function
        cost &amp;lt;- compute_cost(A2, Y)
 
        # Backpropagation
        grads &amp;lt;- backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update
        parameters &amp;lt;- update_parameters(parameters, grads)
        
        # Save intermediate weights for plotting
        w &amp;lt;- c(as.vector(t(parameters$W1)), as.vector(parameters$W2))
        
        train_df &amp;lt;- dfg %&amp;gt;% activate(edges) %&amp;gt;% 
                mutate(weights = w, iteration = i) %&amp;gt;%
                as_tbl_graph()
        
        list_of_params[[i+1]] &amp;lt;- parameters
        list_of_graphs[[i+1]] &amp;lt;- train_df
        costs[i+1] &amp;lt;- cost 
        
    }

    return(list(list_of_params, list_of_graphs, costs))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under &lt;code&gt;# save intermediate weights for plotting&lt;/code&gt; is the overhead introduced by saving objects for the animations throughout this post. The only thing left is the predict function, and we are good to go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_nn&amp;lt;-function(parameters, X) {

    # Forward propagation 
    cache = forward_propagation(X, parameters)
    # Classify 0/1 with 0.5threshold
    predictions = (cache[&amp;quot;A2&amp;quot;][[1]]&amp;gt; 0.5)

    return(predictions)
}
# Run the model: 
# model &amp;lt;- nn_model(X,Y,n_h=4,100)
# Predict - 100th iteration weights:
# predictions = predict_nn(model[[1]][[100]], X)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/data/2017-11-08-net/result.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the plots, I used the packages &lt;a href=&#34;https://github.com/tidyverse/ggplot2&#34;&gt;ggplot&lt;/a&gt;, &lt;a href=&#34;https://github.com/thomasp85/ggraph&#34;&gt;ggraph&lt;/a&gt; &lt;a href=&#34;https://github.com/thomasp85/ggraph&#34;&gt;gganimate&lt;/a&gt;, &lt;a href=&#34;https://github.com/thomasp85/tweenr&#34;&gt;tweenr&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/animation/index.html&#34;&gt;animation&lt;/a&gt;. Thanks to the creators of these awesome tools, I was able to make all the gifs using only R code. The full script for each of the animations is in the Appendix section at the &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/blog/2017/2017-11-11-animated_net.Rmd&#34;&gt;bottom of this .Rmd file&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A tidy text analysis of Rick and Morty</title>
      <link>/blog/a-tidy-text-analysis-of-rick-and-morty/</link>
      <pubDate>Sat, 07 Oct 2017 23:15:14 -0500</pubDate>
      
      <guid>/blog/a-tidy-text-analysis-of-rick-and-morty/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/a841k9g.gif&#34; /&gt;&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;adventures-in-the-multiverse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adventures in the multiverse&lt;/h2&gt;
&lt;p&gt;For those unfamiliar with the show, Rick and Morty is an animated series about the interuniversal exploits of a half-drunk mad scientist Rick, and his daft grandson Morty. Living under one roof with his daughter, Rick constantly drags his grandson Morty along for adventures into unusual worlds inhabited by surreal creatures. At first hesitant to accompany his eccentric granddad, Morty slowly grows into an indispensable sidekick. Using Rick’s portal gun, they leave the rest of their dysfunctional family at home, and travel through space and time.&lt;/p&gt;
&lt;p&gt;Most episodes draw inspiration from or make fun of cult movies such as Back to the Future, A Nightmare on Elm Street, Inception and many other classics by the likes of John Carpenter or David Cronenberg. Besides the ruthless humor and over-the-top visual scenery, the show brilliantly builds independent sci-fi realms, going about their day-to-day according to their wacky rules.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-mans-weekend-project-another-mans-treasure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One man’s weekend project, another man’s treasure&lt;/h2&gt;
&lt;p&gt;After reading the book &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;Tidy Text Mining&lt;/a&gt; online, I have been wanting to try out some of the concepts outlined in the book, and the functions of the &lt;a href=&#34;https://github.com/juliasilge/tidytext&#34;&gt;accompanying package&lt;/a&gt;, on an interesting dataset. So I was pretty stoked to find &lt;a href=&#34;https://github.com/fkeck/subtools&#34;&gt;Francois Keck’s &lt;strong&gt;subtools package&lt;/strong&gt; on GitHub&lt;/a&gt;, that allows for reading &lt;em&gt;.srt&lt;/em&gt; files (the usual format for subtitles) straight into R. With season 3 of Rick and Morty coming to an end last week, the stars have finally aligned to roll up my sleeves and have some fun with text mining.&lt;/p&gt;
&lt;p&gt;It is very easy to find English subtitles for pretty much anything on the Internet. With subtools, an entire series can be read with one command from the containing folder, &lt;code&gt;read.subtitles.serie()&lt;/code&gt;. We convert the resulting MultiSubtitles object to a data.frame with a second command &lt;code&gt;subDataFrame()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(subtools)
a &amp;lt;- read.subtitles.serie(dir = &amp;quot;/series/rick and morty/&amp;quot;)
df &amp;lt;- subDataFrame(a)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Read: 3 seasons, 31 episodes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    16821 obs. of  8 variables:
##  $ ID          : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
##  $ Timecode.in : chr  &amp;quot;00:00:02.445&amp;quot; &amp;quot;00:00:03.950&amp;quot; &amp;quot;00:00:05.890&amp;quot; &amp;quot;00:00:07.420&amp;quot; ...
##  $ Timecode.out: chr  &amp;quot;00:00:03.850&amp;quot; &amp;quot;00:00:05.765&amp;quot; &amp;quot;00:00:07.295&amp;quot; &amp;quot;00:00:08.925&amp;quot; ...
##  $ Text        : chr  &amp;quot;Morty, you got to... come on.&amp;quot; &amp;quot;- You got to come with me. - Rick, what&amp;#39;s going on?&amp;quot; &amp;quot;I got a surprise for you, Morty.&amp;quot; &amp;quot;It&amp;#39;s the middle of the night. What are you talking about?&amp;quot; ...
##  $ season      : chr  &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; &amp;quot;Season_1&amp;quot; ...
##  $ season_num  : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ episode_num : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ serie       : chr  &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; &amp;quot;rick and morty&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;$Text&lt;/code&gt; column contains the subtitle text, surrounded by additional variables for line id, timestamp, season and episode number. This is the structure preferred by the tidytext package, as it is by the rest of tidyverse.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;morty-you-got-tocome-on.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;“Morty, you got to…come on.”&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Let’s start with the bread and butter of text mining, &lt;em&gt;term frequencies&lt;/em&gt;. We split the text by word, exclude stop words,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(stop_words)
tidy_df &amp;lt;- df %&amp;gt;%
  unnest_tokens(word, Text) %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and aggregate and plot the top 10 words per season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)

tidy_df %&amp;gt;% group_by(season) %&amp;gt;%
        count(word, sort = TRUE) %&amp;gt;%
        top_n(10) %&amp;gt;%
        ggplot(aes(reorder(word,n), n, fill = season)) +
        geom_col() +
        coord_flip() +
        facet_wrap(~season, scales = &amp;quot;free_y&amp;quot;) +
        labs(x = NULL) +
        guides(fill = FALSE) +
        scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-6-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both seasons are dominated by, well, Rick and Morty. The main characters are tirelessly addressing each other, talking one another either into or out of the mess they find themselves in. What stands out most is the absence of Rick’s daughter, Beth from the top 10 in all seasons. She’s perhaps the only sane person of the family, but then again, sanity doesn’t get too much airtime on this show.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;network-analysis-on-bi-grams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Network analysis on bi-grams&lt;/h2&gt;
&lt;p&gt;We can similarly get the number of times each &lt;em&gt;two words&lt;/em&gt; appear, called &lt;em&gt;bi-grams&lt;/em&gt;. Besides calculating summary statistics on bi-grams, we can now construct a network of words according to co-occurrence using &lt;a href=&#34;https://cran.r-project.org/web/packages/igraph/index.html&#34;&gt;igraph&lt;/a&gt;, the go-to package for network analysis in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(igraph)

bigram_graph &amp;lt;- df %&amp;gt;%
  unnest_tokens(bigram, Text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;%
  filter(!word1 %in% stop_words$word) %&amp;gt;%
  filter(!word2 %in% stop_words$word) %&amp;gt;% 
  group_by(season) %&amp;gt;%
  count(word1, word2, sort = TRUE) %&amp;gt;%
  select(word1, word2, season, n) %&amp;gt;%
  filter(n &amp;gt; 2) %&amp;gt;%
  graph_from_data_frame()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in graph_from_data_frame(.): In `d&amp;#39; `NA&amp;#39; elements were replaced
## with string &amp;quot;NA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(bigram_graph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IGRAPH 04215db DN-- 311 283 -- 
## + attr: name (v/c), season (e/c), n (e/n)
## + edges from 04215db (vertex names):
## [1] NA      -&amp;gt;NA      NA      -&amp;gt;NA      NA      -&amp;gt;NA      tiny    -&amp;gt;rick   
## [5] yeah    -&amp;gt;yeah   
##  [ reached getOption(&amp;quot;max.print&amp;quot;) -- omitted 21 entries ]
## + ... omitted several edges&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This igraph object contains a directed network, where the vertices are the words and an edge exists between each that appear after one another more than twice. Representing the text as a graph, we can calculate things such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Centrality#Degree_centrality&#34;&gt;degree centrality&lt;/a&gt;, and plot the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-9-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the largest connected network, we arrive at the same conclusion as with term frequencies. Rick and Morty are the most important words. They are at the center of the network and so have the highest degree centrality scores.&lt;/p&gt;
&lt;p&gt;Besides visualising the importance of words in our network, we can similarly differentiate between words that precede either Rick or Morty. These are all the 1st degree connections (words) that have an edge pointing towards the main characters, but aren’t shared among the them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-10-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the red nodes, we recognize many of the things Rick throws at Morty: &lt;em&gt;“Relax Morty!…It’s science Morty!…Run Morty!”&lt;/em&gt;. There is also a handful of words that precede both characters like &lt;em&gt;“Geez”, “Boy”&lt;/em&gt; or &lt;em&gt;“God”&lt;/em&gt;. All other words that are more than one degree away, are colored blue as out of range.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tf-idf&lt;/h2&gt;
&lt;p&gt;Thus far we have looked at all words across seasons. But where do the seasons differ from each other? And can we summarise each season using a handful of topics? To answer the first question, text mining’s most notorious statistic &lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;&lt;strong&gt;tf-idf&lt;/strong&gt;&lt;/a&gt; comes to the rescue. It stands for &lt;strong&gt;term frequency - inverse document frequency&lt;/strong&gt;. We take the word counts per season and multiply it by the &lt;em&gt;scaled inverse fraction of seasons that contain the word&lt;/em&gt;. Simply put, we penalize words that are common across all seasons, and reward ones that are not. This way, we bring forth the words most typical of each season. Again the tidytext implementation is super easy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tf_idf_df &amp;lt;- tidy_df %&amp;gt;% 
        count(season, word, sort = TRUE) %&amp;gt;%
        bind_tf_idf(word, season, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-12-1.png&#34; /&gt;
What we get back are the most important elements, characters, motives or places across episodes. I’m somewhat surprised that Mr. Meeseeks didn’t come in first though. I was sure as hell annoyed out of my mind after hearing it uttered for the 100th time during the episode &lt;a href=&#34;https://en.wikipedia.org/wiki/Meeseeks_and_Destroy&#34;&gt;Meeseeks and Destroy&lt;/a&gt;. But then again, Mr Meeseeks does make a cameo in two other seasons, so that kind of torpedoes his chances for the first spot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic models&lt;/h2&gt;
&lt;p&gt;Having seen the most unique words of the script by seasons, we will take our analysis one last step further and try to capture the gist of a the show using topic modeling. Broadly speaking, it’s an unsupervised classification method that tries to represent a document as a collection of topics. Here, I will take the classic &lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34;&gt;Latent Dirichlet Allocation or shortly LDA&lt;/a&gt; algorithm for a spin. The basic idea is that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“…a topic is defined as a mixture over words where each word has a probability
of belonging to a topic. And a document is a mixture over topics, meaning that a single
document can be composed of multiple topics.”&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We could for example take season two, and tell &lt;code&gt;LDA()&lt;/code&gt; that we want to compress 10 episodes into just 6 topics. To compensate for the omnipresence of the top words across episodes, I will exclude them for the purpose of clearer separation of topics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(topicmodels)
popular_words &amp;lt;- c(&amp;quot;rick&amp;quot;,&amp;quot;morty&amp;quot;, &amp;quot;yeah&amp;quot;,&amp;quot;hey&amp;quot;,
                   &amp;quot;summer&amp;quot;, &amp;quot;jerry&amp;quot;, &amp;quot;uh&amp;quot;, &amp;quot;gonna&amp;quot;)

episodes_dtm &amp;lt;- tidy_df %&amp;gt;% filter(season_num == 2 &amp;amp; !word %in% popular_words) %&amp;gt;%
        group_by(episode_num) %&amp;gt;%
        count(word, sort = TRUE) %&amp;gt;%
        cast_dtm(episode_num, word, n) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;episodes_lda &amp;lt;- LDA(episodes_dtm, k = 6, control = list(seed = 1234))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After &lt;code&gt;tidy()&lt;/code&gt;ing the results, we can plot the top 10 words that contribute (&lt;em&gt;beta&lt;/em&gt;) to most to each topic.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-14-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s definitely a few topics that contain multiple elements of a particular episode. Take for example &lt;strong&gt;topic 1&lt;/strong&gt;. It includes “&lt;em&gt;Roy&lt;/em&gt;”, the name of the videogame Morty plays in the same episode “&lt;em&gt;Fart&lt;/em&gt;” appears, a gaseous creature kept under locks by aliens. Or &lt;strong&gt;topic 5&lt;/strong&gt;, which probably relates to the episode where Rick visits his old lover “&lt;em&gt;Unity&lt;/em&gt;”. It further contains words as “&lt;em&gt;remember&lt;/em&gt;” and “&lt;em&gt;memories&lt;/em&gt;”. The episode ends with Unity repeating “I want it &lt;em&gt;real&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;Not only can we examine the &lt;strong&gt;word per topic probabilities&lt;/strong&gt;, we can also plot &lt;strong&gt;the topic per document probabilities&lt;/strong&gt;, or &lt;em&gt;gamma&lt;/em&gt; values. This lets us see what topic belongs to what episode.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(episodes_lda, matrix = &amp;quot;gamma&amp;quot;) %&amp;gt;%
        inner_join(titles) %&amp;gt;%
        ggplot(aes(factor(topic), gamma)) +
        geom_boxplot() +
        facet_wrap(~ title) +
        ggtitle(&amp;quot;Dominant Topics per Episode&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/2017-10-07-tidyrick_files/figure-html/unnamed-chunk-16-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our previous assumptions are confirmed, the first topic does belong to the episode &lt;em&gt;Mortynight Run&lt;/em&gt; as does the fifth topic to &lt;em&gt;Auto-Erotic Assimilation&lt;/em&gt;. It is important to note that the results strongly depend on the number of topics supplied to &lt;code&gt;LDA()&lt;/code&gt;, so inevitably, some experimentation is required to arrive at meaningful results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;I ran through some very interesting concepts fairly quickly in this post. I owe much of it to the tidytext package. With very little coding, we can mine a tremendous amount of insights from textual data. And I have just scrachted the surface of what’s possible. The seamless integration with the tidyverse, as with igraph and topicmodels does make a huge difference.&lt;/p&gt;
&lt;p&gt;Nonetheless, text mining is a complex topic and when arriving at more advanced material, &lt;a href=&#34;https://github.com/trinker/topicmodels_learning&#34;&gt;further reading&lt;/a&gt; on the inner workings of these algorithms might come in handy for effective use. The full &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/tree/master/data/2017-10-07-tidyrick/rick%20and%20morty&#34;&gt;data&lt;/a&gt; and &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/blog/2017/2017-10-07-tidyrick.Rmd&#34;&gt;code&lt;/a&gt; for this post is available as usual on my Github.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Self-learning Hue Lights</title>
      <link>/blog/self-learning-hue-lights/</link>
      <pubDate>Wed, 30 Aug 2017 23:15:14 -0500</pubDate>
      
      <guid>/blog/self-learning-hue-lights/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;the-rise-of-the-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The rise of the API&lt;/h2&gt;
&lt;p&gt;Rich API interfaces are one of the main ingredients of today’s smart devices. They are by definition built for interconnectivity and there is an active community of developers creating apps as microservices on top of them. Philips Hue is no exception with it’s wide variety of &lt;a href=&#34;http://www.developers.meethue.com/otherapps/otherAppsIOS.html#appsList&#34;&gt;apps&lt;/a&gt; available to users.&lt;/p&gt;
&lt;p&gt;But you don’t need to code an entire mobile application to take advantage of the low level access. Using modern tools it only takes a few lines of code to build a self-learning algorithm, running in production in your home. Not only can we access external API’s, we can just as easily expose static files, functions or models as an API of our own.&lt;/p&gt;
&lt;p&gt;My original inspiration for this post was &lt;a href=&#34;https://sc5.io/posts/autonomous-indoor-lighting-using-neural-networks/&#34;&gt;Max Pagel’s article&lt;/a&gt; on training a neural network to automatically control his Philips Hue lights. In fact, I purchased my first set of Hue bulbs because of it. In summary, this post will describe how to build and productionize a classifier in &lt;code&gt;R&lt;/code&gt; that controls the brightness of Philips Hue lights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-dinnertime&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stop, dinnertime!&lt;/h2&gt;
&lt;p&gt;Much like in my &lt;a href=&#34;http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/&#34;&gt;post on Spotify&lt;/a&gt; I have set up a cronjob to execute the Python script that pings the API and saves the lights’ state data locally, to be picked up by Luigi tasks for parsing and copying to S3 further downstream. You can find the relevant code on my &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/tasks.py&#34;&gt;Github&lt;/a&gt;. The &lt;a href=&#34;https://www.developers.meethue.com/philips-hue-api&#34;&gt;Hue API documentation&lt;/a&gt; contains information on authentication and the types of calls available.&lt;/p&gt;
&lt;p&gt;The starting point for this post will be the parsed &lt;code&gt;.json&lt;/code&gt; file containing all of the log data for my “Dinner Lamps”. They are the two main lights in my living and dining area room at the moment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aws.s3)
library(jsonlite)
# read file from amazon
aws.signature::use_credentials()
df &amp;lt;- s3read_using(object = paste0(&amp;quot;hue_full_2017-08-26.json&amp;quot;), fromJSON, bucket = &amp;quot;ams-hue-data&amp;quot;)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    30322 obs. of  15 variables:
##  $ on.1       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ on.2       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ bri.2      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ type.1     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ type.2     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ bri.1      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ modelid.2  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ modelid.1  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name.1     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; ...
##  $ reachable.1: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ reachable.2: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ name.2     : chr  &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ alert.1    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ log_time   : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:10:02&amp;quot; &amp;quot;2017-05-12 17:15:01&amp;quot; ...
##  $ alert.2    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the original &lt;code&gt;.json&lt;/code&gt; file is such that each lamp has a separate (numbered) column for every variable. The dataset is essentially a timeseries where each row represent a snapshot of the lamps’ state at &lt;code&gt;$log_time&lt;/code&gt;, or &lt;strong&gt;every 5 minutes&lt;/strong&gt;. Before moving on, let’s tidy things up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
tidy_df &amp;lt;- df %&amp;gt;% gather(key, value, -log_time) %&amp;gt;%
        separate(key, into = c(&amp;quot;variable&amp;quot;, &amp;quot;lamp&amp;quot;), sep = &amp;quot;\\.&amp;quot;) %&amp;gt;%
        spread(variable, value)
str(tidy_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    60644 obs. of  9 variables:
##  $ log_time : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; ...
##  $ lamp     : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; ...
##  $ alert    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ bri      : chr  &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; ...
##  $ modelid  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ on       : chr  &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; ...
##  $ reachable: chr  &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; ...
##  $ type     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 15 columns are now reduced to 9 because each variable appears only once thanks to adding the key column &lt;code&gt;$lamp&lt;/code&gt; to the dataset. But we are not quite done cleaning yet: I use the two lamps in sync, so we need only data from one of them. When the lamps are not &lt;code&gt;on&lt;/code&gt; nor &lt;code&gt;reachable&lt;/code&gt;, &lt;code&gt;$bri&lt;/code&gt; should be set to &lt;code&gt;0&lt;/code&gt;. Using the now correct brightness values, we create the four categories for the classifier to work with. Lastly, there were days I wasn’t home, so we can rid of of those observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
binned_df &amp;lt;- tidy_df %&amp;gt;% filter(lamp == &amp;quot;1&amp;quot;) %&amp;gt;%
        mutate(bri = as.numeric(replace(bri, on==&amp;quot;FALSE&amp;quot; | reachable==&amp;quot;FALSE&amp;quot;,0)),
               y = as.factor(ifelse(bri == 0, &amp;quot;zero&amp;quot;,
                                    ifelse(between(bri,0,80), &amp;quot;dim&amp;quot;,
                                           ifelse(between(bri,80,160),&amp;quot;mid&amp;quot;,&amp;quot;bright&amp;quot;)))))

off_days &amp;lt;- binned_df %&amp;gt;% group_by(date = as.Date(log_time,tz=&amp;quot;Europe/Amsterdam&amp;quot;)) %&amp;gt;%
                dplyr::summarise(total_bri = sum(bri)) %&amp;gt;%
                filter(total_bri == 0 ) %&amp;gt;%
                select(date)

binned_df &amp;lt;- binned_df %&amp;gt;% filter(!as.Date(log_time) %in% off_days$date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of our target variable look?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(binned_df$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## bright    dim    mid   zero 
##    598   1533   1710  23889&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Roughly 86% of the time the lamps are off, resulting in an unbalanced dataset. What about brightness values lamps were &lt;em&gt;on&lt;/em&gt;, according to the three remaining categories?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems to be close to normal with a positive skew, and a massive outlier all the way at the end of the spectrum. That’s maximum brightness, the default when I switch the lights on/off with a physical switch.&lt;/p&gt;
&lt;p&gt;To get an intuition for my usage patterns, I’ll also plot a histogram of hour of the day for all four categories.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The only times the lamps are &lt;strong&gt;not&lt;/strong&gt; structurally off, is in the evening and the early hours. Dim and mid values are the dominant category after 8PM instead. Another slight dip in zero appears around and shortly after midnight, compensated by the second largest peak in dim, and a few instances of mid and bright. Bright observations in general are sparse and will be tough to predict.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-we-learn-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can we learn this?&lt;/h2&gt;
&lt;p&gt;The only variables I will use for training, are time based: &lt;em&gt;day of the week&lt;/em&gt;; &lt;em&gt;month&lt;/em&gt;; &lt;em&gt;week number&lt;/em&gt;; &lt;em&gt;weekend or not&lt;/em&gt;; &lt;em&gt;time of the day&lt;/em&gt;; and &lt;em&gt;minutes since 12PM, 6AM, 12AM and 6PM&lt;/em&gt;. A datetime string will then suffice to generate a prediction on the fly, a boon for putting things into production later on. I packaged a chain of dplyr commands inside the function &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/functions.R&#34;&gt;add_vars()&lt;/a&gt; to add the above variables to the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_vars &amp;lt;- binned_df %&amp;gt;% add_vars(extra_var = &amp;quot;yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember we are dealing with an unbalanced dataset, that also happens to be a timeseries. To remedy the former, I will use class weights to penalize the misclassification of the minority classes. Secondly, I will look at the &lt;em&gt;Area Under the Curve&lt;/em&gt; to evaluate the model, which will be less affected than &lt;em&gt;Accuracy&lt;/em&gt; by class imbalance if I set dim as the positive class. If not for these measures, the algorithm would gladly classify 100% of instances as &lt;code&gt;&amp;quot;zero&amp;quot;&lt;/code&gt;, achieving stunning accuracy on paper and complete darkness in my living room.&lt;/p&gt;
&lt;p&gt;Now, why does it matter that we have a timeseries? In any dataset with a time component, the split between train and test sets should not be random. Otherwise, the model can and will &lt;strong&gt;learn from the future&lt;/strong&gt;, and severely overfit the data. The correct cross-validation strategy instead is to fold the data according to time. Train should always the before and test the after. For our convenience &lt;code&gt;caret&lt;/code&gt; provides the &lt;code&gt;createTimeSlices()&lt;/code&gt; function to create the indices of the CV-folds. An extra &lt;code&gt;testing&lt;/code&gt; set will be held out to validate our model on unseen data after we are done modeling.&lt;/p&gt;
&lt;p&gt;We’ll now train a &lt;a href=&#34;https://cran.r-project.org/web/packages/gbm/index.html&#34;&gt;gbm&lt;/a&gt; model, using the &lt;a href=&#34;https://topepo.github.io/caret/&#34;&gt;caret&lt;/a&gt; package, which comes with a myriad of convenience tools to make the process easier and the code a lot more concise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
# Split train and test sets
training &amp;lt;- df_vars[df_vars$date &amp;lt; &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date,-log_time)
testing &amp;lt;- df_vars[df_vars$date &amp;gt;= &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date)

# create cross validation folds
idx &amp;lt;- createTimeSlices(1:nrow(training), 
                      initialWindow = 15000, 
                      horizon = 5000, skip = 1000, fixedWindow = F)

# create model weights vector
model_weights &amp;lt;- ifelse(training$y == &amp;quot;zero&amp;quot;,0.2,
                        ifelse(training$y == &amp;quot;mid&amp;quot;,1.2,1))

# define cross validation logic
fitControl &amp;lt;- trainControl(## 10-fold CV
        index = idx[[1]],
        indexOut = idx[[2]],
        summaryFunction = multiClassSummary,
        classProbs = T)

# create tunegrid for hyperparameter search
gbmGrid &amp;lt;-  expand.grid(interaction.depth = c(1,3,5), 
                        n.trees = c(5,10,30), 
                        shrinkage = c(0.1),
                        n.minobsinnode = 5)

# train model
gbmFit &amp;lt;- train(y ~ ., data = training, 
                method = &amp;quot;gbm&amp;quot;, 
                trControl = fitControl,
                metric = &amp;quot;AUC&amp;quot;,
                weights = model_weights,
                verbose = FALSE,
                tuneGrid = gbmGrid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Printing &lt;code&gt;gbmFit&lt;/code&gt; to the console will give us the performance metrics across hyperparameters, and the ultimately selected values maximizing our metric of choice. While this is certainly useful information, I find it more intuitive to immediately look at the confusion matrix and see where our model is going off the rails:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds&amp;lt;-predict(gbmFit, testing)
table(preds, testing$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
## preds     dim  mid bright zero
##   dim     133  215     30  480
##   mid      37   74      6   35
##   bright   20   49      0    6
##   zero     30  105     29 4223&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most mistakes are made trying to classify bright and mid. The model gets 0 bright values right and only manages to do so correctly 14% of the time for mid. But when do errors happen? To dig a little deeper let’s look at the previous histogram of categories by hour again for the test set, but now with the predictions overlaid on top.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bright values were always going be hard to guess, but the model at least comes close in terms of hours, but off on the exact days. The majority of misclassification comes from overzealously predicting dim in the evening and around midnight, when it should really be either mid or zero. That looks like a workable scenario for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-ship-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Let’s ship it!&lt;/h1&gt;
&lt;p&gt;To control the lights, we can make PUT requests to the Hue bridge. To set &lt;em&gt;bri&lt;/em&gt;, we need actual brightness values. An intuitive option is to pick the median values per category per hour:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_values &amp;lt;- binned_df %&amp;gt;% filter(bri &amp;gt; 0) %&amp;gt;% 
                mutate(hour = lubridate::hour(as.POSIXct(log_time, tz = &amp;quot;Europe/Amsterdam&amp;quot;))) %&amp;gt;%
                select(hour,bri, y) %&amp;gt;% 
                group_by(y, hour) %&amp;gt;%
                dplyr::summarise(med = median(bri)) %&amp;gt;%
                ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we only used date and time based features for modeling, all we need for a prediction is a timestamp string. Using &lt;code&gt;for_sample&lt;/code&gt; and &lt;code&gt;def_vars()&lt;/code&gt;, we define a custom function &lt;code&gt;predict_hue()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_hue &amp;lt;- function(timestamp){
        
        df &amp;lt;- data.frame(log_time =as.POSIXct(timestamp)) %&amp;gt;% 
                add_vars(extra_var = &amp;quot;no&amp;quot;)
        
        pred &amp;lt;- predict(gbmFit, newdata = df)
        
        if (pred==&amp;quot;zero&amp;quot;) {
                x &amp;lt;- 0
        } else {
                x &amp;lt;- median_values %&amp;gt;% filter(y == pred &amp;amp; hour == lubridate::hour(timestamp)) %&amp;gt;%
                select(med) %&amp;gt;% unlist()
        }
        
        return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to expose the above function as an API, we literally need three lines of code with &lt;a href=&#34;https://cran.r-project.org/web/packages/jug/vignettes/jug.html&#34;&gt;jug&lt;/a&gt;. Ever since I saw the package &lt;a href=&#34;https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/jug-Building-Web-APIs-for-R&#34;&gt;presented at useR2017&lt;/a&gt;, I have been looking for a use case to play with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jug)
jug() %&amp;gt;% post(&amp;quot;/predict-hue&amp;quot;, decorate(predict_hue)) %&amp;gt;%
        simple_error_handler_json() %&amp;gt;%
        serve_it()
#Serving the jug at http://127.0.0.1:8080&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great. We can now make calls to this API and get a prediction supplying the current time. The nifty thing is that because API’s are language agnostic, we can access it from the programming paradigm of our choosing. I currently have a basic Python function that communicates with both API’s, transferring a prediction to the Hue Bridge every 5 minutes. But we could just as well build a whole interface on top, or create a chatbot for improved user experience. Perhaps I’ll do a follow-up post on this topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;There is something very satisfying about building your own data products and seeing them at work. Even something as trivial as controlling the light switch for you. I only have data since this year May, so there’s a good chance our model will get smarter as days go by. We can easily schedule to retrain the model every week or even day, without having to lift a finger. Most of the code in this post is packaged up as a handful of &lt;code&gt;R&lt;/code&gt; functions deployed on my Raspberry Pi. Now, when I &lt;em&gt;choose&lt;/em&gt; to pass out on my couch next time, at least lights won’t stay on for too long.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Creating a Spotify Playlist using Luigi</title>
      <link>/blog/creating-a-spotify-playlist-using-luigi/</link>
      <pubDate>Sat, 22 Jul 2017 21:13:14 -0500</pubDate>
      
      <guid>/blog/creating-a-spotify-playlist-using-luigi/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In the &lt;a href=&#34;http://tamaszilagyi.com/blog/analyzing-my-spotify-listening-history/&#34;&gt;previous post&lt;/a&gt;, I shared an analysis of my Spotify listening history using R. In this post, I will discuss what came before having the data: collecting, cleaning and saving it. As the title suggest, we will even go a step further and automate the creation of a weekly top 10 playlist in Spotify using the very same dataset.&lt;/p&gt;
&lt;p&gt;The main ingredient will be Luigi, a Python framework for workflow management Spotify open-sourced a couple of years ago. According to &lt;a href=&#34;http://luigi.readthedocs.io/en/stable/index.html&#34;&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures will happen. These tasks can be anything, but are typically long running things like Hadoop jobs, dumping data to/from databases, running machine learning algorithms, or anything else.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Designed for massive jobs, implementing Luigi on top of tiny little &lt;code&gt;.json&lt;/code&gt; files might seem like a huge overkill, but the logic we will define won’t considerably differ from larger scale applications.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pipeline&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The pipeline&lt;/h1&gt;
&lt;p&gt;We can break down the pipeline into four tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tamaszilagyi.com/img/luigi2.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;cronjob-to-ping-the-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Cronjob to ping the API&lt;/h2&gt;
&lt;p&gt;The initial step is to ping the API, and store the raw response as a &lt;code&gt;.json&lt;/code&gt; file locally. We need to have the &lt;code&gt;client_id&lt;/code&gt;, &lt;code&gt;client_secret&lt;/code&gt; and a &lt;code&gt;refresh_token&lt;/code&gt; to generate a temporary access token. Follow the &lt;a href=&#34;https://developer.spotify.com/web-api/tutorial/&#34;&gt;Web API tutorial by Spotify&lt;/a&gt; to attain them. In turn, the access token is required to make calls to the API.&lt;/p&gt;
&lt;p&gt;We start with two functions: One to generate the &lt;code&gt;access_token&lt;/code&gt; using our credentials (I have them inside &lt;code&gt;spotify_creds&lt;/code&gt;), and a second one to download our listening history, dumping the data in a new &lt;code&gt;.json&lt;/code&gt; file every day. To make sure that the access token doesn’t expire, we’ll generate a new one with every call to the API.&lt;/p&gt;
&lt;p&gt;I will store functions inside &lt;code&gt;functions.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import requests
import json
import datetime
from spotify_creds import *
# Get access token
def access_token():
    
    body_params = {&amp;#39;grant_type&amp;#39; : &amp;#39;refresh_token&amp;#39;,
                &amp;#39;refresh_token&amp;#39; : refresh_token}
    url = &amp;#39;https://accounts.spotify.com/api/token&amp;#39;
    response = requests.post(url, 
                             data = body_params, 
                             auth = (client_id, client_secret))
    
    response_dict = json.loads(response.content)
    accessToken = response_dict.get(&amp;#39;access_token&amp;#39;)
    return accessToken
    
# Get most recent songs and append the response
# to a new json file every day
def download_data():
    current_time = datetime.datetime.now().strftime(&amp;#39;%Y-%m-%d&amp;#39;)
    filename = &amp;#39;/spotify/json/spotify_tracks_%s.json&amp;#39; % current_time
    
    accesToken = access_token()
    headers = {&amp;#39;Authorization&amp;#39;: &amp;#39;Bearer &amp;#39; + accesToken }
    payload = {&amp;#39;limit&amp;#39;: 50}
    url = &amp;#39;https://api.spotify.com/v1/me/player/recently-played&amp;#39;
    response = requests.get(url, headers = headers,
                            params = payload)
    data = response.json()
    with open(filename, &amp;#39;a&amp;#39;) as f:
        json.dump(data[&amp;#39;items&amp;#39;], f)
        f.write(&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to make sure that I don’t miss any songs I have listened to, I will set up a cronjob to execute &lt;code&gt;download_data()&lt;/code&gt; (that’s what &lt;code&gt;logger.py&lt;/code&gt; contains) every three hours. We first make this file executable&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;chmod +x /spotify/logger.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;open crontab,&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;crontab -e&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and add the following line to our list of cronjobs:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;0 */3 * * * /usr/bin/python /spotify/logger.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part with the numbers and &lt;code&gt;*&lt;/code&gt;’s gives the scheduling logic. The second bit is the Python environment from which to call the script. If you prefer self-contained environments instead, then this will look something like &lt;code&gt;/home/pi/miniconda/envs/name_of_env/bin/python&lt;/code&gt; on a Raspberry Pi using &lt;a href=&#34;https://conda.io/miniconda.html&#34;&gt;miniconda&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deduplicate-and-save-to-s3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Deduplicate and save to S3&lt;/h2&gt;
&lt;p&gt;With raw data coming in, the next step is to store the result somewhere more robust than the SD card inside my Pi. Because we are pinging the API every three hours, we have files that contain 8 dictionaries of the last 50 tracks. Unless I listen to Spotify non-stop all day every day, there is going to be lots of redundancy because of duplicate records.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;deduplicate()&lt;/code&gt; takes &lt;code&gt;.json&lt;/code&gt; file we created above, and returns the deduplicated list of dictionaries containing only unique items according to the key &lt;code&gt;played_at&lt;/code&gt;, which is the timestamp of each song played.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Cleaner function to get rid of redundancy
def deduplicate(file):
    result =[]
    
    for line in file:
        data = json.loads(line)
        result.extend(data)
    
    result = {i[&amp;#39;played_at&amp;#39;]:i for i in result}.values()
    return result&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this point onwards, we are going to switch to using &lt;strong&gt;Luigi&lt;/strong&gt;. The main building block is a Task, which &lt;em&gt;usually&lt;/em&gt; consists of three methods:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;requires()&lt;/code&gt;: What other task the current one depends on.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run()&lt;/code&gt;: What is our tasks going to do, usually some function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output()&lt;/code&gt;: Where will the result be stored.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In turn, &lt;code&gt;output()&lt;/code&gt; will end up in the &lt;code&gt;require()&lt;/code&gt; method of a consecutive task. This builds a dependency graph between tasks. Let’s jump right in, and look at how we apply this logic:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import luigi
from datetime import date, timedelta
from functions import *
# External task at the bottom of our dependancy graph,
# only looks to see if output of cronjob exists,
# by default from yesterday.
class local_raw_json(luigi.ExternalTask):
    date = luigi.DateParameter(default = date.today()-timedelta(1)) 
    def output(self):
        return luigi.LocalTarget(&amp;#39;spotify/json/spotify_tracks_%s.json&amp;#39; % 
                                 self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;))
        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first task &lt;code&gt;local_raw_json&lt;/code&gt; is an &lt;strong&gt;External Task&lt;/strong&gt; with only an &lt;code&gt;output()&lt;/code&gt; method. This task does not run anything and does not depend on anything. It simply confirms the existence of a file, namely the output from our cronjob. Luigi allows for parameterization of tasks, so we define a &lt;code&gt;date&lt;/code&gt; parameter with the default value yesterday. We pass this to the &lt;code&gt;output()&lt;/code&gt; method to look for the file with the correct date.&lt;/p&gt;
&lt;p&gt;External tasks with no dependencies are common first steps, especially if we are relying on an external datadump somewhere else.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import json
from luigi.s3 import S3Target, S3Client
# Task that runs our deduplicate() on local file 
# and writes the output to S3 bucket.
class spotify_clean_aws(luigi.Task):
    date = luigi.DateParameter(default = date.today()-timedelta(1)) 
    
    def requires(self):
        return self.clone(local_raw_json)
        
    def run(self):   
        with self.input().open(&amp;#39;r&amp;#39;) as in_file:
            data = deduplicate(in_file)
            
        with self.output().open(&amp;#39;w&amp;#39;) as out_file:
            json.dump(data, out_file)
    def output(self):
        client = S3Client(host = &amp;#39;s3.us-east-2.amazonaws.com&amp;#39;)
        return S3Target(&amp;#39;s3://myspotifydata/spotify_tracks_%s.json&amp;#39; % 
                        self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;), 
                        client=client)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second tasks is &lt;code&gt;spotify_clean_aws&lt;/code&gt;. This is where we run the &lt;code&gt;deduplicate()&lt;/code&gt; function defined earlier and write the output to an &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;AWS S3&lt;/a&gt; bucket. In contrary to the first task, all three methods are present:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Require&lt;/strong&gt; that the raw json file exists, and also &lt;code&gt;clone()&lt;/code&gt; the parameters from the first task. This way the same date parameter will be passed to both tasks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Run&lt;/strong&gt; the function &lt;code&gt;deduplicate()&lt;/code&gt; on the input file and save the result as a .json.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt; the result of the task to S3. Luigi has built-in support for AWS S3 that uses &lt;a href=&#34;https://boto3.readthedocs.io/en/latest/&#34;&gt;boto3&lt;/a&gt; under the hood. To connect, we need to have AWS credentials. They usually reside under &lt;code&gt;~/.aws/credentials&lt;/code&gt;, if you have run &lt;code&gt;aws configure&lt;/code&gt; in the Terminal before:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;[default]
aws_access_key_id = YOUR_ACCESS_KEY
aws_secret_access_key = YOUR_SECRET_KEY &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to pass them explicitly to &lt;code&gt;S3Client()&lt;/code&gt; however.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-relevant-fields-and-create-weekly-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Get relevant fields and create weekly dataset&lt;/h2&gt;
&lt;p&gt;With the data deduplicated and safely stored in the cloud, we can now parse the files, selecting a handful of fields from the &lt;a href=&#34;https://developer.spotify.com/web-api/web-api-personalization-endpoints/get-recently-played/&#34;&gt;response&lt;/a&gt;. Because nobody ever gets excited about ETL code, I will omit the contents of &lt;code&gt;parse_json()&lt;/code&gt; here. It is suffice to say that we get a more compact result than what I used in the previous post. An example record from the resulting dictionary will look like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;{&amp;quot;played_at&amp;quot;: &amp;quot;2017-04-22T18:49:54.108Z&amp;quot;, 
 &amp;quot;track_name&amp;quot;: &amp;quot;Symphony No. 5 In C Minor Part 1&amp;quot;, 
 &amp;quot;duration_ms&amp;quot;: 485293, 
 &amp;quot;type&amp;quot;: &amp;quot;track&amp;quot;, 
 &amp;quot;artist_id&amp;quot;: [&amp;quot;2wOqMjp9TyABvtHdOSOTUS&amp;quot;], 
 &amp;quot;explicit&amp;quot;: false, 
 &amp;quot;uri&amp;quot;: &amp;quot;spotify:track:0ZN01wuIdn4iT8VBggkOMm&amp;quot;, 
 &amp;quot;artist_name&amp;quot;: [&amp;quot;Ludwig van Beethoven&amp;quot;], 
 &amp;quot;track_id&amp;quot;: &amp;quot;0ZN01wuIdn4iT8VBggkOMm&amp;quot;}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can still find all the code for &lt;code&gt;parse_json()&lt;/code&gt; function (and all the others) on my &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/tree/master/data/2017-07-22-spotifyLuigi&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Secondly, we’ll merge a week worth of data and store the intermediate result on S3. With these ingredients, we define our third Luigi Task: &lt;code&gt;spotify_merge_weekly_aws&lt;/code&gt; :&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Task that merges the 7 daily datasets, 
# parses relevant fields, deduplicates records
# and stores the result in S3.
class spotify_merge_weekly_aws(luigi.Task):
    date = luigi.DateParameter(default = (date.today()-timedelta(8)))
    daterange = luigi.IntParameter(7)
    def requires(self):
        # take data from the 7 days following date param (8 days prior to current date by default)
        return [spotify_clean_aws(i) for i in [self.date + timedelta(x) for x in range(self.daterange)]]
     
    def run(self):
        results = []
        for file in self.input():
            
            with file.open(&amp;#39;r&amp;#39;) as in_file:
                data = json.load(in_file)
                parsed = parse_json(data)
                
            results.extend(parsed)
        # merging of daily data creates dupe records still
        result = {v[&amp;#39;played_at&amp;#39;]:v for v in results}.values()
        
        with self.output().open(&amp;#39;w&amp;#39;) as out_file:
            json.dump(result, out_file)
            
    def output(self):
        client = S3Client(host = &amp;#39;s3.us-east-2.amazonaws.com&amp;#39;)
        return S3Target(&amp;#39;s3://myspotifydata/spotify_week_%s.json&amp;#39; % 
                        (self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;) + &amp;#39;_&amp;#39; + str(self.daterange)), 
                         client=client)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-playlist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Create playlist&lt;/h2&gt;
&lt;p&gt;As a last step, we aggregate the weekly data and fill up our playlist on Spotify. These are the last two functions we need to define. Not to complicate things too much, I am simply going to create a top 10 of &lt;em&gt;my most listened to tracks between 7am and 12pm&lt;/em&gt;. Sort of a morning playlist.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# This function takes a list of track uri&amp;#39;s 
# to replace songs in my morning playlist
# and returns the status code of the put request.
def replace_tracks(tracks):
    
    url = &amp;#39;https://api.spotify.com/v1/users/1170891844/playlists/6a2QBfOgCqFQLN08FUxpj3/tracks&amp;#39;
    accesToken = access_token()
    headers = {&amp;#39;Authorization&amp;#39;: &amp;#39;Bearer &amp;#39; + accesToken,
               &amp;#39;Content-Type&amp;#39;:&amp;#39;application/json&amp;#39;}
    data = {&amp;quot;uris&amp;quot;: &amp;#39;,&amp;#39;.join(tracks)}
    response = requests.put(url, headers = headers,
                            params = data)
                            
    return response.status_code
                            
# This function reads in the weekly dataset 
# as a pandas dataframe, outputs the list of 
# top ten tracks and feeds them to replace_tracks()
def create_playlist(dataset, date):
    
    data = pd.read_json(dataset)          
    data[&amp;#39;played_at&amp;#39;] = pd.to_datetime(data[&amp;#39;played_at&amp;#39;])
    
    data = data.set_index(&amp;#39;played_at&amp;#39;) \
               .between_time(&amp;#39;7:00&amp;#39;,&amp;#39;12:00&amp;#39;)
        
    data = data[data.index &amp;gt; str(date)]
    # aggregate data
    songs = data[&amp;#39;uri&amp;#39;].value_counts()\
                       .nlargest(10) \
                       .index \
                       .get_values() \
                       .tolist()
    # make api call
    res_code = replace_tracks(songs)
    
    return res_code&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we wrap the above inside our last Luigi Task, &lt;code&gt;spotify_morning_playlist&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Task to aggregate weekly data and create playlist
class spotify_morning_playlist(luigi.Task):
    date = luigi.DateParameter(default = (date.today()-timedelta(8)))
    daterange = luigi.IntParameter(7)
    def requires(self):
        return self.clone(spotify_merge_weekly_aws)
    
    def run(self):
        
        with self.input().open(&amp;#39;r&amp;#39;) as in_file:
            res_code = create_playlist(in_file, self.date)      
        # write to file if succesful
        if (res_code == 201):
            with self.output().open(&amp;#39;w&amp;#39;) as out_file:
                json.dump(res_code, out_file)
    
    def output(self):
        client = S3Client(host = &amp;#39;s3.us-east-2.amazonaws.com&amp;#39;)
        return S3Target(&amp;#39;s3://myspotifydata/spotify_top10_%s.json&amp;#39; % 
                        (self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;) + &amp;#39;_&amp;#39; + str(self.daterange)), 
                        client=client)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have put all of the tasks in a file named &lt;code&gt;tasks.py&lt;/code&gt;. Luigi does not provide a scheduling mechanism out of the box, so we’ll trigger the tasks from crontab instead. For example every Monday at 7AM:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;0 7 * * 1 /usr/bin/python /spotify/tasks.py spotify_morning_playlist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we should have the &lt;a href=&#34;http://luigi.readthedocs.io/en/stable/central_scheduler.html&#34;&gt;Central Scheduler&lt;/a&gt; running in the background for the above to execute. The neat thing is that we only need to trigger the last task, and then Luigi considers all the dependencies and runs them if needed (ie. if the target file does not exists). Additionally, Luigi has a real nice GUI running on &lt;code&gt;localhost:8082&lt;/code&gt;, where we can visualise the complete dependency graph and monitor the progress of our tasks:
&lt;img src=&#34;http://tamaszilagyi.com/img/dag.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If nothing fails, the tracks in the below playlist get updated every Monday morning:&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/user/1170891844/playlist/6a2QBfOgCqFQLN08FUxpj3&#34; width=&#34;300&#34; height=&#34;380&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;I have tried to give a simple, yet fully reproducible example of how to set up a workflow using Luigi. It is important to note that building data pipelines for production systems does require a little more effort. To name a few shortcomings of the above: We haven’t defined logging, we didn’t clean up our original files containing the raw response data, and it is very likely that the same tracks will end up in this playlist on consecutive weeks. Not something you would want to happen to your Discover Weekly for example.&lt;/p&gt;
&lt;p&gt;If you want to learn more about Luigi, I encourage you to read the &lt;a href=&#34;http://luigi.readthedocs.io/en/stable/index.html&#34;&gt;documentation&lt;/a&gt; and most of all start experimenting on personal projects. I find that is always the best way to learn new skills.&lt;/p&gt;
&lt;p&gt;On the other hand, we could also create playlists that are more useful to us than a simple top 10 playlist. What if we took artists we listen to the most, and automatically put their songs not in our listening history yet in a new playlist. It is perfectly possible, and probably more valuable to us as users. We just need to write a couple new functions, plug them into a similar Luigi pipeline as above and let it do the work for us.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing My Spotify Listening History</title>
      <link>/blog/analyzing-my-spotify-listening-history/</link>
      <pubDate>Sun, 02 Jul 2017 21:13:14 -0500</pubDate>
      
      <guid>/blog/analyzing-my-spotify-listening-history/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;a-new-endpoint&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A new endpoint&lt;/h1&gt;
&lt;p&gt;Following an &lt;a href=&#34;https://github.com/spotify/web-api/issues/20&#34;&gt;avalanche of &lt;em&gt;+1&lt;/em&gt; comments&lt;/a&gt; on the GitHub issue requesting access to a user’s play history, on March 1st Spotify released &lt;a href=&#34;https://developer.spotify.com/web-api/web-api-personalization-endpoints/get-recently-played/&#34;&gt;a new endpoint&lt;/a&gt; to their Web API that allows anyone with a Spotify account to pull data on his or her most recently played tracks. To access it, you need go through the &lt;a href=&#34;https://developer.spotify.com/web-api/authorization-guide/#authorization_code_flow&#34;&gt;Authorization Code Flow&lt;/a&gt;, where you get keys and tokens needed for making calls to the API. The return object contains your 50 most recently played songs enriched by some contextual data.&lt;/p&gt;
&lt;p&gt;Being an avid Spotify user, I figured I could use my recently purchased &lt;a href=&#34;https://www.raspberrypi.org/&#34;&gt;Raspberry Pi&lt;/a&gt; to ping the API every 3 hours, and start collecting my Spotify data. I started begin April, so now I have almost three months worth of listening history.&lt;/p&gt;
&lt;p&gt;How I set up a data pipeline that pings the API, parses the response and stores it as .json file, will be the subject of a follow-up post. Here, I will instead focus on exploring certain aspects of the data I thus far collected, using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-do-we-have-here&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What do we have here&lt;/h1&gt;
&lt;p&gt;Besides my play history, I also store additional variables for every artist, album and playlist that I have listened to as separate json files. For the purpose of this post however, I’ll only focus on my listening history and additional data on artists. You can find both files on my &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/tree/master/data/2017-06-02-spotifyR&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s read the data into &lt;code&gt;R&lt;/code&gt;, using the &lt;code&gt;fromJSON()&lt;/code&gt; function from the &lt;code&gt;jsonlite&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jsonlite)

df_arts &amp;lt;- fromJSON(&amp;quot;/data/spotify_artist_2017-06-30.json&amp;quot;)
df_tracks &amp;lt;- fromJSON(&amp;quot;/data/spotify_tracks_2017-06-30.json&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most important file is &lt;strong&gt;df_tracks&lt;/strong&gt;; this is the parsed response from the &lt;strong&gt;Recently Played Tracks&lt;/strong&gt; endpoint. Let’s take a look.&lt;/p&gt;
&lt;div id=&#34;df_tracks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;df_tracks&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    3274 obs. of  8 variables:
##  $ played_at  : chr  &amp;quot;2017-06-24T18:57:25.899Z&amp;quot; ...
##  $ artist_name:List of 3274
##  $ artist_id  :List of 3274
##  $ track_name : chr  &amp;quot;People In Tha Middle&amp;quot; ...
##  $ explicit   : logi  FALSE ...
##  $ uri        : chr  &amp;quot;spotify:user:1170891844:playlist:29XAftFCmwVBJ64ROX8gzA&amp;quot; ...
##  $ duration_ms: int  302138 226426 ...
##  $ type       : chr  &amp;quot;playlist&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a data.frame of &lt;strong&gt;3274 observations&lt;/strong&gt; and &lt;strong&gt;8 variables&lt;/strong&gt;. The number of rows is equal to the number of songs I have listened to, as the variable &lt;code&gt;played_at&lt;/code&gt; is unique in the dataset. Here’s a short description of the the variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;played_at&lt;/code&gt;: The timestamp when the track started playing.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_name&lt;/code&gt; &amp;amp; &lt;code&gt;artist_id&lt;/code&gt; : List of names and id’s of the artists of the song.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;track_name&lt;/code&gt;: Name of the track.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;explicit&lt;/code&gt;: Do the lyrics contain bad words?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uri&lt;/code&gt;: Unique identifier of the context, either a &lt;em&gt;playlist&lt;/em&gt; or an &lt;em&gt;album&lt;/em&gt; (or empty).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;duration_ms&lt;/code&gt;: Number of miliseconds the song lasts.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;type&lt;/code&gt; : Type of the context in which the track was played.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can see two issues at first glance. For starters, the variable &lt;code&gt;played_at&lt;/code&gt; is of class &lt;code&gt;character&lt;/code&gt; while it should really be a timestamp. Secondly, both &lt;code&gt;artist_...&lt;/code&gt; columns are of class &lt;code&gt;list&lt;/code&gt; because one track can have several artists. This will become inconvenient when we want to use the variable &lt;code&gt;artist_id&lt;/code&gt; to merge the two datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;df_arts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;df_arts&lt;/h2&gt;
&lt;p&gt;The second &lt;code&gt;data.frame&lt;/code&gt; consists of a couple of additional variables concerning the artists:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1810 obs. of  4 variables:
##  $ artist_followers : int  256962 30345 ...
##  $ artist_genres    :List of 1810
##  $ artist_id        : chr  &amp;quot;32ogthv0BdaSMPml02X9YB&amp;quot; ...
##  $ artist_popularity: int  64 57 ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;artist_followers&lt;/code&gt;: The number of Spotify users following the artist.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_genres&lt;/code&gt; : List of genres the artist is associated with.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_id&lt;/code&gt;: Unique identifier of the artist.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_popularity&lt;/code&gt;: Score from 1 to 100 regarding the artist’s popularity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By joining the two dataframes we are mostly looking to enrich the original data with &lt;code&gt;artist_genre&lt;/code&gt;, a variable we’ll use for plotting later on. Similarly to artists, albums and tracks also have &lt;a href=&#34;https://developer.spotify.com/web-api/endpoint-reference/&#34;&gt;API endpoints&lt;/a&gt; containing a genre field. However, the more granular you get, the higher the prevalence of no associated genres. Nevertheless, there is still quite some artists where genres is left blank.&lt;/p&gt;
&lt;p&gt;So, let’s unnest the list columns, convert &lt;code&gt;played_at&lt;/code&gt; to timestamp and merge the the dataset with &lt;strong&gt;df_arts&lt;/strong&gt;, using the key &lt;code&gt;&amp;quot;artist_id&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)

merged &amp;lt;- df_tracks %&amp;gt;% 
        unnest(artist_name, artist_id) %&amp;gt;% 
        mutate(played_at = as.POSIXct(played_at, 
                                      tz = &amp;quot;CET&amp;quot;, 
                                      format = &amp;quot;%Y-%m-%dT%H:%M:%S&amp;quot;)) %&amp;gt;%
        left_join(df_arts, by=&amp;quot;artist_id&amp;quot;) %&amp;gt;% 
        select(-artist_id)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-top-10&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;My Top 10&lt;/h1&gt;
&lt;p&gt;First things first, what was my three month top 10 most often played songs?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top10 &amp;lt;- merged %&amp;gt;% 
        group_by(track_name) %&amp;gt;%
        summarise(artist_name = head(artist_name,1),
                  # cuz a song can have multiple artist
                  plays = n_distinct(played_at)) %&amp;gt;%
        arrange(-plays) %&amp;gt;%
        head(10)
top10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    track_name                      artist_name      plays
##    &amp;lt;chr&amp;gt;                           &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;
##  1 Habiba                          Boef                14
##  2 Too young                       Phoenix             13
##  3 Give Me Water                   John Forte          11
##  4 Gentle Persuasion               Doug Hream Blunt    10
##  5 Dia Ja Manche                   Dionisio Maio        9
##  6 Run, Run, Run                   Ann Peebles          9
##  7 Heygana                         Ali Farka Touré      8
##  8 It Ain&amp;#39;t Me (with Selena Gomez) Kygo                 8
##  9 Perfect World                   Broken Bells         8
## 10 Bencalado                       Zen Baboon           7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How did these songs reach the top? Is there a relationship between the first time I played the song in the past three months, the number of total plays, and the period I played each the song the most? One way to explore these questions is by plotting a cumulative histogram depicting the number of plays over time for each track.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using ggplot2
library(ggplot2)
library(zoo)

plot &amp;lt;- merged %&amp;gt;% 
        filter(track_name %in% top10$track_name) %&amp;gt;%
        mutate(doy = as.Date(played_at, 
                             format = &amp;quot;%Y-%m-%d&amp;quot;),
               track_name = factor(track_name, 
                                   levels = top10$track_name)) %&amp;gt;%
        complete(track_name, doy = full_seq(doy, period = 1)) %&amp;gt;%
        group_by(track_name) %&amp;gt;%
        filter(doy &amp;gt;= doy[min(which(!is.na(played_at)))]) %&amp;gt;% 
        distinct(played_at, doy) %&amp;gt;%
        mutate(cumulative_plays = cumsum(na.locf(!is.na(played_at)))) %&amp;gt;%
        ggplot(aes(doy, cumulative_plays,fill = track_name)) + 
        geom_area(position = &amp;quot;identity&amp;quot;) + 
        facet_wrap(~track_name, nrow  = 2) +
        ggtitle(&amp;quot;Cumulative Histogram of Plays&amp;quot;) +
        xlab(&amp;quot;Date&amp;quot;) +
        ylab(&amp;quot;Cumulative Frequency&amp;quot;) +
        guides(fill = FALSE) +
        theme(axis.text.x = element_text(angle = 90, hjust = 1))
plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-06-02-spotifyR_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the songs in my top 10 have a similar pattern: The first few days after discovering them, there is a sharp increase in the number of plays. Sometimes it takes a couple of listens for me to get into a track, but usually I start obsessing over it immediately. One obvious exception is the song &lt;em&gt;Habiba&lt;/em&gt;, the song I listened to the most. The first time I heard the song, it must have gone unnoticed. Two months later, I started playing it virtually on repeat.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;listening-times&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Listening times&lt;/h1&gt;
&lt;p&gt;Moving on, let’s look at what time of the day I listen to Spotify the most. I expect weekdays to exhibit a somewhat different pattern than weekends. We can plot separate timelines of the total number of listens per hour of the day for both weekdays and weekends. Unfortunately, there are more weekdays than weekends, so we need to normalize their respective counts to arrive at a meaningful comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)

merged %&amp;gt;% group_by(time_of_day = hour(played_at),
                    weekend = ifelse(wday(played_at) %in% c(6:7),
                                   &amp;quot;weekend&amp;quot;, &amp;quot;weekday&amp;quot;)) %&amp;gt;%
        summarise(plays = n_distinct(played_at)) %&amp;gt;%
        mutate(plays = ifelse(weekend == &amp;quot;weekend&amp;quot;, plays/2, plays/5)) %&amp;gt;%
        ggplot(aes(time_of_day, plays, colour = weekend)) +
        geom_line() +
        ggtitle(&amp;quot;Number of Listens per hour of the day&amp;quot;) +
        xlab(&amp;quot;Hour&amp;quot;) +
        ylab(&amp;quot;Plays&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-06-02-spotifyR_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, there’s a few interesting things here. On &lt;strong&gt;weekdays&lt;/strong&gt; I listen to slightly more music than on weekends, mostly due to regular listening habits early on and during the day. The peak in the morning corresponds to me biking to work, followed by dip around 10 (daily stand-ups anyone?). Then, I put my headphones back on until about 14:00, to finish my Spotify activities in the evening when I get home.&lt;/p&gt;
&lt;p&gt;On the other hand, I listen to slightly more music in the afternoon and evening when it’s &lt;strong&gt;weekend&lt;/strong&gt;. Additionally, all early hours listening happens solely on weekends.&lt;/p&gt;
&lt;p&gt;I am also interested whether there is such a thing as &lt;em&gt;morning artists vs. afternoon/evening artists&lt;/em&gt;. In other words, which artists do I listen to more often in the morning than &lt;em&gt;after noon&lt;/em&gt;, or the other way around. The approach I took is to count the number plays by artists, and calculate a ratio of morning / evening for each one. The result I plotted with what is apparently called a &lt;a href=&#34;http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#Diverging%20Lollipop%20Chart&#34;&gt;diverging lollipop chart&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code snippet to produce this plot is tad bit too long to include here, but you can find all the code in the original RMarkdown file on &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/blog/2017/2017-06-02-spotifyR.Rmd&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../blog/2017/2017-06-02-spotifyR_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the y-axis we have the artists. The x-axis depicts the aforementioned ratio, and the size of the &lt;em&gt;lollipop&lt;/em&gt; stands for the number of plays in the given direction, also displayed by the label.&lt;/p&gt;
&lt;p&gt;The artists with the biggest divergences are &lt;a href=&#34;https://open.spotify.com/artist/0HlOk15cW7PeziVcItQLco&#34;&gt;Zen Mechanics&lt;/a&gt; and &lt;a href=&#34;https://open.spotify.com/artist/1k8VBufn1nBs8LN9n4snc8&#34;&gt;Stereo MC’s&lt;/a&gt;. For both artists, the number of plays is almost equal to the difference ratio. That means I played songs in the opposite timeframe &lt;strong&gt;only once&lt;/strong&gt;. As a matter of fact, there are artists such as &lt;a href=&#34;https://open.spotify.com/artist/03HEHGJoLPdARs4nrtUidr&#34;&gt;Junior Kimbrough&lt;/a&gt; or &lt;a href=&#34;https://open.spotify.com/artist/3mNygoyrEKLgo6sx0MzwOL&#34;&gt;Ali Farka Touré&lt;/a&gt; whom I played more often in each direction, but because the plays are distributed more evenly, the ratio is not as extreme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;artist-genres&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Artist Genres&lt;/h1&gt;
&lt;p&gt;Lastly, let’s look at genres. Just as a track can have more than one artist to it, so can an artist have multiple associated genres, or no genre at all. To make our job less cumbersome, we first reduce our data to one genre per artist. We calculate the count of each genre in the whole dataset, and consequently select only one per artist; the one with the highest frequency. What we lose in detail, we gain in comparability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
# unnest genres
unnested &amp;lt;- merged %&amp;gt;% 
        mutate(artist_genres = replace(artist_genres,
                                       map(artist_genres,length) == 0, 
                                       list(&amp;quot;none&amp;quot;))) %&amp;gt;%
        unnest(artist_genres)
# calculate count and push &amp;quot;none&amp;quot; to the bottom 
# so it is not included in the top genres.
gens &amp;lt;- unnested %&amp;gt;% 
        group_by(artist_genres) %&amp;gt;% 
        summarise(genre_count = n()) %&amp;gt;%
        mutate(genre_count = replace(genre_count, 
                                     artist_genres == &amp;quot;none&amp;quot;,
                                     0))
# get one genre per artist
one_gen_per_a &amp;lt;- unnested %&amp;gt;% 
        left_join(gens, by = &amp;quot;artist_genres&amp;quot;) %&amp;gt;%
        group_by(artist_name) %&amp;gt;%  
        filter(genre_count == max(genre_count)) %&amp;gt;%
        mutate(first_genre = head(artist_genres, 1)) %&amp;gt;%
        filter(artist_genres == first_genre)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the genre column is dealt with, we can proceed to look at my favourite genres.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    artist_genres      plays
##    &amp;lt;chr&amp;gt;              &amp;lt;int&amp;gt;
##  1 jazz blues           401
##  2 hip hop              351
##  3 psychedelic trance   302
##  4 funk                 251
##  5 electronic           210
##  6 pop                   79
##  7 psychill              70
##  8 afrobeat              64
##  9 classic rock          63
## 10 chillstep             62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, I am interested in whether there is a pattern in the genres I listen to. More specifically, it would be cool to see how my preferences evolve over time, if at all. The axes I want to plot my data along are the cumulative frequency and recency of songs played of a given genre.&lt;/p&gt;
&lt;p&gt;This is exactly what &lt;strong&gt;lifecycle grids&lt;/strong&gt; are made of, albeit usually used for customer segmentation. In a classical example, the more often you purchased a product, and the more recent your last purchase was, the more valuable you are as customer. I first read about these charts on &lt;a href=&#34;http://analyzecore.com/2015/02/16/customer-segmentation-lifecycle-grids-with-r/&#34;&gt;the analyzecore blog&lt;/a&gt;, which discusses these plots in more detail, including full code examples in &lt;code&gt;ggplot2&lt;/code&gt;. I highly recommend reading it if you’re interested.&lt;/p&gt;
&lt;p&gt;Clearly, we are not concerned with customer segmentation here, but what if we substituted customers with artist genres, and purchases with listens. These charts are like snapshots: how the grid is filled depends on the moment in time it was plotted. So to add an extra layer of intuition, I used the &lt;a href=&#34;https://github.com/dgrtwo/gganimate&#34;&gt;gganimate package&lt;/a&gt; to create an animated plot that follows my preferences as days go by.&lt;/p&gt;
&lt;p&gt;To be able to generate such a plot, we need to expand our dataset to include all possible combinations of dates and genres and deal with resulting missing values appropriately:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genres_by_day &amp;lt;- one_gen_per_a %&amp;gt;%
        # only look at top 20 genres
        filter(artist_genres %in% top20genres$artist_genres) %&amp;gt;%
        group_by(artist_genres, doy = as.Date(played_at)) %&amp;gt;%
        arrange(doy) %&amp;gt;%
        summarise(frequency = n_distinct(played_at)) %&amp;gt;%
        ungroup() %&amp;gt;%
        complete(artist_genres, doy = full_seq(doy, period = 1))  %&amp;gt;%
        group_by(artist_genres) %&amp;gt;%
        mutate(frequency = replace(frequency,
                                   is.na(frequency),
                                   0),
               first_played = min(doy[min(which(frequency != 0))]),
               last_played = as.Date(ifelse(frequency == 0, NA, doy)),
               cumulative_frequency = cumsum(frequency),
               last_played = replace(last_played, 
                                     doy &amp;lt; first_played, 
                                     first_played),
               last_played = na.locf(last_played),
               recency = doy - last_played)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After binning both &lt;code&gt;cumulative_frequency&lt;/code&gt; and &lt;code&gt;recency&lt;/code&gt; from the resulting dataset, we can proceed with creating our animated lifecycle grid using &lt;code&gt;ggplot2&lt;/code&gt; and &lt;code&gt;gganimate&lt;/code&gt;. All we need to do is specify the &lt;code&gt;frame =&lt;/code&gt; variable inside the &lt;code&gt;aes()&lt;/code&gt;, and our plot comes to life!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg_life &amp;lt;- genres_by_day %&amp;gt;%
        ggplot(aes(x = genre, y = cumulative_frequency, 
                   fill = artist_genres, frame = doy, 
                   alpha = 0.8)) +
        theme_bw() +
        theme(panel.grid = element_blank())+
        geom_bar(stat=&amp;quot;identity&amp;quot;,position=&amp;quot;identity&amp;quot;) +
        facet_grid(segm.freq ~ segm.rec, drop = FALSE) +
        ggtitle(&amp;quot;LifeCycle Grid&amp;quot;) +
        xlab(&amp;quot;Genres&amp;quot;) +
        ylab(&amp;quot;Cumulative Frequency&amp;quot;) +
        guides(fill = guide_legend(ncol = 1),
               alpha = FALSE)
        
gganimate(gg_life)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://tamaszilagyi.com/img/lifecycle.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;More than anything, the plot makes it obvious that I cannot go on for too long without listening to my favourite genres such as &lt;code&gt;jazz blues&lt;/code&gt;, &lt;code&gt;hip hop&lt;/code&gt; and &lt;code&gt;psychedelic trance&lt;/code&gt;. My least often played genres from the top 20 on the other hand are distributed pretty evenly across the &lt;strong&gt;recency axis&lt;/strong&gt; of my plot in the last row (containing genres with less than or equal to 50 listens).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-left&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What’s left?&lt;/h1&gt;
&lt;p&gt;Clearly, there are tons of other interesting questions that could be explored using this dataset. We could for example look at how many tracks I usually listen to in one go, which songs I skipped over, how my different playlists are growing over time, which playlist or albums I listen to the most…and the list goes on.&lt;/p&gt;
&lt;p&gt;I’ll go into more detail on my approach to automating acquisition and cleaning of this data in a &lt;a href=&#34;http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/&#34;&gt;next post&lt;/a&gt;, but if you just cannot wait to start collecting your own Spotify listening history, I encourage you to go through &lt;a href=&#34;https://developer.spotify.com/web-api/authorization-guide/#authorization_code_flow&#34;&gt;Spotify’s authoriziation flow&lt;/a&gt; and set up a simple cronjob that pings the API &lt;em&gt;X times a day&lt;/em&gt;. The sooner you start collecting your data, the more you’ll have to play with. Everything else can be dealt with later.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Starting a blog(down)</title>
      <link>/blog/starting-a-blogdown/</link>
      <pubDate>Sun, 14 May 2017 21:13:14 -0500</pubDate>
      
      <guid>/blog/starting-a-blogdown/</guid>
      <description>&lt;div id=&#34;starting-an-analytics-blog&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Starting an analytics blog&lt;/h1&gt;
&lt;p&gt;Having learned lots from the open source community over the past years - from blogs and videos to attending meetups and awesome conferences - I have decided to start a blog myself, and share some of the things I find interesting. I expect most of the posts to be &lt;code&gt;R&lt;/code&gt; specific, because that’s what I am most comfortable with. However I do enjoy fiddling with other technologies such as &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;Spark&lt;/code&gt;, so watch out! In a nuthsell though, this blog will be about using open source tools to build all sorts of cool things with &lt;strong&gt;data&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;its-easy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;It’s easy&lt;/h1&gt;
&lt;p&gt;You can get your blog up and running with literally three lines of &lt;code&gt;R&lt;/code&gt; code. After hearing about the &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;&lt;strong&gt;blogdown&lt;/strong&gt;&lt;/a&gt; package on Twitter, I went ahead and downloaded the current build from Github, running &lt;code&gt;install_github(&#39;rstudio/blogdown&#39;)&lt;/code&gt; inside &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Under the hood, blogdown uses &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; to generate the website, but wraps most functionality nicely, so there’s no need for much manual configuration during the process, if at all.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;We first create a folder for the blog on our computer, and set it as our home directory using &lt;code&gt;setwd(&amp;quot;path-to-blog&amp;quot;)&lt;/code&gt;. Then we simply run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1 if you haven&amp;#39;t already, install blogdown
devtools::install_github(&amp;#39;rstudio/blogdown&amp;#39;)
# 2 install hugo
blogdown::install_hugo()
# 3 create new site
blogdown::new_site()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it. You now have a complete folder structure initialized in your working directory:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/img/folderstruct.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The local build of your new site is now running on &lt;code&gt;localhost&lt;/code&gt;. You can see it in RStudio’s Viewer, or inside a browser by clicking &lt;em&gt;Show in new window&lt;/em&gt; in the top left corner of the Viewer.&lt;/p&gt;
&lt;p&gt;You future blog posts will reside in the &lt;code&gt;content/post&lt;/code&gt; folder. Here we find two pre-existing posts as &lt;code&gt;.Rmd&lt;/code&gt; files. We can start editing these straight away and see the results immediately after saving. Because everytime you save changes, your site is instantly rebuilt. If you come back to work on your existing site, you can simply run the function &lt;code&gt;serve_site()&lt;/code&gt; after you are done editing, and see the site regenerated accordingly in the Viewer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;customization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Customization&lt;/h2&gt;
&lt;p&gt;Now we can begin to customize the look of our blog by installing a theme using the function &lt;code&gt;install_theme(&#39;username/theme&#39;)&lt;/code&gt;. For my site, I picked &lt;a href=&#34;https://github.com/nishanths/cocoa-hugo-theme&#34;&gt;nishanths/cocoa-hugo-theme&lt;/a&gt; which I like very much for its minimalistic design. You can browse other themes on &lt;a href=&#34;https://themes.gohugo.io/&#34;&gt;themes.gohugo.io/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configuration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;p&gt;The only thing left to do, is to edit the &lt;code&gt;config.toml&lt;/code&gt; file and set the name of your blog, avatars, or even link a &lt;code&gt;Google Analytics&lt;/code&gt; account - if the theme allows for. The file contains parameters such as:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;title        = &amp;quot;Tamas Szilagyi&amp;quot;
baseurl      = &amp;quot;http://tamaszilagyi.com/&amp;quot;
relativeurls = true
languageCode = &amp;quot;en-us&amp;quot;
theme        = &amp;quot;cocoa-hugo-theme&amp;quot;
faviconfile  = &amp;quot;img/leaf.ico&amp;quot;
github       = &amp;quot;//github.com/mtoto&amp;quot;
highlightjs  = true
avatar       = &amp;quot;img/profile_pic.png&amp;quot; 
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are going to include &lt;code&gt;R&lt;/code&gt; codechunks in your posts, also don’t forget to set &lt;code&gt;highlightjslanguages = [&amp;quot;r&amp;quot;]&lt;/code&gt;. When the blog is ready, we run &lt;code&gt;build_site()&lt;/code&gt; to compile the files to &lt;code&gt;html&lt;/code&gt; and build the website. What we need for deployment will reside under the &lt;code&gt;/public&lt;/code&gt; folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deployment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Again, publishing is a piece of cake. There are &lt;a href=&#34;https://bookdown.org/yihui/blogdown/deployment.html&#34;&gt;multiple ways&lt;/a&gt; for conveniently deploying a blogdown site, and being somewhat familiar with &lt;a href=&#34;https://pages.github.com/&#34;&gt;Github Pages&lt;/a&gt;, that’s what I went for. I created a new repository named &lt;code&gt;mtoto.github.io&lt;/code&gt; and simply pushed the contents of &lt;code&gt;/public&lt;/code&gt; to the master branch.&lt;/p&gt;
&lt;p&gt;The website should be almost immediately available at the same address as the repo name. If you want an url other than &lt;code&gt;username.github.io&lt;/code&gt; however, you will need to sign up with a hosting provider. Then put a file in the &lt;code&gt;/public&lt;/code&gt; folder called &lt;code&gt;CNAME&lt;/code&gt;, with a one liner containing your blog url such as &lt;code&gt;tamaszilagyi.com&lt;/code&gt;. After, you push this file to Github and ask your provider to point your domain to the github pages url.&lt;/p&gt;
&lt;p&gt;And voilà, we have ourselves a full functioning static website that looks great, is easy to manage and as portable as it gets may you decide to switch for different hosting solutions.&lt;/p&gt;
&lt;p&gt;For a more in-depth overview of what &lt;code&gt;blogdown&lt;/code&gt; is capable of, keep an eye on its &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;bookdown site&lt;/a&gt; which is currently under development.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>