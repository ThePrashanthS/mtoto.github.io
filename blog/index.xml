<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Coding with Data</title>
    <link>http://tamaszilagyi.com/blog/index.xml</link>
    <description>Recent content in Blog on Coding with Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Aug 2017 23:15:14 -0500</lastBuildDate>
    <atom:link href="http://tamaszilagyi.com/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Self-learning Hue Lights</title>
      <link>http://tamaszilagyi.com/blog/self-learning-hue-lights/</link>
      <pubDate>Wed, 30 Aug 2017 23:15:14 -0500</pubDate>
      
      <guid>http://tamaszilagyi.com/blog/self-learning-hue-lights/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;the-rise-of-the-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The rise of the API&lt;/h2&gt;
&lt;p&gt;Rich API interfaces are one of the main ingredients of today’s smart devices. They are by definition built for interconnectivity and there is an active community of developers creating apps as microservices on top of them. Philips Hue is no exception with it’s wide variety of &lt;a href=&#34;http://www.developers.meethue.com/otherapps/otherAppsIOS.html#appsList&#34;&gt;apps&lt;/a&gt; available to users.&lt;/p&gt;
&lt;p&gt;But you don’t need to code an entire mobile application to take advantage of the low level access. Using modern tools it only takes a few lines of code to build a self-learning algorithm, running in production in your home. Not only can we access external API’s, we can just as easily expose static files, functions or models as an API of our own.&lt;/p&gt;
&lt;p&gt;My original inspiration for this post was &lt;a href=&#34;https://sc5.io/posts/autonomous-indoor-lighting-using-neural-networks/&#34;&gt;Max Pagel’s article&lt;/a&gt; on training a neural network to automatically control his Philips Hue lights. In fact, I purchased my first set of Hue bulbs because of it. In summary, this post will describe how to build and productionize a classifier in &lt;code&gt;R&lt;/code&gt; that controls the brightness of Philips Hue lights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-dinnertime&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stop, dinnertime!&lt;/h2&gt;
&lt;p&gt;Much like in my &lt;a href=&#34;http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/&#34;&gt;post on Spotify&lt;/a&gt; I have set up a cronjob to execute the Python script that pings the API and saves the lights’ state data locally, to be picked up by Luigi tasks for parsing and copying to S3 further downstream. You can find the relevant code on my &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/tasks.py&#34;&gt;Github&lt;/a&gt;. The &lt;a href=&#34;https://www.developers.meethue.com/philips-hue-api&#34;&gt;Hue API documentation&lt;/a&gt; contains information on authentication and the types of calls available.&lt;/p&gt;
&lt;p&gt;The starting point for this post will be the parsed &lt;code&gt;.json&lt;/code&gt; file containing all of the log data for my “Dinner Lamps”. They are the two main lights in my living and dining area room at the moment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(aws.s3)
library(jsonlite)
# read file from amazon
aws.signature::use_credentials()
df &amp;lt;- s3read_using(object = paste0(&amp;quot;hue_full_2017-08-26.json&amp;quot;), fromJSON, bucket = &amp;quot;ams-hue-data&amp;quot;)
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    30322 obs. of  15 variables:
##  $ on.1       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ on.2       : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ bri.2      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ type.1     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ type.2     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...
##  $ bri.1      : int  131 131 131 131 131 131 131 131 131 131 ...
##  $ modelid.2  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ modelid.1  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name.1     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; ...
##  $ reachable.1: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ reachable.2: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
##  $ name.2     : chr  &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ alert.1    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ log_time   : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:10:02&amp;quot; &amp;quot;2017-05-12 17:15:01&amp;quot; ...
##  $ alert.2    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the original &lt;code&gt;.json&lt;/code&gt; file is such that each lamp has a separate (numbered) column for every variable. The dataset is essentially a timeseries where each row represent a snapshot of the lamps’ state at &lt;code&gt;$log_time&lt;/code&gt;, or &lt;strong&gt;every 5 minutes&lt;/strong&gt;. Before moving on, let’s tidy things up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
tidy_df &amp;lt;- df %&amp;gt;% gather(key, value, -log_time) %&amp;gt;%
        separate(key, into = c(&amp;quot;variable&amp;quot;, &amp;quot;lamp&amp;quot;), sep = &amp;quot;\\.&amp;quot;) %&amp;gt;%
        spread(variable, value)
str(tidy_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    60644 obs. of  9 variables:
##  $ log_time : chr  &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:00:02&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; &amp;quot;2017-05-12 17:05:01&amp;quot; ...
##  $ lamp     : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; ...
##  $ alert    : chr  &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; &amp;quot;none&amp;quot; ...
##  $ bri      : chr  &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; &amp;quot;131&amp;quot; ...
##  $ modelid  : chr  &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; &amp;quot;LWB010&amp;quot; ...
##  $ name     : chr  &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; &amp;quot;Dinner Lamp 2&amp;quot; &amp;quot;Dinner Lamp 1&amp;quot; ...
##  $ on       : chr  &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; &amp;quot;FALSE&amp;quot; ...
##  $ reachable: chr  &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; &amp;quot;TRUE&amp;quot; ...
##  $ type     : chr  &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; &amp;quot;Dimmable light&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 15 columns are now reduced to 9 because each variable appears only once thanks to adding the key column &lt;code&gt;$lamp&lt;/code&gt; to the dataset. But we are not quite done cleaning yet: I use the two lamps in sync, so we need only data from one of them. When the lamps are not &lt;code&gt;on&lt;/code&gt; nor &lt;code&gt;reachable&lt;/code&gt;, &lt;code&gt;$bri&lt;/code&gt; should be set to &lt;code&gt;0&lt;/code&gt;. Using the now correct brightness values, we create the four categories for the classifier to work with. Lastly, there were days I wasn’t home, so we can rid of of those observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
binned_df &amp;lt;- tidy_df %&amp;gt;% filter(lamp == &amp;quot;1&amp;quot;) %&amp;gt;%
        mutate(bri = as.numeric(replace(bri, on==&amp;quot;FALSE&amp;quot; | reachable==&amp;quot;FALSE&amp;quot;,0)),
               y = as.factor(ifelse(bri == 0, &amp;quot;zero&amp;quot;,
                                    ifelse(between(bri,0,80), &amp;quot;dim&amp;quot;,
                                           ifelse(between(bri,80,160),&amp;quot;mid&amp;quot;,&amp;quot;bright&amp;quot;)))))

off_days &amp;lt;- binned_df %&amp;gt;% group_by(date = as.Date(log_time,tz=&amp;quot;Europe/Amsterdam&amp;quot;)) %&amp;gt;%
                dplyr::summarise(total_bri = sum(bri)) %&amp;gt;%
                filter(total_bri == 0 ) %&amp;gt;%
                select(date)

binned_df &amp;lt;- binned_df %&amp;gt;% filter(!as.Date(log_time) %in% off_days$date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the distribution of our target variable look?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(binned_df$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## bright    dim    mid   zero 
##    598   1533   1710  23889&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Roughly 86% of the time the lamps are off, resulting in an unbalanced dataset. What about brightness values lamps were &lt;em&gt;on&lt;/em&gt;, according to the three remaining categories?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution seems to be close to normal with a positive skew, and a massive outlier all the way at the end of the spectrum. That’s maximum brightness, the default when I switch the lights on/off with a physical switch.&lt;/p&gt;
&lt;p&gt;To get an intuition for my usage patterns, I’ll also plot a histogram of hour of the day for all four categories.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The only times the lamps are &lt;strong&gt;not&lt;/strong&gt; structurally off, is in the evening and the early hours. Dim and mid values are the dominant category after 8PM instead. Another slight dip in zero appears around and shortly after midnight, compensated by the second largest peak in dim, and a few instances of mid and bright. Bright observations in general are sparse and will be tough to predict.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-we-learn-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can we learn this?&lt;/h2&gt;
&lt;p&gt;The only variables I will use for training, are time based: &lt;em&gt;day of the week&lt;/em&gt;; &lt;em&gt;month&lt;/em&gt;; &lt;em&gt;week number&lt;/em&gt;; &lt;em&gt;weekend or not&lt;/em&gt;; &lt;em&gt;time of the day&lt;/em&gt;; and &lt;em&gt;minutes since 12PM, 6AM, 12AM and 6PM&lt;/em&gt;. A datetime string will then suffice to generate a prediction on the fly, a boon for putting things into production later on. I packaged a chain of dplyr commands inside the function &lt;a href=&#34;https://github.com/mtoto/hue/blob/master/functions.R&#34;&gt;add_vars()&lt;/a&gt; to add the above variables to the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_vars &amp;lt;- binned_df %&amp;gt;% add_vars(extra_var = &amp;quot;yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember we are dealing with an unbalanced dataset, that also happens to be a timeseries. To remedy the former, I will use class weights to penalize the misclassification of the minority classes. Secondly, I will look at the &lt;em&gt;Area Under the Curve&lt;/em&gt; to evaluate the model, which will be less affected than &lt;em&gt;Accuracy&lt;/em&gt; by class imbalance if I set dim as the positive class. If not for these measures, the algorithm would gladly classify 100% of instances as &lt;code&gt;&amp;quot;zero&amp;quot;&lt;/code&gt;, achieving stunning accuracy on paper and complete darkness in my living room.&lt;/p&gt;
&lt;p&gt;Now, why does it matter that we have a timeseries? In any dataset with a time component, the split between train and test sets should not be random. Otherwise, the model can and will &lt;strong&gt;learn from the future&lt;/strong&gt;, and severely overfit the data. The correct cross-validation strategy instead is to fold the data according to time. Train should always the before and test the after. For our convenience &lt;code&gt;caret&lt;/code&gt; provides the &lt;code&gt;createTimeSlices()&lt;/code&gt; function to create the indices of the CV-folds. An extra &lt;code&gt;testing&lt;/code&gt; set will be held out to validate our model on unseen data after we are done modeling.&lt;/p&gt;
&lt;p&gt;We’ll now train a &lt;a href=&#34;https://cran.r-project.org/web/packages/gbm/index.html&#34;&gt;gbm&lt;/a&gt; model, using the &lt;a href=&#34;https://topepo.github.io/caret/&#34;&gt;caret&lt;/a&gt; package, which comes with a myriad of convenience tools to make the process easier and the code a lot more concise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
# Split train and test sets
training &amp;lt;- df_vars[df_vars$date &amp;lt; &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date,-log_time)
testing &amp;lt;- df_vars[df_vars$date &amp;gt;= &amp;quot;2017-08-05&amp;quot;,] %&amp;gt;% select(-date)

# create cross validation folds
idx &amp;lt;- createTimeSlices(1:nrow(training), 
                      initialWindow = 15000, 
                      horizon = 5000, skip = 1000, fixedWindow = F)

# create model weights vector
model_weights &amp;lt;- ifelse(training$y == &amp;quot;zero&amp;quot;,0.2,
                        ifelse(training$y == &amp;quot;mid&amp;quot;,1.2,1))

# define cross validation logic
fitControl &amp;lt;- trainControl(## 10-fold CV
        index = idx[[1]],
        indexOut = idx[[2]],
        summaryFunction = multiClassSummary,
        classProbs = T)

# create tunegrid for hyperparameter search
gbmGrid &amp;lt;-  expand.grid(interaction.depth = c(1,3,5), 
                        n.trees = c(5,10,30), 
                        shrinkage = c(0.1),
                        n.minobsinnode = 5)

# train model
gbmFit &amp;lt;- train(y ~ ., data = training, 
                method = &amp;quot;gbm&amp;quot;, 
                trControl = fitControl,
                metric = &amp;quot;AUC&amp;quot;,
                weights = model_weights,
                verbose = FALSE,
                tuneGrid = gbmGrid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Printing &lt;code&gt;gbmFit&lt;/code&gt; to the console will give us the performance metrics across hyperparameters, and the ultimately selected values maximizing our metric of choice. While this is certainly useful information, I find it more intuitive to immediately look at the confusion matrix and see where our model is going off the rails:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds&amp;lt;-predict(gbmFit, testing)
table(preds, testing$y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
## preds     dim  mid bright zero
##   dim     101  196     18  397
##   mid      47   90     19  202
##   bright   33   57      0    9
##   zero     39  100     28 4136&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most mistakes are made trying to classify bright and mid. The model gets 0 bright values right and only manages to do so correctly 14% of the time for mid. But when do errors happen? To dig a little deeper let’s look at the previous histogram of categories by hour again for the test set, but now with the predictions overlaid on top.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../blog/2017/2017-05-14-hue_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bright values were always going be hard to guess, but the model at least comes close in terms of hours, but off on the exact days. The majority of misclassification comes from overzealously predicting dim in the evening and around midnight, when it should really be either mid or zero. That looks like a workable scenario for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-ship-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Let’s ship it!&lt;/h1&gt;
&lt;p&gt;To control the lights, we can make PUT requests to the Hue bridge. To set &lt;em&gt;bri&lt;/em&gt;, we need actual brightness values. An intuitive option is to pick the median values per category per hour:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median_values &amp;lt;- binned_df %&amp;gt;% filter(bri &amp;gt; 0) %&amp;gt;% 
                mutate(hour = lubridate::hour(as.POSIXct(log_time, tz = &amp;quot;Europe/Amsterdam&amp;quot;))) %&amp;gt;%
                select(hour,bri, y) %&amp;gt;% 
                group_by(y, hour) %&amp;gt;%
                dplyr::summarise(med = median(bri)) %&amp;gt;%
                ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we only used date and time based features for modeling, all we need for a prediction is a timestamp string. Using &lt;code&gt;for_sample&lt;/code&gt; and &lt;code&gt;def_vars()&lt;/code&gt;, we define a custom function &lt;code&gt;predict_hue()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_hue &amp;lt;- function(timestamp){
        
        df &amp;lt;- data.frame(log_time =as.POSIXct(timestamp)) %&amp;gt;% 
                add_vars(extra_var = &amp;quot;no&amp;quot;)
        
        pred &amp;lt;- predict(gbmFit, newdata = df)
        
        if (pred==&amp;quot;zero&amp;quot;) {
                x &amp;lt;- 0
        } else {
                x &amp;lt;- median_values %&amp;gt;% filter(y == pred &amp;amp; hour == lubridate::hour(timestamp)) %&amp;gt;%
                select(med) %&amp;gt;% unlist()
        }
        
        return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to expose the above function as an API, we literally need three lines of code with &lt;a href=&#34;https://cran.r-project.org/web/packages/jug/vignettes/jug.html&#34;&gt;jug&lt;/a&gt;. Ever since I saw the package &lt;a href=&#34;https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/jug-Building-Web-APIs-for-R&#34;&gt;presented at useR2017&lt;/a&gt;, I have been looking for a use case to play with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jug)
jug() %&amp;gt;% post(&amp;quot;/predict-hue&amp;quot;, decorate(predict_hue)) %&amp;gt;%
        simple_error_handler_json() %&amp;gt;%
        serve_it()
#Serving the jug at http://127.0.0.1:8080&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great. We can now make calls to this API and get a prediction supplying the current time. The nifty thing is that because API’s are language agnostic, we can access it from the programming paradigm of our choosing. I currently have a basic Python function that communicates with both API’s, transferring a prediction to the Hue Bridge every 5 minutes. But we could just as well build a whole interface on top, or create a chatbot for improved user experience. Perhaps I’ll do a follow-up post on this topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;There is something very satisfying about building your own data products and seeing them at work. Even something as trivial as controlling the light switch for you. I only have data since this year May, so there’s a good chance our model will get smarter as days go by. We can easily schedule to retrain the model every week or even day, without having to lift a finger. Most of the code in this post is packaged up as a handful of &lt;code&gt;R&lt;/code&gt; functions deployed on my Raspberry Pi. Now, when I &lt;em&gt;choose&lt;/em&gt; to pass out on my couch next time, at least lights won’t stay on for too long.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Creating a Spotify Playlist using Luigi</title>
      <link>http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/</link>
      <pubDate>Sat, 22 Jul 2017 21:13:14 -0500</pubDate>
      
      <guid>http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/</guid>
      <description>&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;!-- BLOGDOWN-BODY-BEFORE --&gt;
&lt;!-- /BLOGDOWN-BODY-BEFORE --&gt;
&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;In the &lt;a href=&#34;http://tamaszilagyi.com/blog/analyzing-my-spotify-listening-history/&#34;&gt;previous post&lt;/a&gt;, I shared an analysis of my Spotify listening history using R. In this post, I will discuss what came before having the data: collecting, cleaning and saving it. As the title suggest, we will even go a step further and automate the creation of a weekly top 10 playlist in Spotify using the very same dataset.&lt;/p&gt;
&lt;p&gt;The main ingredient will be Luigi, a Python framework for workflow management Spotify open-sourced a couple of years ago. According to &lt;a href=&#34;http://luigi.readthedocs.io/en/stable/index.html&#34;&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures will happen. These tasks can be anything, but are typically long running things like Hadoop jobs, dumping data to/from databases, running machine learning algorithms, or anything else.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Designed for massive jobs, implementing Luigi on top of tiny little &lt;code&gt;.json&lt;/code&gt; files might seem like a huge overkill, but the logic we will define won’t considerably differ from larger scale applications.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pipeline&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The pipeline&lt;/h1&gt;
&lt;p&gt;We can break down the pipeline into four tasks.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://tamaszilagyi.com/img/luigi2.png&#34; /&gt;

&lt;/div&gt;
&lt;div id=&#34;cronjob-to-ping-the-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Cronjob to ping the API&lt;/h2&gt;
&lt;p&gt;The initial step is to ping the API, and store the raw response as a &lt;code&gt;.json&lt;/code&gt; file locally. We need to have the &lt;code&gt;client_id&lt;/code&gt;, &lt;code&gt;client_secret&lt;/code&gt; and a &lt;code&gt;refresh_token&lt;/code&gt; to generate a temporary access token. Follow the &lt;a href=&#34;https://developer.spotify.com/web-api/tutorial/&#34;&gt;Web API tutorial by Spotify&lt;/a&gt; to attain them. In turn, the access token is required to make calls to the API.&lt;/p&gt;
&lt;p&gt;We start with two functions: One to generate the &lt;code&gt;access_token&lt;/code&gt; using our credentials (I have them inside &lt;code&gt;spotify_creds&lt;/code&gt;), and a second one to download our listening history, dumping the data in a new &lt;code&gt;.json&lt;/code&gt; file every day. To make sure that the access token doesn’t expire, we’ll generate a new one with every call to the API.&lt;/p&gt;
&lt;p&gt;I will store functions inside &lt;code&gt;functions.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import requests
import json
import datetime
from spotify_creds import *

# Get access token
def access_token():
    
    body_params = {&amp;#39;grant_type&amp;#39; : &amp;#39;refresh_token&amp;#39;,
                &amp;#39;refresh_token&amp;#39; : refresh_token}

    url = &amp;#39;https://accounts.spotify.com/api/token&amp;#39;
    response = requests.post(url, 
                             data = body_params, 
                             auth = (client_id, client_secret))
    
    response_dict = json.loads(response.content)
    accessToken = response_dict.get(&amp;#39;access_token&amp;#39;)

    return accessToken
    
# Get most recent songs and append the response
# to a new json file every day
def download_data():

    current_time = datetime.datetime.now().strftime(&amp;#39;%Y-%m-%d&amp;#39;)
    filename = &amp;#39;/spotify/json/spotify_tracks_%s.json&amp;#39; % current_time
    
    accesToken = access_token()
    headers = {&amp;#39;Authorization&amp;#39;: &amp;#39;Bearer &amp;#39; + accesToken }
    payload = {&amp;#39;limit&amp;#39;: 50}

    url = &amp;#39;https://api.spotify.com/v1/me/player/recently-played&amp;#39;
    response = requests.get(url, headers = headers,
                            params = payload)
    data = response.json()

    with open(filename, &amp;#39;a&amp;#39;) as f:
        json.dump(data[&amp;#39;items&amp;#39;], f)
        f.write(&amp;#39;\n&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to make sure that I don’t miss any songs I have listened to, I will set up a cronjob to execute &lt;code&gt;download_data()&lt;/code&gt; (that’s what &lt;code&gt;logger.py&lt;/code&gt; contains) every three hours. We first make this file executable&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;chmod +x /spotify/logger.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;open crontab,&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;crontab -e&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and add the following line to our list of cronjobs:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;0 */3 * * * /usr/bin/python /spotify/logger.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part with the numbers and &lt;code&gt;*&lt;/code&gt;’s gives the scheduling logic. The second bit is the Python environment from which to call the script. If you prefer self-contained environments instead, then this will look something like &lt;code&gt;/home/pi/miniconda/envs/name_of_env/bin/python&lt;/code&gt; on a Raspberry Pi using &lt;a href=&#34;https://conda.io/miniconda.html&#34;&gt;miniconda&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deduplicate-and-save-to-s3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Deduplicate and save to S3&lt;/h2&gt;
&lt;p&gt;With raw data coming in, the next step is to store the result somewhere more robust than the SD card inside my Pi. Because we are pinging the API every three hours, we have files that contain 8 dictionaries of the last 50 tracks. Unless I listen to Spotify non-stop all day every day, there is going to be lots of redundancy because of duplicate records.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;deduplicate()&lt;/code&gt; takes &lt;code&gt;.json&lt;/code&gt; file we created above, and returns the deduplicated list of dictionaries containing only unique items according to the key &lt;code&gt;played_at&lt;/code&gt;, which is the timestamp of each song played.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Cleaner function to get rid of redundancy
def deduplicate(file):
    result =[]
    
    for line in file:
        data = json.loads(line)
        result.extend(data)
    
    result = {i[&amp;#39;played_at&amp;#39;]:i for i in result}.values()
    return result&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this point onwards, we are going to switch to using &lt;strong&gt;Luigi&lt;/strong&gt;. The main building block is a Task, which &lt;em&gt;usually&lt;/em&gt; consists of three methods:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;requires()&lt;/code&gt;: What other task the current one depends on.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run()&lt;/code&gt;: What is our tasks going to do, usually some function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output()&lt;/code&gt;: Where will the result be stored.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In turn, &lt;code&gt;output()&lt;/code&gt; will end up in the &lt;code&gt;require()&lt;/code&gt; method of a consecutive task. This builds a dependency graph between tasks. Let’s jump right in, and look at how we apply this logic:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import luigi
from datetime import date, timedelta
from functions import *

# External task at the bottom of our dependancy graph,
# only looks to see if output of cronjob exists,
# by default from yesterday.
class local_raw_json(luigi.ExternalTask):
    date = luigi.DateParameter(default = date.today()-timedelta(1)) 

    def output(self):
        return luigi.LocalTarget(&amp;#39;spotify/json/spotify_tracks_%s.json&amp;#39; % 
                                 self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;))
        &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first task &lt;code&gt;local_raw_json&lt;/code&gt; is an &lt;strong&gt;External Task&lt;/strong&gt; with only an &lt;code&gt;output()&lt;/code&gt; method. This task does not run anything and does not depend on anything. It simply confirms the existence of a file, namely the output from our cronjob. Luigi allows for parameterization of tasks, so we define a &lt;code&gt;date&lt;/code&gt; parameter with the default value yesterday. We pass this to the &lt;code&gt;output()&lt;/code&gt; method to look for the file with the correct date.&lt;/p&gt;
&lt;p&gt;External tasks with no dependencies are common first steps, especially if we are relying on an external datadump somewhere else.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import json
from luigi.s3 import S3Target, S3Client

# Task that runs our deduplicate() on local file 
# and writes the output to S3 bucket.
class spotify_clean_aws(luigi.Task):
    date = luigi.DateParameter(default = date.today()-timedelta(1)) 
    
    def requires(self):
        return self.clone(local_raw_json)
        
    def run(self):   
        with self.input().open(&amp;#39;r&amp;#39;) as in_file:
            data = deduplicate(in_file)
            
        with self.output().open(&amp;#39;w&amp;#39;) as out_file:
            json.dump(data, out_file)

    def output(self):
        client = S3Client(host = &amp;#39;s3.us-east-2.amazonaws.com&amp;#39;)
        return S3Target(&amp;#39;s3://myspotifydata/spotify_tracks_%s.json&amp;#39; % 
                        self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;), 
                        client=client)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second tasks is &lt;code&gt;spotify_clean_aws&lt;/code&gt;. This is where we run the &lt;code&gt;deduplicate()&lt;/code&gt; function defined earlier and write the output to an &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;AWS S3&lt;/a&gt; bucket. In contrary to the first task, all three methods are present:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Require&lt;/strong&gt; that the raw json file exists, and also &lt;code&gt;clone()&lt;/code&gt; the parameters from the first task. This way the same date parameter will be passed to both tasks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Run&lt;/strong&gt; the function &lt;code&gt;deduplicate()&lt;/code&gt; on the input file and save the result as a .json.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt; the result of the task to S3. Luigi has built-in support for AWS S3 that uses &lt;a href=&#34;https://boto3.readthedocs.io/en/latest/&#34;&gt;boto3&lt;/a&gt; under the hood. To connect, we need to have AWS credentials. They usually reside under &lt;code&gt;~/.aws/credentials&lt;/code&gt;, if you have run &lt;code&gt;aws configure&lt;/code&gt; in the Terminal before:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;[default]
aws_access_key_id = YOUR_ACCESS_KEY
aws_secret_access_key = YOUR_SECRET_KEY &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to pass them explicitly to &lt;code&gt;S3Client()&lt;/code&gt; however.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-relevant-fields-and-create-weekly-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Get relevant fields and create weekly dataset&lt;/h2&gt;
&lt;p&gt;With the data deduplicated and safely stored in the cloud, we can now parse the files, selecting a handful of fields from the &lt;a href=&#34;https://developer.spotify.com/web-api/web-api-personalization-endpoints/get-recently-played/&#34;&gt;response&lt;/a&gt;. Because nobody ever gets excited about ETL code, I will omit the contents of &lt;code&gt;parse_json()&lt;/code&gt; here. It is suffice to say that we get a more compact result than what I used in the previous post. An example record from the resulting dictionary will look like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;{&amp;quot;played_at&amp;quot;: &amp;quot;2017-04-22T18:49:54.108Z&amp;quot;, 
 &amp;quot;track_name&amp;quot;: &amp;quot;Symphony No. 5 In C Minor Part 1&amp;quot;, 
 &amp;quot;duration_ms&amp;quot;: 485293, 
 &amp;quot;type&amp;quot;: &amp;quot;track&amp;quot;, 
 &amp;quot;artist_id&amp;quot;: [&amp;quot;2wOqMjp9TyABvtHdOSOTUS&amp;quot;], 
 &amp;quot;explicit&amp;quot;: false, 
 &amp;quot;uri&amp;quot;: &amp;quot;spotify:track:0ZN01wuIdn4iT8VBggkOMm&amp;quot;, 
 &amp;quot;artist_name&amp;quot;: [&amp;quot;Ludwig van Beethoven&amp;quot;], 
 &amp;quot;track_id&amp;quot;: &amp;quot;0ZN01wuIdn4iT8VBggkOMm&amp;quot;}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can still find all the code for &lt;code&gt;parse_json()&lt;/code&gt; function (and all the others) on my &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/tree/master/data/2017-07-22-spotifyLuigi&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Secondly, we’ll merge a week worth of data and store the intermediate result on S3. With these ingredients, we define our third Luigi Task: &lt;code&gt;spotify_merge_weekly_aws&lt;/code&gt; :&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Task that merges the 7 daily datasets, 
# parses relevant fields, deduplicates records
# and stores the result in S3.
class spotify_merge_weekly_aws(luigi.Task):
    date = luigi.DateParameter(default = (date.today()-timedelta(8)))
    daterange = luigi.IntParameter(7)

    def requires(self):
        # take data from the 7 days following date param (8 days prior to current date by default)
        return [spotify_clean_aws(i) for i in [self.date + timedelta(x) for x in range(self.daterange)]]
     
    def run(self):
        results = []
        for file in self.input():
            
            with file.open(&amp;#39;r&amp;#39;) as in_file:
                data = json.load(in_file)
                parsed = parse_json(data)
                
            results.extend(parsed)
        # merging of daily data creates dupe records still
        result = {v[&amp;#39;played_at&amp;#39;]:v for v in results}.values()
        
        with self.output().open(&amp;#39;w&amp;#39;) as out_file:
            json.dump(result, out_file)
            
    def output(self):
        client = S3Client(host = &amp;#39;s3.us-east-2.amazonaws.com&amp;#39;)
        return S3Target(&amp;#39;s3://myspotifydata/spotify_week_%s.json&amp;#39; % 
                        (self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;) + &amp;#39;_&amp;#39; + str(self.daterange)), 
                         client=client)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-playlist&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Create playlist&lt;/h2&gt;
&lt;p&gt;As a last step, we aggregate the weekly data and fill up our playlist on Spotify. These are the last two functions we need to define. Not to complicate things too much, I am simply going to create a top 10 of &lt;em&gt;my most listened to tracks between 7am and 12pm&lt;/em&gt;. Sort of a morning playlist.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# This function takes a list of track uri&amp;#39;s 
# to replace songs in my morning playlist
# and returns the status code of the put request.
def replace_tracks(tracks):
    
    url = &amp;#39;https://api.spotify.com/v1/users/1170891844/playlists/6a2QBfOgCqFQLN08FUxpj3/tracks&amp;#39;
    accesToken = access_token()
    headers = {&amp;#39;Authorization&amp;#39;: &amp;#39;Bearer &amp;#39; + accesToken,
               &amp;#39;Content-Type&amp;#39;:&amp;#39;application/json&amp;#39;}
    data = {&amp;quot;uris&amp;quot;: &amp;#39;,&amp;#39;.join(tracks)}

    response = requests.put(url, headers = headers,
                            params = data)
                            
    return response.status_code
                            
# This function reads in the weekly dataset 
# as a pandas dataframe, outputs the list of 
# top ten tracks and feeds them to replace_tracks()
def create_playlist(dataset, date):
    
    data = pd.read_json(dataset)          
    data[&amp;#39;played_at&amp;#39;] = pd.to_datetime(data[&amp;#39;played_at&amp;#39;])
    
    data = data.set_index(&amp;#39;played_at&amp;#39;) \
               .between_time(&amp;#39;7:00&amp;#39;,&amp;#39;12:00&amp;#39;)
        
    data = data[data.index &amp;gt; str(date)]
    # aggregate data
    songs = data[&amp;#39;uri&amp;#39;].value_counts()\
                       .nlargest(10) \
                       .index \
                       .get_values() \
                       .tolist()
    # make api call
    res_code = replace_tracks(songs)
    
    return res_code&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we wrap the above inside our last Luigi Task, &lt;code&gt;spotify_morning_playlist&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Task to aggregate weekly data and create playlist
class spotify_morning_playlist(luigi.Task):
    date = luigi.DateParameter(default = (date.today()-timedelta(8)))
    daterange = luigi.IntParameter(7)

    def requires(self):
        return self.clone(spotify_merge_weekly_aws)
    
    def run(self):
        
        with self.input().open(&amp;#39;r&amp;#39;) as in_file:
            res_code = create_playlist(in_file, self.date)      
        # write to file if succesful
        if (res_code == 201):
            with self.output().open(&amp;#39;w&amp;#39;) as out_file:
                json.dump(res_code, out_file)
    
    def output(self):
        client = S3Client(host = &amp;#39;s3.us-east-2.amazonaws.com&amp;#39;)
        return S3Target(&amp;#39;s3://myspotifydata/spotify_top10_%s.json&amp;#39; % 
                        (self.date.strftime(&amp;#39;%Y-%m-%d&amp;#39;) + &amp;#39;_&amp;#39; + str(self.daterange)), 
                        client=client)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have put all of the tasks in a file named &lt;code&gt;tasks.py&lt;/code&gt;. Luigi does not provide a scheduling mechanism out of the box, so we’ll trigger the tasks from crontab instead. For example every Monday at 7AM:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;0 7 * * 1 /usr/bin/python /spotify/tasks.py spotify_morning_playlist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we should have the &lt;a href=&#34;http://luigi.readthedocs.io/en/stable/central_scheduler.html&#34;&gt;Central Scheduler&lt;/a&gt; running in the background for the above to execute. The neat thing is that we only need to trigger the last task, and then Luigi considers all the dependencies and runs them if needed (ie. if the target file does not exists). Additionally, Luigi has a real nice GUI running on &lt;code&gt;localhost:8082&lt;/code&gt;, where we can visualise the complete dependency graph and monitor the progress of our tasks: &lt;img src=&#34;http://tamaszilagyi.com/img/dag.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If nothing fails, the tracks in the below playlist get updated every Monday morning:&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/user/1170891844/playlist/6a2QBfOgCqFQLN08FUxpj3&#34; width=&#34;300&#34; height=&#34;380&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;I have tried to give a simple, yet fully reproducible example of how to set up a workflow using Luigi. It is important to note that building data pipelines for production systems does require a little more effort. To name a few shortcomings of the above: We haven’t defined logging, we didn’t clean up our original files containing the raw response data, and it is very likely that the same tracks will end up in this playlist on consecutive weeks. Not something you would want to happen to your Discover Weekly for example.&lt;/p&gt;
&lt;p&gt;If you are want to learn more about Luigi, I encourage you to read the &lt;a href=&#34;http://luigi.readthedocs.io/en/stable/index.html&#34;&gt;documentation&lt;/a&gt; and most of all start experimenting on personal projects. I find that is always the best way to learn new skills.&lt;/p&gt;
&lt;p&gt;On the other hand, we could also create playlists that are more useful to us than a simple top 10 playlist. What if we took artists we listen to the most, and automatically put their songs not in our listening history yet in a new playlist. It is perfectly possible, and probably more valuable to us as users. We just need to write a couple new functions, plug them into a similar Luigi pipeline as above and let it do the work for us.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing My Spotify Listening History</title>
      <link>http://tamaszilagyi.com/blog/analyzing-my-spotify-listening-history/</link>
      <pubDate>Sun, 02 Jul 2017 21:13:14 -0500</pubDate>
      
      <guid>http://tamaszilagyi.com/blog/analyzing-my-spotify-listening-history/</guid>
      <description>&lt;style type=&#34;text/css&#34;&gt;
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
&lt;/style&gt;
&lt;div id=&#34;a-new-endpoint&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A new endpoint&lt;/h1&gt;
&lt;p&gt;Following an &lt;a href=&#34;https://github.com/spotify/web-api/issues/20&#34;&gt;avalanche of &lt;em&gt;+1&lt;/em&gt; comments&lt;/a&gt; on the GitHub issue requesting access to a user’s play history, on March 1st Spotify released &lt;a href=&#34;https://developer.spotify.com/web-api/web-api-personalization-endpoints/get-recently-played/&#34;&gt;a new endpoint&lt;/a&gt; to their Web API that allows anyone with a Spotify account to pull data on his or her most recently played tracks. To access it, you need go through the &lt;a href=&#34;https://developer.spotify.com/web-api/authorization-guide/#authorization_code_flow&#34;&gt;Authorization Code Flow&lt;/a&gt;, where you get keys and tokens needed for making calls to the API. The return object contains your 50 most recently played songs enriched by some contextual data.&lt;/p&gt;
&lt;p&gt;Being an avid Spotify user, I figured I could use my recently purchased &lt;a href=&#34;https://www.raspberrypi.org/&#34;&gt;Raspberry Pi&lt;/a&gt; to ping the API every 3 hours, and start collecting my Spotify data. I started begin April, so now I have almost three months worth of listening history.&lt;/p&gt;
&lt;p&gt;How I set up a data pipeline that pings the API, parses the response and stores it as .json file, will be the subject of a follow-up post. Here, I will instead focus on exploring certain aspects of the data I thus far collected, using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-do-we-have-here&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What do we have here&lt;/h1&gt;
&lt;p&gt;Besides my play history, I also store additional variables for every artist, album and playlist that I have listened to as separate json files. For the purpose of this post however, I’ll only focus on my listening history and additional data on artists. You can find both files on my &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/tree/master/data/2017-06-02-spotifyR&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s read the data into &lt;code&gt;R&lt;/code&gt;, using the &lt;code&gt;fromJSON()&lt;/code&gt; function from the &lt;code&gt;jsonlite&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jsonlite)

df_arts &amp;lt;- fromJSON(&amp;quot;/data/spotify_artist_2017-06-30.json&amp;quot;)
df_tracks &amp;lt;- fromJSON(&amp;quot;/data/spotify_tracks_2017-06-30.json&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most important file is &lt;strong&gt;df_tracks&lt;/strong&gt;; this is the parsed response from the &lt;strong&gt;Recently Played Tracks&lt;/strong&gt; endpoint. Let’s take a look.&lt;/p&gt;
&lt;div id=&#34;df_tracks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;df_tracks&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    3274 obs. of  8 variables:
##  $ played_at  : chr  &amp;quot;2017-06-24T18:57:25.899Z&amp;quot; ...
##  $ artist_name:List of 3274
##   .. [list output truncated]
##  $ artist_id  :List of 3274
##   .. [list output truncated]
##  $ track_name : chr  &amp;quot;People In Tha Middle&amp;quot; ...
##  $ explicit   : logi  FALSE ...
##  $ uri        : chr  &amp;quot;spotify:user:1170891844:playlist:29XAftFCmwVBJ64ROX8gzA&amp;quot; ...
##  $ duration_ms: int  302138 226426 ...
##  $ type       : chr  &amp;quot;playlist&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a data.frame of &lt;strong&gt;3274 observations&lt;/strong&gt; and &lt;strong&gt;8 variables&lt;/strong&gt;. The number of rows is equal to the number of songs I have listened to, as the variable &lt;code&gt;played_at&lt;/code&gt; is unique in the dataset. Here’s a short description of the the variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;played_at&lt;/code&gt;: The timestamp when the track started playing.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_name&lt;/code&gt; &amp;amp; &lt;code&gt;artist_id&lt;/code&gt; : List of names and id’s of the artists of the song.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;track_name&lt;/code&gt;: Name of the track.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;explicit&lt;/code&gt;: Do the lyrics contain bad words?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uri&lt;/code&gt;: Unique identifier of the context, either a &lt;em&gt;playlist&lt;/em&gt; or an &lt;em&gt;album&lt;/em&gt; (or empty).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;duration_ms&lt;/code&gt;: Number of miliseconds the song lasts.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;type&lt;/code&gt; : Type of the context in which the track was played.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can see two issues at first glance. For starters, the variable &lt;code&gt;played_at&lt;/code&gt; is of class &lt;code&gt;character&lt;/code&gt; while it should really be a timestamp. Secondly, both &lt;code&gt;artist_...&lt;/code&gt; columns are of class &lt;code&gt;list&lt;/code&gt; because one track can have several artists. This will become inconvenient when we want to use the variable &lt;code&gt;artist_id&lt;/code&gt; to merge the two datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;df_arts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;df_arts&lt;/h2&gt;
&lt;p&gt;The second &lt;code&gt;data.frame&lt;/code&gt; consists of a couple of additional variables concerning the artists:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1810 obs. of  4 variables:
##  $ artist_followers : int  256962 30345 ...
##  $ artist_genres    :List of 1810
##   .. [list output truncated]
##  $ artist_id        : chr  &amp;quot;32ogthv0BdaSMPml02X9YB&amp;quot; ...
##  $ artist_popularity: int  64 57 ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;artist_followers&lt;/code&gt;: The number of Spotify users following the artist.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_genres&lt;/code&gt; : List of genres the artist is associated with.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_id&lt;/code&gt;: Unique identifier of the artist.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist_popularity&lt;/code&gt;: Score from 1 to 100 regarding the artist’s popularity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By joining the two dataframes we are mostly looking to enrich the original data with &lt;code&gt;artist_genre&lt;/code&gt;, a variable we’ll use for plotting later on. Similarly to artists, albums and tracks also have &lt;a href=&#34;https://developer.spotify.com/web-api/endpoint-reference/&#34;&gt;API endpoints&lt;/a&gt; containing a genre field. However, the more granular you get, the higher the prevalence of no associated genres. Nevertheless, there is still quite some artists where genres is left blank.&lt;/p&gt;
&lt;p&gt;So, let’s unnest the list columns, convert &lt;code&gt;played_at&lt;/code&gt; to timestamp and merge the the dataset with &lt;strong&gt;df_arts&lt;/strong&gt;, using the key &lt;code&gt;&amp;quot;artist_id&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)

merged &amp;lt;- df_tracks %&amp;gt;% 
        unnest(artist_name, artist_id) %&amp;gt;% 
        mutate(played_at = as.POSIXct(played_at, 
                                      tz = &amp;quot;CET&amp;quot;, 
                                      format = &amp;quot;%Y-%m-%dT%H:%M:%S&amp;quot;)) %&amp;gt;%
        left_join(df_arts, by=&amp;quot;artist_id&amp;quot;) %&amp;gt;% 
        select(-artist_id)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-top-10&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;My Top 10&lt;/h1&gt;
&lt;p&gt;First things first, what was my three month top 10 most often played songs?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top10 &amp;lt;- merged %&amp;gt;% 
        group_by(track_name) %&amp;gt;%
        summarise(artist_name = head(artist_name,1),
                  # cuz a song can have multiple artist
                  plays = n_distinct(played_at)) %&amp;gt;%
        arrange(-plays) %&amp;gt;%
        head(10)
top10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##                         track_name      artist_name plays
##                              &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
##  1                          Habiba             Boef    14
##  2                       Too young          Phoenix    13
##  3                   Give Me Water       John Forte    11
##  4               Gentle Persuasion Doug Hream Blunt    10
##  5                   Dia Ja Manche    Dionisio Maio     9
##  6                   Run, Run, Run      Ann Peebles     9
##  7                         Heygana  Ali Farka Touré     8
##  8 It Ain&amp;#39;t Me (with Selena Gomez)             Kygo     8
##  9                   Perfect World     Broken Bells     8
## 10                       Bencalado       Zen Baboon     7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How did these songs reach the top? Is there a relationship between the first time I played the song in the past three months, the number of total plays, and the period I played each the song the most? One way to explore these questions is by plotting a cumulative histogram depicting the number of plays over time for each track.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Using ggplot2
library(ggplot2)
library(zoo)

plot &amp;lt;- merged %&amp;gt;% 
        filter(track_name %in% top10$track_name) %&amp;gt;%
        mutate(doy = as.Date(played_at, 
                             format = &amp;quot;%Y-%m-%d&amp;quot;),
               track_name = factor(track_name, 
                                   levels = top10$track_name)) %&amp;gt;%
        complete(track_name, doy = full_seq(doy, period = 1)) %&amp;gt;%
        group_by(track_name) %&amp;gt;%
        filter(doy &amp;gt;= doy[min(which(!is.na(played_at)))]) %&amp;gt;% 
        distinct(played_at, doy) %&amp;gt;%
        mutate(cumulative_plays = cumsum(na.locf(!is.na(played_at)))) %&amp;gt;%
        ggplot(aes(doy, cumulative_plays,fill = track_name)) + 
        geom_area(position = &amp;quot;identity&amp;quot;) + 
        facet_wrap(~track_name, nrow  = 2) +
        ggtitle(&amp;quot;Cumulative Histogram of Plays&amp;quot;) +
        xlab(&amp;quot;Date&amp;quot;) +
        ylab(&amp;quot;Cumulative Frequency&amp;quot;) +
        guides(fill = FALSE) +
        theme(axis.text.x = element_text(angle = 90, hjust = 1))
plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../blog/2017/2017-06-02-spotifyR_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the songs in my top 10 have a similar pattern: The first few days after discovering them, there is a sharp increase in the number of plays. Sometimes it takes a couple of listens for me to get into a track, but usually I start obsessing over it immediately. One obvious exception is the song &lt;em&gt;Habiba&lt;/em&gt;, the song I listened to the most. The first time I heard the song, it must have gone unnoticed. Two months later, I started playing it virtually on repeat.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;listening-times&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Listening times&lt;/h1&gt;
&lt;p&gt;Moving on, let’s look at what time of the day I listen to Spotify the most. I expect weekdays to exhibit a somewhat different pattern than weekends. We can plot separate timelines of the total number of listens per hour of the day for both weekdays and weekends. Unfortunately, there are more weekdays than weekends, so we need to normalize their respective counts to arrive at a meaningful comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)

merged %&amp;gt;% group_by(time_of_day = hour(played_at),
                    weekend = ifelse(wday(played_at) %in% c(6:7),
                                   &amp;quot;weekend&amp;quot;, &amp;quot;weekday&amp;quot;)) %&amp;gt;%
        summarise(plays = n_distinct(played_at)) %&amp;gt;%
        mutate(plays = ifelse(weekend == &amp;quot;weekend&amp;quot;, plays/2, plays/5)) %&amp;gt;%
        ggplot(aes(time_of_day, plays, colour = weekend)) +
        geom_line() +
        ggtitle(&amp;quot;Number of Listens per hour of the day&amp;quot;) +
        xlab(&amp;quot;Hour&amp;quot;) +
        ylab(&amp;quot;Plays&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../blog/2017/2017-06-02-spotifyR_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, there’s a few interesting things here. On &lt;strong&gt;weekdays&lt;/strong&gt; I listen to slightly more music than on weekends, mostly due to regular listening habits early on and during the day. The peak in the morning corresponds to me biking to work, followed by dip around 10 (daily stand-ups anyone?). Then, I put my headphones back on until about 14:00, to finish my Spotify activities in the evening when I get home.&lt;/p&gt;
&lt;p&gt;On the other hand, I listen to slightly more music in the afternoon and evening when it’s &lt;strong&gt;weekend&lt;/strong&gt;. Additionally, all early hours listening happens solely on weekends.&lt;/p&gt;
&lt;p&gt;I am also interested whether there is such a thing as &lt;em&gt;morning artists vs. afternoon/evening artists&lt;/em&gt;. In other words, which artists do I listen to more often in the morning than &lt;em&gt;after noon&lt;/em&gt;, or the other way around. The approach I took is to count the number plays by artists, and calculate a ratio of morning / evening for each one. The result I plotted with what is apparently called a &lt;a href=&#34;http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#Diverging%20Lollipop%20Chart&#34;&gt;diverging lollipop chart&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code snippet to produce this plot is tad bit too long to include here, but you can find all the code in the original RMarkdown file on &lt;a href=&#34;https://github.com/mtoto/mtoto.github.io/blob/master/blog/2017/2017-06-02-spotifyR.Rmd&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../blog/2017/2017-06-02-spotifyR_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the y-axis we have the artists. The x-axis depicts the aforementioned ratio, and the size of the &lt;em&gt;lollipop&lt;/em&gt; stands for the number of plays in the given direction, also displayed by the label.&lt;/p&gt;
&lt;p&gt;The artists with the biggest divergences are &lt;a href=&#34;https://open.spotify.com/artist/0HlOk15cW7PeziVcItQLco&#34;&gt;Zen Mechanics&lt;/a&gt; and &lt;a href=&#34;https://open.spotify.com/artist/1k8VBufn1nBs8LN9n4snc8&#34;&gt;Stereo MC’s&lt;/a&gt;. For both artists, the number of plays is almost equal to the difference ratio. That means I played songs in the opposite timeframe &lt;strong&gt;only once&lt;/strong&gt;. As a matter of fact, there are artists such as &lt;a href=&#34;https://open.spotify.com/artist/03HEHGJoLPdARs4nrtUidr&#34;&gt;Junior Kimbrough&lt;/a&gt; or &lt;a href=&#34;https://open.spotify.com/artist/3mNygoyrEKLgo6sx0MzwOL&#34;&gt;Ali Farka Touré&lt;/a&gt; whom I played more often in each direction, but because the plays are distributed more evenly, the ratio is not as extreme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;artist-genres&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Artist Genres&lt;/h1&gt;
&lt;p&gt;Lastly, let’s look at genres. Just as a track can have more than one artist to it, so can an artist have multiple associated genres, or no genre at all. To make our job less cumbersome, we first reduce our data to one genre per artist. We calculate the count of each genre in the whole dataset, and consequently select only one per artist; the one with the highest frequency. What we lose in detail, we gain in comparability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
# unnest genres
unnested &amp;lt;- merged %&amp;gt;% 
        mutate(artist_genres = replace(artist_genres,
                                       map(artist_genres,length) == 0, 
                                       list(&amp;quot;none&amp;quot;))) %&amp;gt;%
        unnest(artist_genres)
# calculate count and push &amp;quot;none&amp;quot; to the bottom 
# so it is not included in the top genres.
gens &amp;lt;- unnested %&amp;gt;% 
        group_by(artist_genres) %&amp;gt;% 
        summarise(genre_count = n()) %&amp;gt;%
        mutate(genre_count = replace(genre_count, 
                                     artist_genres == &amp;quot;none&amp;quot;,
                                     0))
# get one genre per artist
one_gen_per_a &amp;lt;- unnested %&amp;gt;% 
        left_join(gens, by = &amp;quot;artist_genres&amp;quot;) %&amp;gt;%
        group_by(artist_name) %&amp;gt;%  
        filter(genre_count == max(genre_count)) %&amp;gt;%
        mutate(first_genre = head(artist_genres, 1)) %&amp;gt;%
        filter(artist_genres == first_genre)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the genre column is dealt with, we can proceed to look at my favourite genres.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##         artist_genres plays
##                 &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
##  1         jazz blues   401
##  2            hip hop   351
##  3 psychedelic trance   302
##  4               funk   251
##  5         electronic   210
##  6                pop    79
##  7           psychill    70
##  8           afrobeat    64
##  9       classic rock    63
## 10          chillstep    62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, I am interested in whether there is a pattern in the genres I listen to. More specifically, it would be cool to see how my preferences evolve over time, if at all. The axes I want to plot my data along are the cumulative frequency and recency of songs played of a given genre.&lt;/p&gt;
&lt;p&gt;This is exactly what &lt;strong&gt;lifecycle grids&lt;/strong&gt; are made of, albeit usually used for customer segmentation. In a classical example, the more often you purchased a product, and the more recent your last purchase was, the more valuable you are as customer. I first read about these charts on &lt;a href=&#34;http://analyzecore.com/2015/02/16/customer-segmentation-lifecycle-grids-with-r/&#34;&gt;the analyzecore blog&lt;/a&gt;, which discusses these plots in more detail, including full code examples in &lt;code&gt;ggplot2&lt;/code&gt;. I highly recommend reading it if you’re interested.&lt;/p&gt;
&lt;p&gt;Clearly, we are not concerned with customer segmentation here, but what if we substituted customers with artist genres, and purchases with listens. These charts are like snapshots: how the grid is filled depends on the moment in time it was plotted. So to add an extra layer of intuition, I used the &lt;a href=&#34;https://github.com/dgrtwo/gganimate&#34;&gt;gganimate package&lt;/a&gt; to create an animated plot that follows my preferences as days go by.&lt;/p&gt;
&lt;p&gt;To be able to generate such a plot, we need to expand our dataset to include all possible combinations of dates and genres and deal with resulting missing values appropriately:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genres_by_day &amp;lt;- one_gen_per_a %&amp;gt;%
        # only look at top 20 genres
        filter(artist_genres %in% top20genres$artist_genres) %&amp;gt;%
        group_by(artist_genres, doy = as.Date(played_at)) %&amp;gt;%
        arrange(doy) %&amp;gt;%
        summarise(frequency = n_distinct(played_at)) %&amp;gt;%
        ungroup() %&amp;gt;%
        complete(artist_genres, doy = full_seq(doy, period = 1))  %&amp;gt;%
        group_by(artist_genres) %&amp;gt;%
        mutate(frequency = replace(frequency,
                                   is.na(frequency),
                                   0),
               first_played = min(doy[min(which(frequency != 0))]),
               last_played = as.Date(ifelse(frequency == 0, NA, doy)),
               cumulative_frequency = cumsum(frequency),
               last_played = replace(last_played, 
                                     doy &amp;lt; first_played, 
                                     first_played),
               last_played = na.locf(last_played),
               recency = doy - last_played)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After binning both &lt;code&gt;cumulative_frequency&lt;/code&gt; and &lt;code&gt;recency&lt;/code&gt; from the resulting dataset, we can proceed with creating our animated lifecycle grid using &lt;code&gt;ggplot2&lt;/code&gt; and &lt;code&gt;gganimate&lt;/code&gt;. All we need to do is specify the &lt;code&gt;frame =&lt;/code&gt; variable inside the &lt;code&gt;aes()&lt;/code&gt;, and our plot comes to life!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg_life &amp;lt;- genres_by_day %&amp;gt;%
        ggplot(aes(x = genre, y = cumulative_frequency, 
                   fill = artist_genres, frame = doy, 
                   alpha = 0.8)) +
        theme_bw() +
        theme(panel.grid = element_blank())+
        geom_bar(stat=&amp;quot;identity&amp;quot;,position=&amp;quot;identity&amp;quot;) +
        facet_grid(segm.freq ~ segm.rec, drop = FALSE) +
        ggtitle(&amp;quot;LifeCycle Grid&amp;quot;) +
        xlab(&amp;quot;Genres&amp;quot;) +
        ylab(&amp;quot;Cumulative Frequency&amp;quot;) +
        guides(fill = guide_legend(ncol = 1),
               alpha = FALSE)
        
gganimate(gg_life)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://tamaszilagyi.com/img/lifecycle.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;More than anything, the plot makes it obvious that I cannot go on for too long without listening to my favourite genres such as &lt;code&gt;jazz blues&lt;/code&gt;, &lt;code&gt;hip hop&lt;/code&gt; and &lt;code&gt;psychedelic trance&lt;/code&gt;. My least often played genres from the top 20 on the other hand are distributed pretty evenly across the &lt;strong&gt;recency axis&lt;/strong&gt; of my plot in the last row (containing genres with less than or equal to 50 listens).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-left&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What’s left?&lt;/h1&gt;
&lt;p&gt;Clearly, there are tons of other interesting questions that could be explored using this dataset. We could for example look at how many tracks I usually listen to in one go, which songs I skipped over, how my different playlists are growing over time, which playlist or albums I listen to the most…and the list goes on.&lt;/p&gt;
&lt;p&gt;I’ll go into more detail on my approach to automating acquisition and cleaning of this data in a &lt;a href=&#34;http://tamaszilagyi.com/blog/creating-a-spotify-playlist-using-luigi/&#34;&gt;next post&lt;/a&gt;, but if you just cannot wait to start collecting your own Spotify listening history, I encourage you to go through &lt;a href=&#34;https://developer.spotify.com/web-api/authorization-guide/#authorization_code_flow&#34;&gt;Spotify’s authoriziation flow&lt;/a&gt; and set up a simple cronjob that pings the API &lt;em&gt;X times a day&lt;/em&gt;. The sooner you start collecting your data, the more you’ll have to play with. Everything else can be dealt with later.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Starting a blog(down)</title>
      <link>http://tamaszilagyi.com/blog/starting-a-blogdown/</link>
      <pubDate>Sun, 14 May 2017 21:13:14 -0500</pubDate>
      
      <guid>http://tamaszilagyi.com/blog/starting-a-blogdown/</guid>
      <description>&lt;!-- BLOGDOWN-HEAD --&gt;
&lt;!-- /BLOGDOWN-HEAD --&gt;

&lt;!-- BLOGDOWN-BODY-BEFORE --&gt;
&lt;!-- /BLOGDOWN-BODY-BEFORE --&gt;
&lt;div id=&#34;starting-an-analytics-blog&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Starting an analytics blog&lt;/h1&gt;
&lt;p&gt;Having learned lots from the open source community over the past years - from blogs and videos to attending meetups and awesome conferences - I have decided to start a blog myself, and share some of the things I find interesting. I expect most of the posts to be &lt;code&gt;R&lt;/code&gt; specific, because that’s what I am most comfortable with. However I do enjoy fiddling with other technologies such as &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;Spark&lt;/code&gt;, so watch out! In a nuthsell though, this blog will be about using open source tools to build all sorts of cool things with &lt;strong&gt;data&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;its-easy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;It’s easy&lt;/h1&gt;
&lt;p&gt;You can get your blog up and running with literally three lines of &lt;code&gt;R&lt;/code&gt; code. After hearing about the &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;&lt;strong&gt;blogdown&lt;/strong&gt;&lt;/a&gt; package on Twitter, I went ahead and downloaded the current build from Github, running &lt;code&gt;install_github(&#39;rstudio/blogdown&#39;)&lt;/code&gt; inside &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Under the hood, blogdown uses &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; to generate the website, but wraps most functionality nicely, so there’s no need for much manual configuration during the process, if at all.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;We first create a folder for the blog on our computer, and set it as our home directory using &lt;code&gt;setwd(&amp;quot;path-to-blog&amp;quot;)&lt;/code&gt;. Then we simply run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1 if you haven&amp;#39;t already, install blogdown
devtools::install_github(&amp;#39;rstudio/blogdown&amp;#39;)
# 2 install hugo
blogdown::install_hugo()
# 3 create new site
blogdown::new_site()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it. You now have a complete folder structure initialized in your working directory:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/mtoto/mtoto.github.io/master/blog/2017/img/folderstruct.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The local build of your new site is now running on &lt;code&gt;localhost&lt;/code&gt;. You can see it in RStudio’s Viewer, or inside a browser by clicking &lt;em&gt;Show in new window&lt;/em&gt; in the top left corner of the Viewer.&lt;/p&gt;
&lt;p&gt;You future blog posts will reside in the &lt;code&gt;content/post&lt;/code&gt; folder. Here we find two pre-existing posts as &lt;code&gt;.Rmd&lt;/code&gt; files. We can start editing these straight away and see the results immediately after saving. Because everytime you save changes, your site is instantly rebuilt. If you come back to work on your existing site, you can simply run the function &lt;code&gt;serve_site()&lt;/code&gt; after you are done editing, and see the site regenerated accordingly in the Viewer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;customization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Customization&lt;/h2&gt;
&lt;p&gt;Now we can begin to customize the look of our blog by installing a theme using the function &lt;code&gt;install_theme(&#39;username/theme&#39;)&lt;/code&gt;. For my site, I picked &lt;a href=&#34;https://github.com/nishanths/cocoa-hugo-theme&#34;&gt;nishanths/cocoa-hugo-theme&lt;/a&gt; which I like very much for its minimalistic design. You can browse other themes on &lt;a href=&#34;https://themes.gohugo.io/&#34;&gt;themes.gohugo.io/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;configuration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;p&gt;The only thing left to do, is to edit the &lt;code&gt;config.toml&lt;/code&gt; file and set the name of your blog, avatars, or even link a &lt;code&gt;Google Analytics&lt;/code&gt; account - if the theme allows for. The file contains parameters such as:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;title        = &amp;quot;Tamas Szilagyi&amp;quot;
baseurl      = &amp;quot;http://tamaszilagyi.com/&amp;quot;
relativeurls = true
languageCode = &amp;quot;en-us&amp;quot;
theme        = &amp;quot;cocoa-hugo-theme&amp;quot;
faviconfile  = &amp;quot;img/leaf.ico&amp;quot;
github       = &amp;quot;//github.com/mtoto&amp;quot;
highlightjs  = true
avatar       = &amp;quot;img/profile_pic.png&amp;quot; 
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are going to include &lt;code&gt;R&lt;/code&gt; codechunks in your posts, also don’t forget to set &lt;code&gt;highlightjslanguages = [&amp;quot;r&amp;quot;]&lt;/code&gt;. When the blog is ready, we run &lt;code&gt;build_site()&lt;/code&gt; to compile the files to &lt;code&gt;html&lt;/code&gt; and build the website. What we need for deployment will reside under the &lt;code&gt;/public&lt;/code&gt; folder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deployment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Again, publishing is a piece of cake. There are &lt;a href=&#34;https://bookdown.org/yihui/blogdown/deployment.html&#34;&gt;multiple ways&lt;/a&gt; for conveniently deploying a blogdown site, and being somewhat familiar with &lt;a href=&#34;https://pages.github.com/&#34;&gt;Github Pages&lt;/a&gt;, that’s what I went for. I created a new repository named &lt;code&gt;mtoto.github.io&lt;/code&gt; and simply pushed the contents of &lt;code&gt;/public&lt;/code&gt; to the master branch.&lt;/p&gt;
&lt;p&gt;The website should be almost immediately available at the same address as the repo name. If you want an url other than &lt;code&gt;username.github.io&lt;/code&gt; however, you will need to sign up with a hosting provider. Then put a file in the &lt;code&gt;/public&lt;/code&gt; folder called &lt;code&gt;CNAME&lt;/code&gt;, with a one liner containing your blog url such as &lt;code&gt;tamaszilagyi.com&lt;/code&gt;. After, you push this file to Github and ask your provider to point your domain to the github pages url.&lt;/p&gt;
&lt;p&gt;And voilà, we have ourselves a full functioning static website that looks great, is easy to manage and as portable as it gets may you decide to switch for different hosting solutions.&lt;/p&gt;
&lt;p&gt;For a more in-depth overview of what &lt;code&gt;blogdown&lt;/code&gt; is capable of, keep an eye on its &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;bookdown site&lt;/a&gt; which is currently under development.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>